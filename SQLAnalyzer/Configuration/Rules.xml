<IssuesList>
    <PTOClinicIssue enabled = "true">
        <Title>Page file set to "Automatically manage paging file size for all drives".</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The paging file (Pagefile.sys) is a hidden file on your computer's hard disk that Windows uses as if it were random access memory (RAM). The paging file and physical memory make up virtual memory. By default, Windows stores the paging file on the boot partition (the partition that contains the operating system and its support files).]]>
        </Impact>
        <Recommendation>
            <![CDATA[When Windows 2008 page file is configured to "Automatically manage paging file for all drives", page file is managed by the system according to how much virtual memory and disk space you have. 
By default, "Automatically manage paging file size for all drives" setting is selected so that Windows 2008 system can manage the paging file without users interruption and configures roughly to two times to the size of physical RAM. If you want change the paging file size, move the pagefile.sys to another drive, or disable virtual memory paging, uncheck the check box of Automatically manage paging file size for all drives.
Follow the guidelines as discussed in the KB "How to determine the appropriate page file size for 64-bit versions of Windows Server 2003 or Windows XP" and configure the page file manually to set to custom sizes on each drive (http://support.microsoft.com/kb/889654) 
How to set Paging file to Custom Size: 
1. Click Start, right-click Computer and select Properties 
2. Select the Advanced System Settings 
3. Under Performance, Click Settings button. 
4. Click the Advanced tab 
5. Under Virtual Memory, click Change button 
6. If "Automatically manage paging file size for all drives" is selected, uncheck the option 
7. Select the appropriate drives 
8. Choose Custom size 
9. Set Initial Size (MB) and Maximum Size (MB) as appropriate
]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to determine the appropriate page file size for 64-bit versions of Windows Server 2003 or Windows XP (http://support.microsoft.com/kb/889654) 

Error message when you try to upgrade to Windows 7 or to Windows Server 2008 R2: "There is not enough free space on partition (C:)" (http://support.microsoft.com/kb/972502) 

Multiple page files are created when you set the AutomaticManagedPagefile property of the Win32_ComputerSystem class to False on a computer that is running Windows Server 2008 or Windows Vista SP1 (http://support.microsoft.com/kb/959516)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The Windows OS power saving setting may affect the CPU Performance.</Title>
        <Category>Windows Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Windows 2008 Operating system introduced energy saving settings by means of new power saving settings that allow saving the power consumption whenever possible.
By default, the Windows sets the power saving mode to Balanced, which means that when the system is not busy various components, such as the CPU and storage, are scaled back for saving the power consumption.
In some cases, you may experience degraded overall performance on a Windows Server 2008 and above machine when running with the default (Balanced) power plan.
On Intel X5500 and other last-generation CPUs, the clock is throttled down to save power (Processor P-state), and only increases when CPU utilization reaches a certain point.
By default, Windows Server 2008 and above set the Balanced power plan, which enables energy conservation by scaling the processor performance based on current CPU utilization.
The Minimum and Maximum Processor Performance State parameters are expressed as a percentage of maximum processor frequency, with a value in the range 0  100.
If a server requires ultra-low latency, invariant CPU frequency, or the very highest performance levels, such as a database server, it might not be helpful that the processors keep switching to lower-performance states. As such, the High Performance power plan caps the minimum processor performance state at 100 percent.]]>
        </Impact>
        <Recommendation>
            <![CDATA[To work around the performance degradation issue, you can switch to the High Performance power plan. But, this will disable dynamic performance scaling on the platform. Depending on the environment, if the SQL Server is always under a heavy load, then this is a viable solution.

To change a power plan:
Click on Start and then Control Panel.
From the list of displayed item under Control Panel click on Power Options, which takes you to select a power plan page. If you do not see Power Options, type the word 'power' in the Search Control Panel box and then select choose a power plan.
By default, the option to change power plans is disabled. To enable this, click the Change settings that are currently unavailable link.
Choose the High Performance option.
Close the Power Option window.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Degraded overall performance on Windows Server 2008 R2 (http://support.microsoft.com/kb/2207548)

Windows Server 2008 Power Savings 
(http://www.microsoft.com/en-us/download/details.aspx?id=5809)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables have been identified that have more indexes than columns.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[During Inserts, Updates and Deletes every index on a table has to be evaluated and potentially updated. If too many indexes exist, SQL Server will spend unnecessary resources maintaining indexes that may not be used. 
How indexes are chosen significantly affects the amount of disk I/O generated and, subsequently, performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not build any indexes, including a clustering index on a primary key, until a query demonstrates a requirement for it. In general, all columns named in a multi-table query should be covered by an index.

Review the indexes and remove those that are unnecessary.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Index-Related Dynamic Management Views and Functions (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187974.aspx)

SQL Server 7.0 Performance Tuning Guide (http://support.microsoft.com/kb/322883)

Following is a SQL Server 7.0 document that also applies to SQL Server 2000. For SQL Server 2005 and 2008, ignore the platform-specific information and concentrate on the Index Selection section: (http://support.microsoft.com/kb/322883)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Low page life expectancy.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Low page life expectancy counter is less than 300. This performance monitor counter tells you, on average, how long data pages are staying in the buffer without being referenced. If this value gets below 300 seconds, this is a potential indication that your SQL Server could use more memory in order to boost performance (there is the risk that additional physical I/O is required to read pages from disk instead of reading them from the buffer pool).]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Consider adding additional memory or reducing the amount of required reads by tuning the queries and indexes.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Top SQL Server 2005 Performance Issues for OLTP Applications
(http://technet.microsoft.com/en-us/library/cc966401.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
	<Counters>
		<Counter>:Buffer Manager\Page life expectancy</Counter>
	</Counters>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables have been identified that do not have a clustered index, and have one or more non-clustered indexes.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[In most situations, each table in the database should have a clustered index defined for it. Without a clustered index, the heap will not be defragmented as part of your regular index rebuild or reorganization process. As a result, query performance could suffer.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Clustered indexes define the physical order of the rows in a table, with the leaf level of the clustered index containing the actual data rows for the table. When selecting a clustered index key, select columns that have a high cardinality, are frequently referenced in query criteria, and are used within range queries.

Evaluate your application query activity against the heap table. In addition, define the best column candidate for the index key and create the clustered index.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : CREATE INDEX (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188783.aspx)

SQL Server Books Online : General Index Design Guidelines (http://msdn2.microsoft.com/en-us/library/ms191195.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Review the Fillfactor setting for indexes.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Fill factor is used to determine how much free space is required for an index page. This is necessary in order to keep the index as compact as possible while as the same time preventing performance delays when splitting data to a new page after the current page fills up. 
 
When a new row is added to a full index page, the database engine moves approximately half of the rows to a new page to make room for the new row. This reorganization is known as a page split. Although a page split allows room for new records, it can also take time to perform and is a resource-intensive operation. In addition, a page split can result in fragmentation that causes increased I/O operations. However, selecting a correct fill factor value can reduce the potential for page splits by providing enough space for index expansion as data is added to the underlying table. 
 
Creating a new index page and moving a portion of the records to the new page can be an expensive operation. Having index pages with only a few records and lots of available space can be inefficient during read operations. This is because the system will have to retrieve and search through more pages to find the data than it would if the pages were more compact.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Generally, the correct fill factor will depend significantly on the amount and type of operations being performed on the table. 
In an environment where most of the activity on a table is due to insert operations, use a lower fill factor to prevent the fragmentation and increased I/O associated with page splits. 
In an environment where most of the activity is due to select operations, use a higher fill factor.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : CREATE INDEX (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188783.aspx)

SQL Server Books Online : General Index Design Guidelines (http://msdn2.microsoft.com/en-us/library/ms191195.aspx)

SQL Server Books Online: Fill Factor
(http://msdn2.microsoft.com/en-us/library/ms191005.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Too many virtual log files may impact the SQL Server performance.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The transaction log files are internally divided into sections called Virtual Log Files (VLFs) and the more transaction log file experiences the fragmentation, the more the VLFs created. Several VLFs are generated if the transaction log file is created by small initial size and small growth increments. Once the transaction log file builds more than 100 VLFs, you may start noticing the performance issues with the operations that use the transaction log file such as log reads for transactional replication, rollback, log backups and database recovery etc.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Create the transaction log file with an appropriate initial size, anticipate the future needs and set the auto growth to an adequate size. While sizing the transaction log file consider the factors like transaction size (long-running transactions cannot be cleared from the log until they complete) and log backup frequency (since this is what removes the inactive portion of the log).

Adjust your transaction log files to their needed size in advance. Each VLF spawned should not exceed 512MB, so calculate the needed autogrow setting based on this guideline and the VLF creation methods, outlined below.
VLF creation is achieved according to the following method:
Growth in chunks less than 64MB will spawn 4 VLFs.
Growth in chunks of 64MB and up to 1GB will spawn 8 VLFs.
Growth in chunks of larger than 1GB will spawn 16 VLFs.
In SQL Server 2014, this somewhat changes:
Growth is less than 1/8 of the size of the current log size? Spawn 1 VLF.
Otherwise, use the common method.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Top Tips for Effective Database Maintenance
(http://technet.microsoft.com/en-us/magazine/2008.08.database.aspx)

SQL Server and Log File usage (http://blogs.msdn.com/b/blogdoezequiel/archive/2010/05/31/sql-server-and-log-file-usage.aspx)

SQL Swiss Army Knife #10 - VLFs again. Whats your current status? (http://blogs.msdn.com/b/blogdoezequiel/archive/2011/07/21/sql-swiss-army-knife-10-vlfs-again-what-s-your-current-status.aspx)

SQL Swiss Army Knife #9 - Fixing VLFs (http://blogs.msdn.com/b/blogdoezequiel/archive/2011/05/24/sql-swiss-army-knife-9-vlfs-revisited.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting: Max Degree of Parallelism is set to non-optimal value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The SQL Server configuration setting, max degree of parallelism (MAXDOP), not set to optimal value.
The max degree of parallelism option controls the number of processors that can be used to run a single Microsoft SQL Server statement using a parallel execution plan. The default value for this configuration is 0 and indicates that all available processors can be used.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Parallelism is often beneficial for longer running queries or for queries that have complicated execution plans. However, OLTP-centric application performance could sometimes suffer when parallel plans use more processors than the number of physical processors. In that case the time that it takes SQL Server to coordinate all the processors on a high-end server outweighs the advantages of using a parallel plan. Thus, consider modifying this value if you are experiencing excessive CXPACKET wait types on the SQL Server instance.

Use the following guidelines when you configure the MAXDOP value:
- For servers that use more than eight processors, use the following configuration: MAXDOP=8.
- For servers that have eight or less processors, use the following configuration where N equals the number of processors: MAXDOP=0 to N.
- For servers that have NUMA configured, MAXDOP should not exceed the number of CPUs that are assigned to each NUMA node.
- For servers that have hyper-threading enabled, the MAXDOP value should not exceed the number of physical processors.

If your SQL Server is hosting databases for the applications (for instance SAP, BizTalk, etc.) that require special considerations for MAXDOP, please consult the application vendor before altering the MAXDOP configuration.

If the affinity mask option is not set to the default, it may restrict the number of processors available to SQL Server on symmetric multiprocessing (SMP) systems, and this should be taken into account when configuring the MAXDOP setting. 

We recommend testing the application workload before you alter the MAXDOP configuration in the production environment to make sure that the new value provides possible performance gains.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188787.aspx)

SQL Server Books Online: max degree of parallelism Option
(http://msdn2.microsoft.com/en-us/library/ms181007.aspx)

General guidelines to use to configure the MAXDOP option:
(http://support.microsoft.com/kb/329204)

OLTP Blueprint - A Performance Profile of OLTP Applications
(http://blogs.msdn.com/sqlcat/archive/2006/06/23/Tom-Davidson-SQLCAT-Best-Practices.aspx)

How It Works: Maximizing Max Degree Of Parallelism 
(http://blogs.msdn.com/b/psssql/archive/2013/09/27/how-it-works-maximizing-max-degree-of-parallelism-maxdop.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Hypothetical statistics have been identified.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Hypothetical statistics are statistics that have been created by the Database Tuning Wizard (Microsoft SQL Server 2005 and above) or the Index Tuning Wizard (Microsoft SQL Server 2000). These hypothetical statistics are used to test whether a statistic is effective and whether it should be recommended. If these wizards are terminated before running completely, the hypothetical statistics will not be deleted. Also, in some versions of SQL Server 2000, the Index Tuning Wizard does not delete these test statistics after recommendations have been made, regardless of whether the wizard was interrupted.
While these statistics are hypothetical and are not used by the Query Optimizer, they are statistics seen in sys.stats, which can be mistaken by real objects when analysing the database design when performing query tuning, for instance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Hypothetical statistics should be deleted whenever they are found.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Index Tuning Wizard Fails to Remove Hypothetical Clustered Indexes (http://support.microsoft.com/default.aspx?scid=kb;EN-US;290414) 

Deleting Hypothetical Indexes and Statistics (http://msdn.microsoft.com/en-us/library/ms190172.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Hypothetical indexes have been identified.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Low</Severity>
        <Impact>
            <![CDATA[Hypothetical indexes are indexes that have been created by the Database Tuning Wizard (Microsoft SQL Server 2005 and above) or the Index Tuning Wizard (Microsoft SQL Server 2000). These hypothetical indexes are used to test whether an index is effective and whether it should be recommended. If these wizards are terminated before running completely, the hypothetical indexes will not be deleted. Also, in some versions of SQL Server 2000, the Index Tuning Wizard does not delete these test indexes after recommendations have been made, regardless of whether the wizard was interrupted, which can create performance issues in this version. 
While these indexes are hypothetical and are not used by the Query Optimizer, they are indexes that can be seen in sys.indexes and the associated statistics seen in sys.stats, which can be mistaken by real objects when analysing the database design when performing query tuning, for instance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Hypothetical indexes should be deleted whenever they are found as they are not used.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Index Tuning Wizard Fails to Remove Hypothetical Clustered Indexes (http://support.microsoft.com/default.aspx?scid=kb;EN-US;290414) 

Hypothetical Clustered Index From Index Tuning Wizard May Cause Recompile Loop (http://support.microsoft.com/default.aspx?scid=kb;EN-US;293177) 

Deleting Hypothetical Indexes and Statistics (http://msdn.microsoft.com/en-us/library/ms190172.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Login rate higher than logout rate.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Login and logout rates should be approximately the same. A login rate higher than the logout rate suggests that the server is not in a steady state, or that applications are not correctly using connection pooling. This could result in an increased load on the server.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Verify if the .NET connection string is using the pooling=true e connection reset=true parameters. 
If so, a profiler trace with the Audit login and Audit logout Events would reveal the usage of sp_reset_connection stored procedure, which is used by SQL Server to support remote stored procedure calls in a transaction. 
This stored procedure also causes Audit Login and Audit Logout events to fire when a connection is reused from a connection pool.
Also, the EventSubClass column in the trace will show if the connections are being pooled or not. 
Therefore focus the comparison only on the rate of non-pooled Logins and Logouts, as pooled connections will be reflected in the Logins/sec counter, but not on the Logouts/sec counter.

Review the login and logout rates over a longer period of time to determine if logins and logouts balance each other.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: SQL Server: General Statistics Object (http://technet.microsoft.com/en-us/library/ms190697.aspx)

SQL Server Connection Pooling (http://msdn.microsoft.com/en-us/library/8xx3tyca.aspx)

SQL Server Books Online: Audit Login Event Class (http://msdn.microsoft.com/en-us/library/ms190260.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Stalled I/O was found.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[In a slow I/O subsystem that suffers from many I/O stalls, users may experience performance problems such as slow response times or tasks that abort due to timeouts.]]>
        </Impact>
        <Recommendation>
            <![CDATA[SQL Server performance depends heavily on the I/O subsystem. Unless a database fits into memory, SQL Server constantly moves data pages in and out of the buffer pool. This can generate substantial I/O traffic.
In much the same way, log records need to be flushed to disk before a transaction can be committed (write-ahead logging).
Also, SQL Server uses a common resource (tempdb) for various purposes such as to store intermediate results, hashing, sorting, row versioning, amongst other uses.
Therefore, a good I/O subsystem is critical to the performance of SQL Server.

Test and configure your storage subsystem according to the vendor specifications for a SQL Server workload. Furthermore, find the queries that are most I/O bound and tune them.
The sys.dm_io_virtual_file_stats DMV will show an I/O stall when any wait occurs to access a physical data file. I/O stalls are stored per file in each database, and this DMV can output the I/O.
Check which database files have the most I/O and if necessary, isolate them in separate spindles for added performance. Correlate the sys.dm_io_virtual_file_stats DMV information with disk performance counters, sys.dm_io_pending_io_requests DMV and other SQL Server performance counters to troubleshoot slow I/O scenarios.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sys.dm_io_virtual_file_stats
(http://msdn.microsoft.com/en-us/library/ms190326.aspx)
The SQL Swiss Army Knife 3 - View I/O per file
(http://blogs.msdn.com/b/blogdoezequiel/archive/2012/03/08/the-sql-swiss-armyknife-3-view-i-o-per-file-updated.aspx)
Leveraging sys.dm_io_virtual_file_stats
(http://blogs.msdn.com/b/dpless/archive/2010/12/01/leveraging-sys-dm-io-virtualfile-stats.aspx)

Analyzing I/O Characteristics and Sizing Storage Systems for SQL Server Database Applications
(http://msdn.microsoft.com/en-us/library/ee410782.aspx)

Identifying the cause of SQL Server IO bottlenecks using XPerf
(http://blogs.msdn.com/b/sql_pfe_blog/archive/2013/04/23/identifying-cause-of-sql-server-io-bottleneck-using-xperf.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The network designated as the private network is configured as the primary network in the binding order.</Title>
        <Category>Windows Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Incorrectly configured binding order can cause inconsistent results in cluster operations and name resolution.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The correct configuration should reflect that the external facing or public network is first in the binding order, followed by the internal or private network, and then by any remote access connections.
Although binding orders might seem correct at first look, you might have disabled or "ghosted" NIC configurations on the system. "Ghosted" NIC configurations can affect the binding order and cause the binding order rule to issue a warning.
In Windows Server 2012/2012R2, the Cluster Network Driver (NETFT.SYS) adapter is automatically placed at the bottom in the binding order list.

To configure the binding order, follow these steps:
For Windows Server 2003
Click Start and then select Settings.
Click the Control Panel and then double-click Network and Dial-up Connections.
On the Advanced menu, click Advanced Settings.
In the Connections box, confirm that your bindings are in the following order and then click OK:
External public network
Internal private network (heartbeat)
[Remote Access Connections]

For Windows Server 2008 and above
Click Start, click Network, click Network and Sharing Center, and then click
Manage Network Connections.
Press the ALT key, click Advanced, and then click Advanced Settings. If you are prompted for an administrator password or confirmation, type the password or provide confirmation.
Click the Adapters and Bindings tab, and confirm that your bindings are in the following order and then click OK:
External public network
Internal private network (heartbeat)
[Remote Access Connections]

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Recommended Private "Heartbeat" Configuration on a Cluster Server (http://support.microsoft.com/kb/258750/en-us) 

You receive a warning about the network binding order on the Setup Support Rules page when you install SQL Server 2008 in a failover cluster 
(http://support2.microsoft.com/kb/955963)

Before Installing Failover Clustering
(http://msdn.microsoft.com/en-us/library/ms189910(v=sql.110).aspx?ocid=aff-n-we-loc--ITPRO40922&WT.mc_id=aff-n-we-loc--ITPRO40922#Network)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Pending I/O was found.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[In a slow I/O subsystem that suffers from many I/O stalls, users may experience performance problems such as slow response times or tasks that abort due to timeouts.]]>
        </Impact>
        <Recommendation>
            <![CDATA[SQL Server performance depends heavily on the I/O subsystem. Unless a database fits into memory, SQL Server constantly moves data pages in and out of the buffer pool. This can generate substantial I/O traffic.
In much the same way, log records need to be flushed to disk before a transaction can be committed (write-ahead logging).
Also, SQL Server uses a common resource (tempdb) for various purposes such as to store intermediate results, hashing, sorting, row versioning, amongst other uses.
Therefore, a good I/O subsystem is critical to the performance of SQL Server.

Test and configure your storage subsystem according to the vendor specifications for a SQL Server workload.
The sys.dm_io_virtual_file_stats DMV will show an I/O stall when any wait occurs to access a physical data file. I/O stalls are stored per file in each database, and this DMV can output the I/O.
Check which database files have the most I/O and if necessary, isolate them in separate spindles for added performance. Correlate the sys.dm_io_virtual_file_stats DMV information with disk performance counters, sys.dm_io_pending_io_requests DMV and other SQL Server performance counters to troubleshoot slow I/O scenarios.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sys.dm_io_pending_io_requests
(http://msdn.microsoft.com/en-us/library/ms188762.aspx)
The SQL Swiss Army Knife 3 - View I/O per file
(http://blogs.msdn.com/b/blogdoezequiel/archive/2012/03/08/the-sql-swiss-armyknife-3-view-i-o-per-file-updated.aspx)
How It Works: sys.dm_io_pending_io_requests
(http://blogs.msdn.com/b/psssql/archive/2012/01/23/how-it-works-sys-dm-io-pending-io-requests.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High parallelism waits exist in an OLTP environment.</Title>
        <Category>Performance Metrics</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[CXPACKET waits occur when trying to synchronize the query processor exchange iterator.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Properly designed reporting, DSS and warehouse systems use query parallelism to their advantage. Yet, many OLTP workloads benefit from suppressing or limiting query parallelism by reconfiguring the value for max degree of parallelism. 

CXPACKET waits accounting for more than 5% of total relevant resource waits indicate a query parallelism bottleneck.

You may consider lowering the degree of parallelism if contention on this wait type becomes a problem. 
In an OLTP environment, if only a fraction of your workload contributes to this type of wait, consider adding the appropriate MAXDOP value using a query hint. 

If the workload that contributes to this type of wait is executed in the context of an identifiable host name, application name or logins name, consider using Resource Governor to limit the available parallelism.

Conversely, if most of your workload contributes to this type of wait, then consider changing the server wide parameter for MAXDOP.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sys.dm_os_wait_stats (http://msdn.microsoft.com/en-us/library/ms179984(SQL.100).aspx) 

SQL Server Books Online: SQL Server, Wait Statistics Object (http://msdn.microsoft.com/en-us/library/ms179984(SQL.90).aspx) 

Recommendations and Guidelines for 'max degree of parallelism' configuration option (http://support.microsoft.com/kb/2023536) 

SQL Server Waits and Queues (http://www.microsoft.com/technet/prodtechnol/sql/bestpractice/performance_tuning_waits_queues.mspx) 

SQL Server Books Online: max degree of parallelism Option (http://msdn.microsoft.com/en-us/library/ms181007.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High memory waits exist.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Memory waits occur in several situations, and are a sign of memory allocation issues on the server.]]>
        </Impact>
        <Recommendation>
            <![CDATA[RESOURCE_SEMAPHORE is occurs when a query memory request cannot be granted immediately due to other concurrent queries. 
RESOURCE_SEMAPHORE_SMALL_QUERY occurs when memory request by a query cannot be granted immediately due to other concurrent queries, mainly queries doing hashes and sorts.
RESOURCE_SEMAPHORE_QUERY_COMPILE indicates that there are a large number of concurrent compilations. In order to prevent inefficient use of query memory, SQL Server limits the number of concurrent compile operations that need extra memory. UTIL_PAGE_ALLOC occurs when transaction log scans wait for memory to be available during memory pressure.
SOS_VIRTUALMEMORY_LOW occurs when a memory allocation waits for a resource manager to free up virtual memory.
SOS_RESERVEDMEMBLOCKLIST occurs during internal synchronization in the SQL Server memory manager.
CMEMTHREAD occurs when a task is waiting on a thread-safe memory object. This indicates that the rate of insertion of entries into the plan cache is very high and there is contention.

Find the most memory intensive queries, examine their execution plans, and tune them. Also look into inefficient or missing indexes used in the queries and implement proper indexing.
RESOURCE_SEMAPHORE_SMALL_QUERY usually one should look for execution plans with excessive hashing or sorts that must be tuned.
RESOURCE_SEMAPHORE_QUERY_COMPILE usually means there is a high compilation or recompilation scenario (higher ratio of prepared plans vs. compiled plans), which means that SQL Server is not autoparameterizing your queries.
SOS_RESERVEDMEMBLOCKLIST usually one should look for procedures with a large number of parameters, or queries with a long list of expression values specified in an IN clause, which would require multi-page allocations
If CMEMTHREAD waits are causing problems, refer to the article below on CMEMTHREAD and debugging them. Look for memory objects top consumers, and if these are of type Partitioned by Node, you may use startup trace flag 8048 to further partition by CPU.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sys.dm_os_wait_stats (http://msdn.microsoft.com/en-us/library/ms179984(SQL.100).aspx) 

CSS blog: The SQL Server Wait Type Repository (http://blogs.msdn.com/b/psssql/archive/2009/11/03/the-sql-server-wait-type-repository.aspx)

CSS Blog: How it works: CMEMTHREAD and debugging them (http://blogs.msdn.com/b/psssql/archive/2012/12/20/how-it-works-cmemthread-and-debugging-them.aspx)

Troubleshooting Plan Cache Issues (http://technet.microsoft.com/en-us/library/cc293620.aspx)

SQL Server 2008/2008 R2 on Newer Machines with More Than 8 CPUs Presented per NUMA Node May Need Trace Flag 8048 (http://blogs.msdn.com/b/psssql/archive/2011/09/01/sql-server-2008-2008-r2-on-newer-machines-with-more-than-8-cpus-presented-per-numa-node-may-need-trace-flag-8048.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High log write waits exist.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[WRITELOG waits occur while waiting for a log flush to complete. Common operations that cause log flushes are checkpoints and transaction commits.]]>
        </Impact>
        <Recommendation>
            <![CDATA[For transactional workloads I/O performance of the writes to the SQL Server transaction log is critical to both throughput and application response time. When troubleshooting performance issues related to log performance, always consider optimizing the storage configuration to provide the best response time possible to the log device. This is critical to transactional performance in OLTP workloads.

If you see WRITELOG wait type, before adding more disk, check how frequently you transactions commit and how good is the log disk response time.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sys.dm_os_wait_stats (http://msdn.microsoft.com/en-us/library/ms179984(SQL.100).aspx) 

Diagnosing Transaction Log Performance Issues and Limits of the Log Manager (http://blogs.msdn.com/b/sqlcat/archive/2013/09/10/diagnosing-transaction-log-performance-issues-and-limits-of-the-log-manager.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High non-page latch waits exist.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The SQL Server Wait Statistics\Non-Page Latch Waits counter indicates the average time that each SQL Server session is waiting on non-page latches. A high number for this counter means that many processes are waiting for latch classes that are not related to buffers which, depending on the latch class, could affect your workload.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Latches are used to synchronize access to cached data pages and other in-memory objects. Typically, latches are only held briefly, and latch wait times should be correspondingly small.

The non-buffer and non-page latches provide synchronization services to in-memory data structures or provide re-entrance protection for concurrency-sensitive code. These latches can be used for a variety of things, but they are not used to synchronize access to buffer pages.
According to the latch class, correlate with wait types and take appropriate actions if these are unwarranted.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: SQL Server, Wait Statistics Object (http://technet.microsoft.com/en-us/library/ms190732.aspx)

SQL Server Best Practices Article (http://technet.microsoft.com/en-us/library/cc966413.aspx) 

Troubleshooting Performance Problems in SQL Server 2005 (http://www.microsoft.com/technet/prodtechnol/sql/2005/tsprfprb.mspx) 

Diagnosing and Resolving Latch Contention (http://download.microsoft.com/download/B/9/E/B9EDF2CD-1DBF-4954-B81E-82522880A2DC/SQLServerLatchContention.pdf)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High Page latch waits exist.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[SQL Server Wait Statistics\Page latch Waits counter indicates average time that each SQL Server session is waiting page latches, not including I/O latches.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Latches are used to synchronize access to cached data pages and other in-memory objects. Typically, latches are only held briefly, and latch wait times should be correspondingly small.

The buffer latches are used to synchronize access to BUF structures and their associated database pages. 
The typical buffer latching occurs during operations that require serialization on a buffer page, (during a page split or during the allocation of a new page, for example). These latches are not held for the duration of a transaction. These are indicated in the sys.dm_exec_requests DMV by the PAGELATCH wait types and accumulated in the sys.dm_os_query_stats..

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: SQL Server, Wait Statistics Object (http://technet.microsoft.com/en-us/library/ms190732.aspx 

SQL Server Best Practices Article (http://technet.microsoft.com/en-us/library/cc966413.aspx) 

Troubleshooting Performance Problems in SQL Server 2005 (http://www.microsoft.com/technet/prodtechnol/sql/2005/tsprfprb.mspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High Page IO latch waits exist.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[SQL Server Wait Statistics\Page IO latch Waits counter indicates average time that each SQL Server session is waiting I/O latches.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Latches are used to synchronize access to cached data pages and other in-memory objects. Typically, latches are only held briefly, and latch wait times should be correspondingly small.

The IO latches are a subset of BUF latches that are used when the buffer and associated data page or the index page is in the middle of an IO operation. PAGEIOLATCH wait types are used for disk-to-memory transfers and a significant wait time for these wait types suggests disk I/O subsystem issues. 

Review the disk performance counters include Physical Disk: disk sec/Read and disk sec/Write and SQL Server Buffer Manager: Page Life Expectancy.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: SQL Server, Wait Statistics Object (http://technet.microsoft.com/en-us/library/ms190732.aspx]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Disk response times are too long.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[At least one drive has an average read or write response time that is over 10 milliseconds (ms). There is the risk of degraded performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Either Disk sec/Read or Disk sec/Write were outside of recommended values.

Avg. Disk sec/Read is the average time, in seconds, of a read of data from the disk.

Reads
Excellent -- less than 8 ms (.008 seconds)
Good -- less than  12 ms (.012 seconds)
Fair -- less than  20 ms (.020 seconds)
Poor -- greater than  20 ms (.020 seconds)

Avg. Disk sec/Write is the average time, in seconds, of a write of data to the disk.

Non cached Writes
Excellent -- less than 8 ms (.008 seconds)
Good -- less than  12 ms (.012 seconds)
Fair -- less than  20 ms (.020 seconds)
Poor -- greater than  20 ms (.020 seconds)

Cached Writes Only
Excellent -- less than 1 ms (.001 seconds)
Good -- less than 2 ms (.002 seconds)
Fair -- less than 4 ms (.004 seconds)
Poor -- greater than 4 ms (.004 seconds)

Investigate disk performance. In addition, identify queries that are using excessive reads or writes and tune them to reduce I/O. Consider moving highly active databases requiring significant I/O to drives that are less utilized. Use SQLIO.exe or IOmeter.exe (http://www.iometer.org) during low usage hours to test whether the storage system is configured for maximum throughput.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Storage Best Practices for SQL Server (http://technet.microsoft.com/en-us/library/cc966534.aspx)
Storage Engine Capacity Planning Tips (http://technet.microsoft.com/en-us/library/cc966460.aspx) 
SQLIO Disk Subsystem Benchmark Tool (http://www.microsoft.com/downloads/details.aspx?familyid=9a8b005b-84e4-4f24-8d65-cb53442d9e19)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High number of single use plans in plan cache.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[If you run ad-hoc workloads, whose statements might not be auto-parameterized, SQL Server may not leverage plan re-use, which leads to wasted memory.]]>
        </Impact>
        <Recommendation>
            <![CDATA[For a scalable production system, you should consider using stored procedures for your application, if the input parameters are known, as these will also leverage a better usage of SQL Server resources, and usage over multiple connections. 
If dynamic, ad-hoc statements are used, then consider using sp_executesql to execute these statements, allowing these to be parameterized. This will also leverage plan re-use.
sp_executesql is method is recommended for Transact-SQL statements, stored procedures, or triggers that generate SQL statements dynamically.

Find the number of single-use plans that are cached in SQL Server using the following script:

-- number of cached plans with usecounts = 1.
SELECT objtype, cacheobjtype, AVG(usecounts) AS Avg_UseCount, SUM(refcounts) AS AllRefObjects, SUM(CAST(size_in_bytes AS bigint))/1024/1024 AS Size_MB
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype LIKE '%Plan%' AND usecounts = 1
GROUP BY objtype, cacheobjtype
GO

-- number of cached plans with usecounts > 1.
SELECT objtype, cacheobjtype, AVG(usecounts) AS Avg_UseCount_perPlan, SUM(refcounts) AS AllRefObjects, SUM(CAST(size_in_bytes AS bigint))/1024/1024 AS Size_MB
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype LIKE '%Plan%' AND usecounts > 1
GROUP BY objtype, cacheobjtype
GO

If the number of single-use plans take a significant portion of SQL Servers memory in an OLTP server, and these plans are Ad-hoc plans, use the optimize for ad-hoc workloads server option, available since SQL Server 2008, to decrease memory usage with these objects.
If the number of single-use plans take a significant portion of SQL Servers memory, and these plans are Prepared plans, although parameterization is taking place, widely varying parameters do not allow plan re-use. In this case, consider using stored procedures. Additionally, you might also consider clearing the SQL Plan cache (where ad-hoc, auto parameterized and prepared plans are cached) on a regular basis using DBCC FREESYSTEMCACHE('SQL Plans')  check the Too many single use plans, now what? post on the previous section for an example code.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_executesql (Transact-SQL)  (http://msdn.microsoft.com/en-us/library/ms188001.aspx) 

SQL Server Books Online: optimize for ad hoc workloads Server Configuration Option (http://msdn.microsoft.com/en-us/library/cc645587.aspx)

SQL Server Books Online: Parameters and Execution Plan Reuse (http://msdn.microsoft.com/en-us/library/ms175580.aspx) 

When to Use Stored Procedures and Other Caching Mechanisms (http://technet.microsoft.com/en-us/library/cc293619.aspx)

Execution Plan Caching and Reuse (http://msdn.microsoft.com/en-us/library/ms181055.aspx) 

Caching Mechanisms (http://technet.microsoft.com/en-us/library/cc293623.aspx)

SQL Server Books Online: DBCC FREESYSTEMCACHE (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms178529(v=sql.100).aspx)

How to troubleshoot the performance of Ad-Hoc queries in SQL Server (http://support.microsoft.com/kb/243588)

Too many single use plans, now what? (http://blogs.msdn.com/b/blogdoezequiel/archive/2014/07/30/too-many-single-use-plans-now-what.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Low free pages.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Free pages is the total number of pages on all free lists. This counter is expressed in the number of 8-kilobyte pages existing in the cache that are not currently being used. If this counter value is far below 640 pages (about 5 megabytes), SQL Server could be running low on physical memory.
In SQL Server 2012 and above this counter has been replaced by Free Memory (KB), but we should apply the same 5MB threshold.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Free pages counter should not have a low sustained value. When there are no free pages in the buffer pool, the memory requirements of your SQL Server may have become so intense that the lazy writer or the checkpoint process is unable to keep up. 
Typical signs of buffer pool pressure are a higher than normal number of lazy writes per second or a higher number of checkpoint pages per second as SQL Server attempts to empty the procedure and the data cache to get enough free memory to service the incoming query plan executions. 
This is an effective detection mechanism that indicates that your procedure or data cache is starved for memory.

Either increase the RAM that is allocated to SQL Server, or locate the large number of hashes or sorts that may be occurring. Analyze this counter in conjunction with the other Buffer Manager counters Page life expectancy, Lazy writes/sec, checkpoint pages/sec to evaluate the overall SQL Server memory pressure. Optimizing your queries significantly improve the value for this counter. Avoid unnecessary scans, hash operations, and in general reduce the amount of pages that need to be scanned to find the matching rows for each query.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Troubleshooting Performance Problems in SQL Server 2005 (http://technet.microsoft.com/en-us/library/cc966540.aspx)

SQL Server 2008 Books Online: SQL Server, Buffer Node Object (http://technet.microsoft.com/en-us/library/ms345597(SQL.100).aspx)

SQL Server 2008 Books Online: SQL Server, Buffer Manager Object (http://technet.microsoft.com/en-us/library/ms189628(v=sql.100).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables and indexed views have been identified that have duplicate indexes.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[An index is considered to be a duplicate if it references the same column and ordinal position as another index in the same database. Duplicate indexes provide no performance or manageability benefits. In addition, each duplicate index increases the I/O overhead of ongoing insert, update, and delete operations, as well as index rebuilds and index reorganizations. The overall result is reduced insert, update, and delete performance, and also prolonged index maintenance periods and wasted disk space.  Eliminating duplicate indexes can provide an immediate performance benefit for data modifications and also for index rebuilds and reorganizations.  Existing queries that reference the index will continue to use the original, non-duplicated index.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Duplicate indexes should always be avoided and usually indicate that correct processes for developer change control are not being enforced. You should remove duplicate indexes from the database unless they are explicitly referenced within the calling application. If they are referenced in the calling application, try to remove the referencing index hints or the batch references so that you can drop the duplicate indexes from the database.

Ensure that no applications explicitly reference the duplicate indexes that you plan to drop. Also, script out the duplicate index in Microsoft SQL Server Management Studio in the event that you will have to create it again. 
After scripting the index, issue a DROP INDEX command against the duplicate index in order to remove it from the database.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: CREATE INDEX (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188783.aspx)

SQL Server Books Online: DROP INDEX (Transact-SQL)
(https://msdn2.microsoft.com/en-us/library/ms176118.aspx)

SQL Server Books Online: How to: Generate a Script (SQL Server
Management Studio)
(https://msdn2.microsoft.com/en-us/library/ms178078.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Total server memory is less than target server memory.</Title>
        <Category>Performance Metrics</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The Target Server Memory counter indicates the size that the buffer pool should be for SQL Server. The Total Server Memory counter indicates the amount of memory that SQL Server is currently using. Eventually, Total Server Memory should be about the same as Target Server Memory. There is the risk of possible trimming of the SQL Server working set.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Grant "Lock Pages in Memory" rights to the SQL Server service account and then monitor this performance counter.
This permission will allow for a copy of the buffer cache to stay resident in physical memory, preventing the system from paging the data to virtual memory on disk. 
This privilege must also be set to configure Address Windowing Extensions (AWE) on 32-bit versions of SQL Server, when your server has 4GB of memory or above. On x86 operating systems, setting this privilege when not using AWE can significantly impair system performance. 
Also, setting this user right implies a restart of SQL Server service. Note that for 64-bit editions of SQL Server only Enterprise Edition can use the Lock pages in memory user right. This is applicable for SQL Server 2005 (RTM, SP1, SP2, SP3) and for SQL Server 2008 (RTM and SP1).
SQL Server 2008 SP1 CU2 and SQL Server 2005 SP3 CU4 introduce support for SQL Server Standard editions to use the Lock pages in memory user right, that also add the trace flag 845, that must be set up as a start-up parameter to support page locking.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server, Memory Manager Object(http://msdn.microsoft.com/en-us/library/ms190924.aspx)

How to reduce paging of buffer pool memory in the 64-bit version of SQL Server (http://support.microsoft.com/kb/918483)

Enable the Lock Pages in Memory Option (Windows) (http://msdn.microsoft.com/en-us/library/ms190730.aspx)

How to enable the "locked pages" feature in SQL Server 2012
(http://support.microsoft.com/kb/2659143)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Available memory MB too low.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Available MBytes is the amount of physical memory, in megabytes, that is available to the processes running on the computer. Ideally, this value should be greater than the LowMemThreshold. External memory pressure will occur if this value is lower than LowMemThreshold, which depending on your configurations, may even lead to paging out the sqlservr process and system-wide performance issues.
From Windows Internals book by David Solomon and Mark Russinovich:
"The default level of available memory that signals a low-memory-resource notification event is approximately 32 MB per 4 GB, to a maximum of 64 MB. The default level that signals a high-memory-resource notification event is three times the default low-memory value."]]>
        </Impact>
        <Recommendation>
            <![CDATA[It is best practice to ensure that there is a minimum of 64MB of available memory on the server, depending on the overall system memory.
For a server that has 8GB or above, the following are the default thresholds:
1. If the available memory on the system goes below 192 MB, HighMemoryResourceNotification (MEMPHYSICAL_HIGH) signal is revoked by Windows and SQL Server memory will not grow. 
2. If the available memory on the system goes below 64 MB, LowMemoryResourceNotification (MEMPHYSICAL_LOW) is signalled by Windows and SQL Server will reduce its memory usage.
3. When the available memory in the system is between 192MB and 64 MB, SQL Server will not grow or shrink its memory usage (RESOURCE_MEM_STEADY).

Determine which applications are consuming high memory. If it is the SQL Server instance that is seizing the memory, set the Max server memory for SQL Server as appropriate. Also check if any third-party DLLs are loaded into SQL Server memory space and causing memory leaks. If other major, memory-consuming applications share the same server resources, dedicate the server to run only SQL Server.
In specific cases, we can also increase the LowMemoryThreshold value so the OS will notify applications such as SQL on low memory conditions much earlier and SQL Server can respond to memory pressure earlier, before the system is starving for memory. And how to set the LowMemoryThreshold value (in MB)?

In Regedit -> go to HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\SessionManager\MemoryManagement -> Right click on the right pane, select New -> select and click DWORD Value -> enter LowMemoryThreshold -> double click LowMemoryThreshold -> value (choose decimal) -> enter a reasonable value depending on the installed memory on your system (ex.: 1024 for 1GB).

System Reboot is required to take effect.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Monitoring SQL Server Memory (http://msdn.microsoft.com/en-us/library/aa905152.aspx)

Monitoring Memory Usage (http://msdn.microsoft.com/en-us/library/ms176018.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Blocking issues have been identified.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Long term blocking in a system can lead to degraded overall performance, reduced concurrency, and repeated user attempts.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid long running queries and optimize performance or break into smaller transactions, avoid isolation levels that can cause blocking ex REPEATABLE READ or SERIALIZABLE and use two-part name for stored procedures to avoid compile locks.

Avoid long running queries that consume many resources or read-write a high number of records in a single transaction.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to troubleshoot SQL Server performance issues
(http://support.microsoft.com/kb/298475) 

SQL Swiss Army Knife #11.1 - Locking, blocking and active transactions
(http://blogs.msdn.com/b/blogdoezequiel/archive/2012/09/18/sql-swiss-army-knife-11-1-locking-blocking-and-active-transactions.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Deadlocking issues have been identified.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Deadlocks were identified during the data capture process.  All deadlock issues should be investigated and addressed as necessary by using the correct indexing and T-SQL code optimization. 

Deadlocks occur when two transactions use incompatible locks to hold on to the resources that the other needs. Deadlocks are generally caused by wrong-order access of objects, lock escalation or long-running transactions. This can result in blocking, reduced concurrency, and if the error is returned to the end user, it gives the appearance that the application does not work.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The following best practices help reduce deadlocking: 

1.  Run the queries involved in the deadlock through Database Tuning Advisor.  
2.  Make sure the query is using the minimum necessary transaction isolation level. 
3.  Make sure that transactions are as brief as they can be while still meeting the relevant business constraints.  
4.  Look for other opportunities to improve the efficiency of the queries involved in the deadlock.
5.  It is also possible to specify the relative importance that the current session continue processing if it is deadlocked with another session, using SET DEADLOCK_PRIORITY.

The Microsoft SQL Server Database Engine deadlock monitor periodically checks for tasks that are in a deadlock. If the monitor detects a cyclic dependency, it selects one of the tasks as a victim and terminates its transaction with an error. This allows the other task to complete its transaction. The application with the transaction that was terminated with an error can retry the transaction. Generally, the application will then complete the transaction after the other deadlocked transaction has finished. 

SQL Server 2008 and later capture deadlocks automatically in the system_health extended event session.  Periodically review the system_health session for deadlocks to determine if corrective actions need to occur.

You can also use SQL Server Profiler with the Deadlock Graph Event, or leverage the Extended Events system health session to capture information about the deadlocks.

It is also possible to specify the relative importance that the current session continue processing if it is deadlocked with another session, using SET DEADLOCK_PRIORITY.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Detecting and Ending Deadlocks (http://technet.microsoft.com/en-us/library/ms178104.aspx) 

SQL Server Books Online: Minimizing Deadlocks (http://technet.microsoft.com/en-us/library/ms191242.aspx) 

SQL Server Books Online: SET DEADLOCK_PRIORITY (Transact-SQL)
(http://technet.microsoft.com/en-us/library/ms186736.aspx)

How To Monitor Deadlocks in SQL Server (http://blogs.technet.com/b/mspfe/archive/2012/06/28/how_2d00_to_2d00_monitor_2d00_deadlocks_2d00_in_2d00_sql_2d00_server.aspx)

How to use the system_health Extended Event session:
https://docs.microsoft.com/en-us/sql/relational-databases/extended-events/use-the-system-health-session?view=sql-server-2017 
]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use old Style Join Syntax: '*=' or '=*'.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[The old style join syntax is not supported starting with SQL Server 2012. Any statements using the Transact-SQL outer joins should be changed to use the SQL-92 syntax.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Use the more readable ANSI-Standard Join clauses instead of the old style joins. With ANSI joins, the WHERE clause is used only for filtering data. With older style joins, the WHERE clause handles both the join condition and filtering data.

Use the current keywords LEFT JOIN and LEFT OUTER JOIN instead of '*='.
Use the current keywords RIGHT JOIN and RIGHT OUTER JOIN instead of '=*'.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : Using Joins 
(http://msdn2.microsoft.com/en-us/library/ms191472.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High number of SQL Recompilations.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Before SQL Server begins executing any of the individual query plans, the server checks for validity (correctness) and optimality of that query plan. If one of the checks fails, the statement corresponding to the query plan or the entire batch is compiled again, and a possibly different query plan produced. Such compilations are known as Re-compilations.
Stored procedure recompiles can be caused by SET statements, changes in statistics, and references to temporary tables that were not present at the time the procedure was compiled. Ideally, recompiles should be a small percentage of the number of compiles. 
Recompiles carry the risk of inefficient CPU resource utilization, and can cause deadlocks and compile locks that are not compatible with any locking type.]]>
        </Impact>
        <Recommendation>
            <![CDATA[SQL Profiler can provide information about what processors are recompiling, what statement, and the reason for recompilation. In Profiler, select the stored procedure event class and SP:recompilation event, and include the data column event subclass. Review the trace searching for event subclass values 1 through 6. The previous statements caused the recompilation. 
In SQL Server 2005, recompilations are statement-scoped. However, in Microsoft SQL Server 2000, they are batch-scoped. Therefore, direct comparison of values of this counter between SQL Server 2005 and earlier versions is not appropriate.
Generally, this counter should be as low as possible, not more than 1 Re-compilation per every 10 SQL Compilations.

Identify the statements or batches that are recompiled. Also identify the causes of recompilation using SQL Server Profiler Tracing and then fine-tune the queries to avoid the recompilations.
It is best to fully qualify stored procedure names when you execute a procedure. This allows for better clarity and easier reuse of the existing execution plan by the current user.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[HOW TO: Troubleshoot Application Performance with SQL Server (http://www.microsoft.com/technet/prodtechnol/sql/2005/recomp.mspx) 

Troubleshooting stored procedure recompilations in SQL Server 2000: (http://support.microsoft.com/kb/243586/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High number of SQL compilations.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The objective of caching plans is to minimize compilation and optimization in order to increase throughput. A high percentage of compilations compared to batch requests indicates that SQL Server is not able to use already compiled plans in the procedure cache. 
This could be the result of too many dynamic queries and there is the risk of inefficient CPU resource utilization.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If there is a sustained high value such as over 100 for a given amount of time, then it is an indication that there are many ad-hoc queries running, which might cause excessive CPU usage.
Generally, this counter should be as low as possible, not more than 1 Compilation per every 100 Batch Requests.

Use sp_executesql for dynamic SQL statements. In addition, investigate the possibility of changing prepared SQL and ad hoc SQL statements to stored procedures.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Compilation Bottlenecks (http://blogs.msdn.com/grahamk/archive/2009/02/03/compilation-bottlenecks-error-8628-severity-17-state-0-part-1.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High number of page lookups/sec exists.</Title>
        <Category>Performance Metrics</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Page Lookups/sec is the number of requests to find a page in the buffer pool.  This value is essentially the number of logical reads occuring on the system at any given time.

When the ratio of page lookups to batch requests is greater than 1000, this is an indication that while query plans are looking up too many pages in the buffer pool, these plans may be inefficient - especially if this is an OLTP centric workload.

Identify queries with the highest amount of logical I/O's and tune them.  Use this query to find the top 10 most expensive queries on the system:

SELECT TOP 20
	AverageLogicalReads = total_logical_reads/execution_count,
	AverageRunTimeSeconds = (total_elapsed_time/1000000.0)/execution_count,
	execution_count,
	last_worker_time,
	last_physical_reads,
	total_logical_writes,
	last_logical_writes,
	last_logical_reads,
	last_elapsed_time,
	StatementText = 
		REPLACE(REPLACE((SUBSTRING(text, statement_start_offset/2+1,
		(CASE WHEN statement_end_offset=-1
		THEN LEN(CONVERT(NVARCHAR(MAX), text))*2
		ELSE statement_end_offset
		END-statement_start_offset)/2)),CHAR(10),''),CHAR(13),''), 
		query_hash,
	query_plan_hash,
	sql_handle
FROM
	sys.dm_exec_query_stats q
	CROSS APPLY sys.dm_exec_sql_text(sql_handle)
	CROSS APPLY sys.dm_exec_query_plan(plan_handle)
ORDER BY
	total_logical_reads/execution_count  DESC



]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Identify queries with the highest amount of logical I/O's and tune the queries. Missing indexes, poor queries, and full scans can cause a high number of lookups.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: SQL Server, Buffer Manager Object
(http://msdn.microsoft.com/en-us/library/ms189628.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High number of free space scans exists.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Inserts into heaps require SQL Server to perform freespace scans to identify pages with free space to insert rows. Freespace scans are additional I/O for inserts and can cause contention on the GAM, SGAM, and PFS pages when there are many inserts. Evaluate clustered indexes for base tables.

This issue exists if there are greater than 10 Free Space Scans/sec per 100 batch requests/secInserts into heaps require SQL Server to perform freespace scans to identify pages with free space to insert rows. An insert into a heap often spends more time searching for a location to insert new row. On the other hand the insert into a clustered table does not have to spend time searching for time.

Number of scans per second that were initiated to search for free space within pages already allocated to an allocation unit to insert or modify record fragment. Each scan may find multiple pages.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

In most cases it is best to start with a clustered index on every table. There are occasional instances where this is not optimal, but it is extremely rare that the existence of a clustered index hurts performance and it usually helps.

Test your workload against replacing with the cluster indexes for performance effects of adding new cluster indexes.Freespace scans are additional I/O for inserts and can cause contention on the GAM, SGAM, and PFS pages when there are many inserts also can cause excessive CPU usage.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online (September 2009) SQL Server, Access Methods Object
(http://msdn.microsoft.com/en-us/library/ms177426.aspx)
INF: Poor performance on a Heap
(http://support.microsoft.com/kb/297861)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Too many lazy writes per second.</Title>
        <Category>Performance Metrics</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Lazy Writes/Sec tracks how many times per second the Lazy Writer process is moving dirty pages from the buffer to disk in order to free up buffer space. Generally speaking, this should be a low value - less than 10 per second on average.  Ideally, it should be close to zero. If it is zero, this indicates that your SQL Server's buffer cache is adequate and SQL Server doesn't have to free up dirty pages, and instead is waiting for this to occur during regular checkpoints. 
            
            If this counter value is a sustained high value (over 10), then a need for more memory is indicated or there are opportunities to tune the given workload.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If the Lazy Writer process is having problems keeping the free buffer steady, or at least above zero, it could indicate either memory pressure or that the disk subsystem is not able to provide Lazy Writer with the disk I/O performance that LazyWriter needs. (Compare drops in free buffer level to any disk queuing to see if this is true) 
 
Memory pressure is indicated if this counter is high together with low page life expectancy (~<300 seconds).

]]></Recommendation>
        <Reading>
            <![CDATA[SQL Server, Buffer Manager Object 
(http://msdn.microsoft.com/en-us/library/ms189628.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
	<Counters>
	<Counter>:Buffer Manager\Lazy writes/sec</Counter>
	</Counters>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables and/or indexed views have been identified that have redundant indexes.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This test case checks if there are any redundant indexes created on tables or indexed views. Two or more indexes are considered redundant if those indexes have the same subset of index columns and ordinal positions. For example, an index on col1 and another index on col1 and col2 are considered redundant. There is no need for a separate index on col1.

An index is considered to be a redundant if it references the same subset of columns and ordinal positions as another index in the same database. Redundant indexes provide no performance or manageability benefits. In addition, each redundant index increases the I/O overhead of ongoing insert, update, and delete operations, as well as index rebuilds and index reorganizations. The overall result is reduced insert, update, and delete performance, and also prolonged index maintenance periods and wasted disk space.
Eliminating the redundant index or indexes can provide an immediate performance benefit for data modifications and also for index rebuilds and reorganizations.
Existing queries that reference the index will continue to use the original, non-redundant index.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Redundant indexes should always be avoided and usually indicate that correct processes for developer change control are not being enforced. You should remove redundant indexes from the database unless they are explicitly referenced within the calling application. If they are referenced in the calling application, try to remove the referencing index hints or the batch references so that you can drop the redundant indexes from the database.
The only exception to this case is a covering index for the primary key of a wide row. In certain scenarios, such a duplicate index provides performance benefits for aggregates run against the primary key.

Ensure that no applications explicitly reference the redundant indexes that you plan to drop. Also, script out the redundant index in Microsoft SQL Server Management Studio in the event that you will have to create it again.
After scripting the index, issue a DROP INDEX command against the redundant index in order to remove it from the database.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: CREATE INDEX (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188783.aspx)

SQL Server Books Online: DROP INDEX (Transact-SQL)
(https://msdn2.microsoft.com/en-us/library/ms176118.aspx)

SQL Server Books Online: How to: Generate a Script (SQL Server
Management Studio)
(https://msdn2.microsoft.com/en-us/library/ms178078.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables have been identified that may have indexes that are never used.</Title>
        <Category>Database Design</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[During inserts, updates and deletes every index on a table has to be evaluated and potentially updated. If too many indexes exist, SQL Server will spend unnecessary resources maintaining indexes that may not be used. Unused indexes can be indexes that are neither updated nor read, but this issue is ever more relevant when these unused indexes have updates.
How indexes are chosen significantly affects the amount of disk I/O generated and, subsequently, performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not build any indexes, including a clustering index on a primary key, until a query demonstrates a requirement for it. In general, all columns named in a multi-table query should be covered by an index.

Review the indexes and remove those that are unnecessary, namely those that are incurring updates, but are not used for reading, more so if all the relevant workload business cycles have passed.
For SQL Server 2008 and SQL Server 2005, use the dynamic management view sys.dm_db_index_usage_stats to find unused indexes.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Index-Related Dynamic Management Views and Functions (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms187974.aspx)

SQL Server 7.0 Performance Tuning Guide
(http://support.microsoft.com/kb/322883)

Following is a SQL Server 7.0 document that also applies to SQL Server 2000. For SQL Server 2005 and 2008, ignore the platform-specific information and concentrate on the Index Selection section:
http://support.microsoft.com/kb/322883]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Indexes have been identified that are larger than the recommended size (900 bytes).</Title>
        <Category>Database Design</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[The total size of an index or primary key cannot exceed 900 bytes. With variable length data types, you will be allowed to exceed the 900-byte limit. However, if an actual INSERT or UPDATE operation exceeds the limit, you will receive an error message and the operation will fail.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid exceeding the 900-byte limit when creating indexes in order to avoid INSERT and UPDATE operation failures. Narrower indexes also take up less disk space and require fewer system resources to maintain. To allow for larger indexes without exceeding the index key byte limit, use the INCLUDE option provided in Microsoft SQL Server 2005 and above.

Evaluate how the index is currently being used by querying the sys.dm_db_index_usage_stats dynamic management view. If it is frequently queried, determine the impact of reducing the number of columns or the width of the underlying data types in a development environment. This step will help ensure that the index is still useful to the application queries that are referencing it. If the index is rarely or infrequently used, consider dropping it in a development environment to gauge the impact on the referencing queries.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: CREATE INDEX (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188783.aspx)

SQL Server Books Online: DROP INDEX (Transact-SQL)
(https://msdn2.microsoft.com/en-us/library/ms176118.aspx)

SQL Server Books Online Topic: sys.dm_db_index_usage_stats (http://msdn2.microsoft.com/en-us/library/ms188755.aspx) 
 
SQL Server Books Online Topic : Index with Included Columns (http://msdn2.microsoft.com/en-us/library/ms190806.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>User database is set to compatibility level lower than the default installation level.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Setting the databases to a lower compatibility level should be done only as an interim aid during migration. Keeping the databases at the same level indefinitely will disable some of the new features that later versions provide.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Review the issues that caused you to operate at the lower compatibility level and then upgrade to the default level.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Database Compatibility Level Option (http://msdn.microsoft.com/en-us/library/ms191137.aspx)

SQL Server Books Online: ALTER DATABASE Compatibility Level (Transact-SQL) (http://msdn.microsoft.com/en-us/library/bb510680.aspx)

SQL Server Books Online: sp_dbcmptlevel (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms178653.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases found that have collations different from master/model databases.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[One or more user-defined databases are defined by using a collation that is different from the master and model databases and could cause the collation conflicts that might prevent code from executing or giving unwarranted results.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Microsoft recommends not using collations for user databases different from master and model databases. Therefore, verify if the databases that will be hosted on your SQL Server instance require a specific collation, and install your SQL Server instance with that default collation.

If you experience collation conflict errors, consider one of the following solutions:  
Export the data from the user database and import it into new tables that have the same collation as the master and model databases.
Rebuild the system databases to use a collation that matches the user database collation.
Modify any stored procedures that join user tables to tables in tempdb to create the tables in tempdb by using the collation of the user database. To do this, add the COLLATE database_default clause to the column definitions of the temporary table.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Setting and Changing the Database Collation (http://msdn.microsoft.com/en-us/library/ms175835(SQL.100).aspx)

SQL Server Books Online: Setting and Changing the Column Collation (http://msdn.microsoft.com/en-us/library/ms190920(SQL.100).aspx)

SQL Server Books Online: How to transfer a database from one collation to another collation in SQL Server (http://support.microsoft.com/kb/325335)

ALTER DATABASE (Transact-SQL)	 (http://msdn.microsoft.com/en-us/library/ms174269.aspx)

COLLATE (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms184391.aspx)

The Impact of Changing Collations and of Changing Data Types from Non-Unicode to Unicode Whitepaper (http://download.microsoft.com/download/d/9/4/d948f981-926e-40fa-a026-5bfcf076d9b9/SQL_bestpract_CollationChange.docx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Usage of SQL table compression can be beneficial.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[By not using compression on a SQL Server 2008 or above, and if the server is not CPU bound, this can lead to a higher load on the disk I/O subsystem.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The impact of compression depends on the read/write ratio on a table. This is relatively easy to determine by using the sys.dm_db_index_usage_stats DMV, and looking in the user_seeks, user_scans, user_lookups and user_updates columns.
The more a table is updated, the heavier compression will weigh in the resource loads. So be sure to always test the impact of compression before implementing compression in a production environment. 
On a table, both Page and Row compression is available. Nevertheless, evaluate Page Compression first, and after a load test choose whether the compression method must be changed to Row Compression, or even if there is not available CPU headroom for compression to be used.
Note: Compression is applied on object level, therefore on the same table, it is possible to have compressed and non-compressed indexes.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Data Compression 
(http://technet.microsoft.com/en-us/library/cc280449.aspx)

Data Compression: Strategy, Capacity Planning and Best Practices Whitepaper 
(http://msdn.microsoft.com/en-us/library/dd894051.aspx)

The SQL Swiss Army Knife #6 - Evaluating compression gains (http://blogs.msdn.com/b/blogdoezequiel/archive/2011/01/03/the-sql-swiss-army-knife-6-evaluating-compression-gains.aspx) 

SQL Server Database Compression - speed up your applications without extra programming and complex maintenance (Rene Balzano) (http://blogs.technet.com/b/swisssql/archive/2011/07/09/sql-server-database-compression-speed-up-your-applications-without-programming-and-complex-maintenance.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use the Force Order query hint.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Hints override the query optimizer and affect the resulting query optimizer plan. Thus, hints should be used judiciously and never by default in your code. In addition, misusing hints can result in unsatisfactory query performance and reduced database concurrency. In some cases, misusing hints can also result in data integrity issues.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The query optimizer in Microsoft SQL Server usually selects the best execution plan for a query. Thus, Microsoft recommends that the join, query, table, and view hints be used only as a last resort. Hints can improve performance in the short term, but can cause problems later if the query activity and cardinality of the data changes.

Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Query Hint (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms181714.aspx) 

Microsoft Help and Support: How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server
(http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Microsoft SQL Server staff roles are not clearly defined and filled.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Microsoft SQL Server is an enterprise-level, database server tool and consists of a very subtle and complex series of components and interdependencies. Thus, dedicated and trained staff is essential to ensure successful implementation.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Dedicated SQL Server operations and administration staff should be hired and trained. If development work is ongoing, Microsoft recommends that a SQL Server developer role also be included. In addition, database operation and administration roles should be well defined and published. Each role should also have a staff member assigned to it.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Data Center Availability: Facilities, Staffing, and Operations 
(http://www.microsoft.com/technet/prodtechnol/sql/2000/reskit/part4/c1461.mspx?mfr=true)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that declare an insensitive cursor.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Cursors require substantial overhead to store the information being processed, and rows are processed on an individual basis. Using cursors can increase contention on a system. Queries may also take substantially longer to execute when using cursors than when executing queries without using cursors. Insensitive cursors do not reflect data modifications. They are read-only constructs, and thus do not support updates of the data underlying them.
Furthermore, cursors that have been opened and never closed can also cause performance problems. In extreme cases, orphaned cursors can ultimately consume so much system memory that out-of-memory errors, such as 17803, will occur.
Insensitive cursor will create a temporary table in TEMPDB, which increases overhead and can cause resource contention issues. Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Insensitive cursor will create a temporary table in TEMPDB, which increases overhead and can cause resource contention issues. Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method. 
Here are some alternatives to using a cursor:
Use WHILE LOOPS
Use temp tables
Use derived tables
Use correlated sub-queries
Use the CASE statement
Perform multiple queries

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : DECLARE CURSOR (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms180169.aspx)

Prescriptive Architecture : Chapter 14 "Improving SQL Server Performance" 
(http://msdn2.microsoft.com/en-us/library/ms998577.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that declare a static cursor.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Cursors require substantial overhead to store the information being processed, and rows are processed on an individual basis. Using cursors can increase contention on a system. Queries could also take substantially longer to execute when using cursors than when executing queries without using cursors. Static cursors create a temporary table in tempdb that stores a copy of all the data used by the cursor. Static cursors can increase the amount of contention in tempdb, especially if they are commonly used on a system.
Furthermore, cursors that have been opened and never closed can also cause performance problems. In extreme cases, orphaned cursors can ultimately consume so much system memory that out-of-memory errors, such as 17803, will occur.
Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Insensitive cursor will create a temporary table in TEMPDB, which increases overhead and can cause resource contention issues. Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method. 
Here are some alternatives to using a cursor:
Use WHILE LOOPS
Use temp tables
Use derived tables
Use correlated sub-queries
Use the CASE statement
Perform multiple queries
.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : DECLARE CURSOR (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms180169.aspx)

Prescriptive Architecture : Chapter 14 "Improving SQL Server Performance" 
(http://msdn2.microsoft.com/en-us/library/ms998577.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that declare a cursor, but do not close the cursor.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Closing a cursor releases the current result set and frees any cursor locks that are held on the rows in which the cursor is positioned. Closing a cursor leaves the data structures intact so the cursor can be reopened. However, fetches and positioned updates are not allowed when the cursor is closed. Cursors that have been opened and never closed can cause performance problems. In extreme cases, orphaned cursors can ultimately consume so much memory on the system that out-of-memory errors, such as 17803 errors, will be thrown.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Insensitive cursor will create a temporary table in TEMPDB, which increases overhead and can cause resource contention issues. Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method. 
Here are some alternatives to using a cursor:
Use WHILE LOOPS
Use temp tables
Use derived tables
Use correlated sub-queries
Use the CASE statement
Perform multiple queries
.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: CLOSE (Transact-SQL)
(https://msdn.microsoft.com/en-us/library/ms175035.aspx) 

Prescriptive Architecture : Chapter 14 "Improving SQL Server Performance" 
(http://msdn2.microsoft.com/en-us/library/ms998577.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that declare a cursor, but do not deallocate the cursor.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Closing a cursor releases the current result set and frees any cursor locks that are held on the rows in which the cursor is positioned. However, this leaves the data structures intact so that the cursor can be reopened. The cursor object is not destroyed until the DEALLOCATE CURSOR operation has been performed on it. Cursors that have been opened and never closed or deallocated can cause performance problems. In extreme cases, orphaned cursors can ultimately consume so much memory on the system that out-of-memory errors, such as 17803 errors, will be thrown.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Insensitive cursor will create a temporary table in TEMPDB, which increases overhead and can cause resource contention issues. Cursors should be used only when necessary. Many times, the same tasks can be performed just by using a different method. 
Here are some alternatives to using a cursor:
Use WHILE LOOPS
Use temp tables
Use derived tables
Use correlated sub-queries
Use the CASE statement
Perform multiple queries
.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: DEALLOCATE (Transact-SQL)
(https://msdn.microsoft.com/en-us/library/ms188782.aspx) 

Prescriptive Architecture : Chapter 14 "Improving SQL Server Performance" 
(http://msdn2.microsoft.com/en-us/library/ms998577.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified using Deprecated features.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Deprecated features are scheduled to be removed in a future release of SQL Server and should not be used in new applications.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Monitor your workload for the use of deprecated features by using the SQL Server Deprecated Features Object performance counter and trace events.

Do not use these features in new development work, and modify applications that currently use these features as soon as possible.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Deprecation Announcement Event Class (http://msdn.microsoft.com/en-us/library/ms186302.aspx) 

Deprecation Final Support Event Class (http://msdn.microsoft.com/en-us/library/ms178053.aspx)

Deprecated Database Engine Features in SQL Server 2012 (http://msdn.microsoft.com/en-us/library/ms143729.aspx)

Deprecated Database Engine Features in SQL Server 2008 R2 (http://msdn.microsoft.com/en-us/library/ms143729(v=sql.105).aspx)

Deprecated Database Engine Features in SQL Server 2008 (http://msdn.microsoft.com/en-us/library/ms143729(v=sql.100).aspx)

Deprecated Database Engine Features in SQL Server 2005 (http://msdn.microsoft.com/en-us/library/ms143729(v=sql.90).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use one or more SELECT statements that retrieves all columns of a single table or View.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[SELECT * can cause both performance and application issues. Applications that reference SELECT * that are relying on the ordinal position rather than metadata may encounter errors if the table definition is changed. 
Use of SELECT * can result in inefficient query plans by pulling column sets for which a covering index cannot be created. Depending on the number of rows being accessed by the query, this can create higher numbers of lookups, or even full scans. 
SELECT * is often used in place of manually designating the columns to be viewed. This results in columns being accessed and passed over the network without actually being used by the application.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Always explicitly reference columns in the SELECT clause. For any new query code, always designate column names.

Replace all references to SELECT * with explicit column names instead. If this is a query from a vendor application, contact the vendor about this issue and establish a timeframe for correcting the situation.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : SELECT Clause (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms176104.aspx)
 
SQL Server Books Online: Choosing All Columns 
(http://msdn.microsoft.com/en-us/library/ms189287.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>SELECT statements have been identified that have no WHERE clause.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[The WHERE clause is used to specify which rows are returned by a query. A missing WHERE clause on a large or frequently executed, mid-sized result set can lead to query performance issues and excessive table scanning. Excessive scans can produce both I/O and network throughput bottlenecks.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Queries should be as selective as possible in order to maximize query performance and minimize excessive resource consumption. To encourage index seeks instead of scans, write search conditions in the WHERE clause that reference high cardinality columns.

Evaluate how the query is used and look for opportunities to reduce the result set. Also consider adding a WHERE clause and associated search conditions. In addition, if a WHERE clause cannot be used, consider whether the TOP keyword or ROW_NUMBER can be used to restrict or page through the dataset.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : WHERE (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188047.aspx)
 
SQL Server Books Online : Query Tuning 
(http://msdn2.microsoft.com/en-us/library/ms176005.aspx)
 
SQL Server Books Online : Troubleshooting Poor Query Performance: Cardinality Estimation 
(http://msdn2.microsoft.com/en-us/library/ms181034.aspx)

SQL Server Books Online : TOP (Transact-SQL) 
(http://msdn2.microsoft.com/en-us/library/ms189463.aspx)
 
SQL Server Books Online : ROW_NUMBER (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms186734.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The stored Procedure Naming convention "sp_" is being used.</Title>
        <Category>Database Design</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The "sp_" programmability object naming convention has special meaning and functionality in Microsoft SQL Server.  
User-created programmability objects should never be created with names identical to those of Microsoft SQL Server system stored procedures (e.g., "sp_who", "sp_lock", "sp_monitor", "sp_datatype_info", etc.).For these "special" stored procedures, SQL Server will always execute the copy in master. 
If a user-created programmability object is created in the master database and its name begins with "sp_", it is accessible from other databases via a one-part call. However, say a programmability object with the same "sp_" name is created in a user database. A one-part call to that object from that database will execute the local copy rather than the one in master. If one intends to create these objects in a user database but instead creates them in master, this can lead to unanticipated results, especially if it is intended to create different objects in different databases with the same name. 
For these reasons, it is recommended to entirely avoid the practice of giving "sp_" names to user-created programmability objects.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not use the "sp_" prefix when naming user-defined objects. In addition to the possibility of performance issues, the name you select could ultimately conflict with actual system object names used by Microsoft in future versions of SQL Server.

Examine all references to stored procedures that have the "sp_" naming convention prefix and rename the stored procedures with a new naming convention. If the stored procedures are vendor-written, contact the vendor about this issue and work with them to arrange a resolution date. Microsoft also recommends that you address all objects before upgrading to a new version of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_rename (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188351.aspx) 

SQL Server Books Online: Creating Stored Procedures (Database Engine) (http://msdn.microsoft.com/en-us/library/ms190669.aspx) 

SR0016: Avoid using sp_ as a prefix for stored procedure (http://msdn.microsoft.com/en-us/library/dd172115.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use the NOLOCK hint and perform data modification.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Hints override the query optimizer and affect the resulting query optimizer plan. Thus, hints should be used judiciously and never by default in your code. In addition, misusing hints can result in unsatisfactory query performance and reduced database concurrency. In some cases, misusing hints can also result in data integrity issues. 
In particular, the NOLOCK hint suppresses the issuance of shared locks, enabling "dirty" reads (reads from uncommitted transactions). This approach can have serious implications for data concurrency, and is strongly discouraged.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The query optimizer in Microsoft SQL Server usually selects the best execution plan for a query. Microsoft recommends that the join, query, table, and view hints be used only as a last resort. Hints can improve performance in the short term, but can cause problems later if the query activity and cardinality of the data changes.

Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Table Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187373.aspx)
 
SQL Server Books Online: Join Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173815.aspx)
 
SQL Server Books Online: Query Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms181714.aspx)

How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server (http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Table Hint specified without the WITH clause</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[With some exceptions, table hints are supported in the FROM clause only when the hints are specified with the WITH keyword. Table hints also must be specified with parentheses. Omitting the WITH keyword is a deprecated feature: This feature will be removed in a future version of Microsoft SQL Server.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Use the WITH keyword when specifying table hints
Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Table Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187373.aspx)
 
SQL Server Books Online: Join Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173815.aspx)
 
SQL Server Books Online: Query Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms181714.aspx)

How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server (http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>At least one paging file is set to System Managed Size.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The paging file (Pagefile.sys) is a hidden file on your computer's hard disk that Windows uses as if it were random access memory (RAM). The paging file and physical memory make up virtual memory. By default, Windows stores the paging file on the boot partition (the partition that contains the operating system and its support files).]]>
        </Impact>
        <Recommendation>
            <![CDATA[When a WS 2003 or WS 2008 page file is configured to System Managed Size, depending on the free space on the disk, it would calculate the page file size with Initial Size =1.5 *RAM and Max Size =3*RAM.

As the amount of RAM in a computer increases, the need for a large page file decreases. Depending on the RAM in your machine, you must have up to:
2 GB of paging file available on the boot volume in the case of x86 WS 2003.
8 GB of paging file available on the boot volume in the case of x64 WS 2003, the minimum to get the kernel memory dump.
WS 2008 does not use the paging file to perform kernel memory dumps.

However, setting page file configuration to System Managed Size ensures that you are unlikely to encounter page file-related resource depletion. This can lead to severe disk and page file fragmentation as the page file continuously shrinks and expands to keep up with the needs of the system. 

If you are in a situation where there is severe disk fragmentation and you have a dynamic page file, Microsoft strongly recommends reconfiguring the server with a static page file. You will also want to make sure that the disk is properly defragmented.

Determine the minimum and maximum page file size, using the following rule of thumb: 
Minimum page file = sum of peak private bytes that are used by each process on the system. Then, subtract the amount of memory on the system. 
Maximum page file = sum of peak private bytes that are used by each process on the system. Then, add a margin of additional space. Do not subtract the amount of memory on the system. The size of the additional margin can be adjusted based on your confidence in the snapshot data that is used to estimate page file requirements. 

To set the paging file to Custom Size, follow these steps:
Click Start, right-click Computer, and then select Properties.
Select Advanced System Settings.
Under Performance, click Settings.
Click the Advanced tab.
In Virtual Memory, click Change. If the option, Automatically manage paging file size for all drives, is selected in a Windows 2008 operating system, uncheck the option.
Select the appropriate drives.
Choose Custom size.

Set Initial Size (MB) and Maximum Size (MB) as appropriate.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to determine the appropriate page file size for 64-bit versions of Windows Server (http://support.microsoft.com/kb/889654)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use the Merge Join hint.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Hints override the query optimizer and affect the resulting query optimizer plan. Thus, hints should be used judiciously and never by default in your code. In addition, misusing hints can result in unsatisfactory query performance and reduced database concurrency. In some cases, misusing hints can also result in data integrity issues.
In particular, the MERGE JOIN query or join hint specifies that JOIN operations in the query be completed using the merge join method.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The query optimizer in Microsoft SQL Server usually selects the best execution plan for a query. Microsoft recommends that the join, query, table, and view hints be used only as a last resort. Hints can improve performance in the short term, but can cause problems later if the query activity and cardinality of the data changes.

Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Table Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187373.aspx)
 
SQL Server Books Online: Join Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173815.aspx)
 
SQL Server Books Online: Query Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms181714.aspx)

How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server (http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use the Loop Join hint.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Hints override the query optimizer and affect the resulting query optimizer plan. Thus, hints should be used judiciously and never by default in your code. In addition, misusing hints can result in unsatisfactory query performance and reduced database concurrency. In some cases, misusing hints can also result in data integrity issues.
In particular, the LOOP JOIN query or join hint specifies that JOIN operations in the query be completed using the nested loop join method.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The query optimizer in Microsoft SQL Server usually selects the best execution plan for a query. Microsoft recommends that the join, query, table, and view hints be used only as a last resort. Hints can improve performance in the short term, but can cause problems later if the query activity and cardinality of the data changes.

Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Table Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187373.aspx)
 
SQL Server Books Online: Join Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173815.aspx)
 
SQL Server Books Online: Query Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms181714.aspx)

How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server (http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use the Hash Join hint.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Hints override the query optimizer and affect the resulting query optimizer plan. Thus, hints should be used judiciously and never by default in your code. In addition, misusing hints can result in unsatisfactory query performance and reduced database concurrency. In some cases, misusing hints can also result in data integrity issues.
In particular, the HASH JOIN query or join hint specifies that JOIN operations in the query be completed using the hash join method.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The query optimizer in Microsoft SQL Server usually selects the best execution plan for a query. Microsoft recommends that the join, query, table, and view hints be used only as a last resort. Hints can improve performance in the short term, but can cause problems later if the query activity and cardinality of the data changes.

Determine the cause of the hint usage and test whether the hint is still overriding the original behavior that was intended. If possible, remove the hint from the code. If this is not possible, document the location and background of the hint so you can evaluate the benefits of using it in future service packs and versions of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Table Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187373.aspx)
 
SQL Server Books Online: Join Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173815.aspx)
 
SQL Server Books Online: Query Hint (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms181714.aspx)

How to Troubleshoot the Performance of Ad-Hoc Queries in SQL Server (http://support.microsoft.com/kb/243588/)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>SQL Server objects have been identified that use Negative Logic in TSQL Syntax.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Negative logic can limit overall query performance. Negative logic includes TSQL syntax such as:
!<
!=
!><>
NOT EXISTS
NOT IN
NOT LIKE
NOT BETWEEN
Furthermore, it introduces additional contention because it often results in evaluation of each row (index scans) in order to determine if a search condition is met for the query.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid using NOT LIKE in search expressions. Instead, elect to use inclusive equals and range queries. In addition, construct your WHERE clause to reference high-cardinality, indexed columns.

It is not always possible to rewrite negative logic. However, if you are experiencing excessive scanning and I/O issues, rewriting the search conditions can help improve query performance significantly. Evaluate search conditions that use negative logic and try to find more inclusive conditions for filtering the result set. The key objective is to reduce the number of rows that must be evaluated by Microsoft SQL Server. Focus on search conditions that affect as few rows as possible and evaluate the query execution plan to ensure that SQL Server is using index seeks whenever possible.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Search Condition (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173545.aspx)
 
SQL Server Books Online: Comparison Operators (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms188074.aspx)
 
SQL Server Books Online: Logical Operators (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms189773.aspx)
 
SQL Server Books Online: Displaying Execution Plans by Using the Showplan SET Options (Transact-SQL) 
(http://msdn.microsoft.com/en-us/library/ms180765.aspx)
 
SQL Server Books Online: Displaying Graphical Execution Plans (SQL Server Management Studio) 
(http://msdn.microsoft.com/en-us/library/ms178071.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use T-SQL reserved words or special characters as object names.</Title>
        <Category>Database Design</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[SQL Server reserves certain keywords for its exclusive use. No user-defined objects in the database should be given a name that matches a reserved keyword. Also, avoid using special characters in object names, such as spaces or mathematical symbols.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid using reserved keywords for user-defined objects. If you identify objects using reserved keywords, rename them in order to avoid issues with future versions of SQL Server.

Rename objects that use reserved keywords. If you cannot modify the object immediately, the object must always be referred to using delimited identifiers. However, you should still plan to rename the object before updating to a new version of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : Reserved Keywords (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms189822.aspx) 

SQL Server Books Online : sp_rename (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188351.aspx)

SR0011: Avoid using special characters in object names (http://msdn.microsoft.com/en-us/library/dd172134.aspx)

SR0011: Avoid using reserved words for type names (http://msdn.microsoft.com/en-us/library/dd193421.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Objects have been identified that use T-SQL reserved words or special characters as column names.</Title>
        <Category>Database Design</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[SQL Server reserves certain keywords for its exclusive use. No user-defined objects or columns in the database should be given a name that matches a reserved keyword.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid using reserved keywords for column names. If you identify columns using reserved keywords, rename them in order to avoid issues with future versions of SQL Server.

Rename columns that use reserved keywords. If you cannot modify the column name immediately, the column must always be referred to using delimited identifiers. However, you should still plan to rename the column before updating to a new version of SQL Server.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : Reserved Keywords (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms189822.aspx) 

SQL Server Books Online : sp_rename (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188351.aspx)

SR0011: Avoid using special characters in object names (http://msdn.microsoft.com/en-us/library/dd172134.aspx)

SR0011: Avoid using reserved words for type names (http://msdn.microsoft.com/en-us/library/dd193421.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>User objects have been identified in the Master database.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The master database records all the system-level information for a SQL Server system. This includes instance-wide metadata such as logon accounts, endpoints, linked servers, and system configuration settings. In SQL Server, system objects are no longer stored in the master database; instead, they are stored in the Resource database. Also, master is the database that records the existence of all other databases and the location of those database files and records the initialization information for SQL Server. Therefore, SQL Server cannot start if the master database is unavailable.
Therefore, creating user objects in the Master database creates unwarranted and possibly unstable interference with this system database.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not create user objects in master. If you do, master must be backed up more frequently. When you put user objects into master, you may be able to access them without database context. However, this is essentially unintended behavior  SQL Server will check for some objects (depending on the object name and the version of SQL Server) in the master database before checking the current database context. Do not design your processes around this behaviour.

Relocate all user objects outside the Master database, and invoke them using their two part name (<schema>.<name>) in the context database.

]]></Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : Master Database 
(https://msdn.microsoft.com/en-us/library/ms187837.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Use of the GOTO T-SQL syntax has been detected.</Title>
        <Category>T-SQL Coding</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[GOTO is a T-SQL programming construct that allows processing to pass to another location in the stored procedure. However, using it contributes to poor performance in systems because of the overhead involved as well as the ordinary design of the procedures that employ it.]]>
        </Impact>
        <Recommendation>
            <![CDATA[You should be able to build a more effective and maintainable system by using alternative approaches to meet the coding requirement. 
We recommend, whenever possible, to not use GOTO, and change to constructs such as a TRY CATCH block. 
Excessive use of the GOTO statement can make it difficult to understand the logic of a Transact-SQL batch. The logic implemented using GOTO can almost always be implemented using the other control-of-flow statements. GOTO is best used for breaking out of deeply nested control-of-flow statements

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : Using GOTO
(http://msdn.microsoft.com/en-us/library/ms188729.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Sparse files were detected that do not belong to a Database Snapshot.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Only files that belong to database snapshots are expected to be sparse. However, this also occurs in certain scenarios, such as:
After you migrate a database from Windows Server 2003 to a later version of a Windows operating system.
If a 3rd party tool (such as a backup agent), uses alternate streams and incorrectly sets database files as sparse.
When running DBCC CHECKDB in SQL Server below Cumulative update package 1 for SQL Server 2008 Service Pack 3, cumulative update package 16 for SQL Server 2008 Service Pack 1, cumulative update package 6 for SQL Server 2008 Service Pack 2, cumulative update package 9 for SQL Server 2008 R2 or cumulative update package 2 for SQL Server 2008 R2 Service Pack 1, and the file system incorrectly marks a whole database data file as sparse after an alternative stream that was originally marked as sparse is removed. The alternative stream is removed when a DBCC CHECKDB command ends.
You might also notice unexplained performance degradation when query data from these files.]]>
        </Impact>
        <Recommendation>
            <![CDATA[To determine whether the database data files are marked as sparse in the SQL Server metadata, run the following statement, and then check whether the Is_Sparse column is set to a nonzero value in the result set:

Use <database name>
select is_sparse , physical_name from sys.database_files

To query the status of the physical files in the file system, run the following Windows command after the database is offline:

fsutil sparse queryflag 

<files path>\<database file>

If you receive the following output, the file is marked as sparse in the file system:

This file is set as sparse.

Both the SQL Server metadata and the file system metadata must be marked as sparse only for database data files that are part of a database snapshot.

If you are above the recommended build level for SQL Server, then for any files that are not snapshot databases, the data needs to be copied out of the file, then drop the file, create a new file and load the data.  I.E.: Transfer your data to a new file.

]]></Recommendation>
        <Reading>
            <![CDATA[FIX: Database data files might be incorrectly marked as sparse in SQL Server 2008 R2 or in SQL Server 2008 even when the physical files are marked as not sparse in the file system
(http://support.microsoft.com/kb/2574699) 

SQL Server database files are incorrectly marked with the sparse attribute
(http://support.microsoft.com/kb/2028447)

Did your backup program/utility leave your SQL Server running in an squirrely scenario? (Version 2)
(http://blogs.msdn.com/b/psssql/archive/2011/02/21/did-your-backup-program-utility-leave-your-sql-server-running-in-an-squirrely-scenario-version-2.aspx) 

How It Works: SQL Server Sparse Files (DBCC and Snapshot Databases) Revisited
(http://blogs.msdn.com/b/psssql/archive/2009/01/20/how-it-works-sql-server-sparse-files-dbcc-and-snapshot-databases-revisited.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Suspect pages have been identified.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[SQL Server contains logic to evaluate a page when it is retrieved that determines if the page is in a damaged state. If a page is damaged during a write operation, check values throughout the data page will not be in agreement. SQL Server will then write a message to the error log stating that a torn or damaged page has been detected. 
These errors not only indicate the existence of corrupt data, but generally indicate that there is a hardware or packet filter driver problem on the system. If the problem is allowed to continue unchecked, irrecoverable damage can occur to the data on the system.]]>
        </Impact>
        <Recommendation>
            <![CDATA[In general, you should first try to back up the databases on the system. Note that corrupted data will also be backed up, but the primary reason for the backup is to protect the uncorrupted data on the system, in the event the corruption begins to spread. Restoring the affected database from a recent backup is the only substantial way to recover from this type of situation. 
 
In addition, you should also thoroughly inspect the hardware, especially the disks, disk controllers, cables, and drivers. Processes that use packet filter technologies (antivirus software and third-party backups) should also be evaluated to ensure that they are not intercepting Microsoft SQL Server writes and that their drivers are up to date. 
 
DBCC CHECKDB should be run against all databases on the server to verify the health of the pages on those systems as well. Both 823 and 824 errors are recorded when the pages are read into memory from the disk. It is possible for corrupted pages to go unreported through normal means simply because they have not been read into memory recently. Running DBCC CHECKDB will help to proactively uncover this type of issue and also determine the extent of damage when corrupted pages are found.

Restoring the affected database from a recent backup is the only substantial way to recover from a situation where torn pages have been detected. The underlying cause should be resolved before restoring the data. Otherwise, the issue will only continue.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Suspect_pages (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms174425.aspx)

Understanding and Managing the suspect_pages Table (http://msdn.microsoft.com/en-us/library/ms191301.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>There is no page file configured.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The paging file (Pagefile.sys) is a hidden file on your computer's hard disk that Windows uses as if it were random access memory (RAM). The paging file and physical memory make up virtual memory. By default, Windows stores the paging file on the boot partition (the partition that contains the operating system and its support files).]]>
        </Impact>
        <Recommendation>
            <![CDATA[When programs on your computer request for an allocation of virtual memory space, they may demand a way more than they ever could possibly bring into use.  If there no page file available, the system assigns the RAM and it would waste the physical memory usage. 
Though the Windows 2003 designed to run without paging file configured, it is not recommended to configure with "No page file" unless you are  certain that your system has enough physical memory than the server would ever needed. 
In heavy load scenarios if the server runs out of physical RAM, the server may become non-responsive and may need a hard reboot.

Determine the minimum and maximum page file size, using the following rule of thumb: 
Minimum page file = sum of peak private bytes that are used by each process on the system. Then, subtract the amount of memory on the system. 
Maximum page file = sum of peak private bytes that are used by each process on the system. Then, add a margin of additional space. Do not subtract the amount of memory on the system. The size of the additional margin can be adjusted based on your confidence in the snapshot data that is used to estimate page file requirements. 

To set the paging file to Custom Size, follow these steps:
Click Start, right-click Computer, and then select Properties.
Select Advanced System Settings.
Under Performance, click Settings.
Click the Advanced tab.
In Virtual Memory, click Change. If the option, Automatically manage paging file size for all drives, is selected in a Windows 2008 operating system, uncheck the option.
Select the appropriate drives.
Choose Custom size.

Set Initial Size (MB) and Maximum Size (MB) as appropriate.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to determine the appropriate page file size for 64-bit versions of Windows Server (http://support.microsoft.com/kb/889654)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Antivirus software exclusion list is incorrectly configured.</Title>
        <Category>Windows Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Although running antivirus software is essential for protecting your data and systems, failure to exclude Microsoft SQL Server directories from antivirus scans can cause both performance issues and corruption. 
Antivirus software can lock files while scanning them, thus causing blocking and contention when the files being locked are needed for SQL Server operations and queries. 
Corruption can be caused when antivirus software checks the information being written to disk on the fly by intercepting the packets and checking them. This can cause corruption if the data is quarantined or written out to an area of the disk that SQL Server is unaware of. Subsequent calls for these data pages may return old or completely incorrect data if the information was not written where SQL Server expects to find it. The delay in writing the data due to the scanning process could also cause stale data to be read from the disk if the data is requested again before the antivirus software has finished scanning and writing out the data.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Follow these best practices to prevent antivirus software from causing performance and corruption issues: 

The following files or folders should be excluded from virus scanning: 

SQL Server data and transaction log files (these usually have the extensions .mdf, .ldf and .ndf).
Database backup (.bak) and transaction log backup files (.trn).
The folder used for FILESTREAM storage in SQL Server 2008 and above.
Full-Text catalog files in a default instance: Program Files\Microsoft SQL Server\MSSQL\FTDATA
Full-Text catalog files in a named instance: Program Files\Microsoft SQL Server\MSSQL$instancename\FTDATA
Trace files (these files usually have the extension .trc)
SQL query files (these files usually have the extension .sql)
For SQL Server 2008 or later versions, SQL audit files (these files usually have the extension .sqlaudit)
The directory that holds Analysis Services data.
The directory that holds Analysis Services temporary files that are used during Analysis Services processing.
Analysis Services backup files.
Directories for any Analysis Services 2005 and later-version partitions that are not stored in the default data directory.

Also, the following processes should be excluded from virus scanning:

In a SQL Server 2008 R2 instance:

%ProgramFiles%\Microsoft SQL Server\MSSQL10_50.<Instance Name>\MSSQL\Binn\SQLServr.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL10_50.<Instance Name>\Reporting Services\ReportServer\Bin\ReportingServicesService.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL10_50.<Instance Name>\OLAP\Bin\MSMDSrv.exe

In a SQL Server 2008 instance:

%ProgramFiles%\Microsoft SQL Server\MSSQL10.<Instance Name>\MSSQL\Binn\SQLServr.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL10.<Instance Name>\Reporting Services\ReportServer\Bin\ReportingServicesService.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL10.<Instance Name>\OLAP\Bin\MSMDSrv.exe

In a SQL Server 2005 instance:

%ProgramFiles%\Microsoft SQL Server\MSSQL.1\MSSQL\Binn\SQLServr.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL.3\Reporting Services\ReportServer\Bin\ReportingServicesService.exe
%ProgramFiles%\Microsoft SQL Server\MSSQL.2\OLAP\Bin\MSMDSrv.exe

Additionally, if SQL Server is on a clustered installation, Microsoft recommends that the following also be excluded from antivirus scanning: 

In a Windows 2003 Cluster environment: 

\MSCS directory in the quorum drive 
\MSDTC directory in the MSDTC share drive 
C:\Windows\Cluster directory on each cluster node 

In a Windows 2008 Cluster environment: 

Avoid virus scanning on the disk witness (quorum) if used. 
\MSDTC directory in the MSDTC share drive 
·  %Systemroot%\Cluster folder on each cluster node

]]></Recommendation>
        <Reading>
            <![CDATA[Guidelines for choosing antivirus software to run on the computers that are running SQL Server (http://support.microsoft.com/kb/309422)

Antivirus Software May Cause Problems with Cluster Services (http://support.microsoft.com/kb/250355)

You may experience performance and consistency issues with SQL Server when certain modules are loaded into SQL Server address space (http://support.microsoft.com/kb/2033238)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>DBCC CHECKDB has not been run within seven days.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The frequency in which DBCC CHECKDB should be run against any particular database depends largely on the individual business needs and the importance of the information in the database. However, at a minimum, DBCC CHECKDB should be run against all production databases at least once a week.
DBCC CHECKDB should also be run on a regular basis to proactively identify inconsistencies in data structures and other types of corruption.
Failure to resolve the problems detected by DBCC CHECKDB in a timely manner can lead to significant data loss and to the server failing to respond.]]>
        </Impact>
        <Recommendation>
            <![CDATA[DBCC CHECKDB should be run regularly. In addition, the results should be reviewed as soon as possible after execution completes in order to identify and resolve any errors before they become critical.
If DBCC CHECKDB uncovers any errors, it will specify the minimum repair level that should be used to fix the problem. Note that running DBCC CHECKDB with the repair option might not resolve the issue, depending on the type of damage uncovered. This could potentially result in a significant loss of data.
DBCC CHECKDB in Microsoft SQL 2005 uses an internal database snapshot to perform the checks in order to avoid blocking and concurrency problems. However, because SQL Server 2005 performs much more extensive checks than earlier versions, it may take considerably longer to complete. Thus, Microsoft recommends that the PHYSICAL_ONLY option be used for frequent checks on production databases.
Because DBCC CHECKDB performs the following checks, you do not need to run these commands:

DBCC CHECKALLOC
DBCC CHECKTABLE
DBCC CHECKCATALOG

Plan maintenance cycles that include tasks such as index defragmentation, integrity checking, history cleanup and other tasks you deem important for your server.

Alternatively, leverage the scripts shared here: http://blogs.msdn.com/b/blogdoezequiel/archive/2012/09/18/about-maintenance-plans-grooming-sql-server.aspx.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: DBCC CHECKDB (Transact-SQL) (https://msdn2.microsoft.com/en-us/library/ms176064.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases need data purity check.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[For databases created in SQL Server 2005 and later, column-value integrity checks are enabled by default and do not require the DATA_PURITY option. 
For databases upgraded from earlier versions of SQL Server, column-value checks are not enabled by default until DBCC CHECKDB WITH DATA_PURITY has been run error free on the database. 
After this, DBCC CHECKDB checks column-value integrity by default.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Use the following command to check data purity for a given database.
Under the context of the database, run : DBCC CHECKDB WITH DATA_PURITY
For our recommendations on implementing automated maintenance tasks, from integrity checking to index defrag, among other maintenance tasks, please refer the following article: http://blogs.msdn.com/b/blogdoezequiel/archive/2012/09/18/about-maintenance-plans-grooming-sql-server.aspx
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: DBCC CHECKDB (Transact-SQL) (https://msdn2.microsoft.com/en-us/library/ms176064.aspx)

Troubleshooting DBCC error 2570 in SQL Server 2005 and later versions (http://support.microsoft.com/kb/923247)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases found with most recent full database backup older than 7 days or that have never been backed up.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[To recover a database to the time of failure or to a specific point in time, you must restore the most recent full database backup, restore the most recent differential database backup, restore all transaction log file backups that are more recent than the last full or differential database backup, and manually initiate recovery. The time required to fully recover a database depends on the number and size of these backup files. To decrease the number of these files and improve the total recovery time, it is important to perform frequent full database backups.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Perform a full database backup as often as is practical to reduce the amount of data that must be recovered in addition to the most recent full-database backup. The recommended frequency of a full database backup is every day but also depends on the time required to perform a full backup and the impact of performing it on the datacenter. The longer the time between each full database backup, the more data has to be recovered from differential and transaction log backup files if the failure occurs just before the next full database backup. The more data that has to be recovered from differential and transaction log backup files, the longer the total recovery time.

Estimate the time it takes to back up the database using the command:  
backup database [Database name] to disk ='null' 
Estimate the size of the full database backup using sp_spaceused system stored procedure.
After you decide what types of backups you require and how frequently you have to perform each type, we recommend that you schedule regular backups as part of a database maintenance plan for the database.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Introduction to Backup and Restore Strategies in SQL Server  (http://msdn.microsoft.com/en-us/library/ms191239(v=SQL.100).aspx) 

Backup More Than 1GB per Second Using SQL2008 Backup Compression (http://blogs.msdn.com/b/sqlcat/archive/2008/03/02/backup-more-than-1gb-per-second-using-sql2008-backup-compression.aspx)

A Technical Case Study: Fast and Reliable Backup and Restore of Multi-Terabytes Database over the Network
(http://download.microsoft.com/download/d/9/4/d948f981-926e-40fa-a026-5bfcf076d9b9/Technical%20Case%20Study-Backup%20VLDB%20Over%20Network_Final.docx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>User Databases found with no Transactional log backups that are in Full recovery mode.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The transaction log is a serial record of all the transactions that have been performed against the database since the transaction log was last backed up. With transaction log backups, you can recover the database to a specific point in time (for example, prior to entering unwanted data), or to the point of failure.]]>
        </Impact>
        <Recommendation>
            <![CDATA[On production servers, databases with the recovery option set to "Full", frequent transaction log backups are essential, to avoid filling up the transaction log of a database. Under the simple recovery model, log truncation occurs automatically after you back up the database, but under the full recovery model, log truncation occurs after you back up the transaction log

Also, important to note is that a database in Full Recovery Model will undergo transaction log truncation on checkpoint (similar to Simple Recovery Model) until the first database backup is taken. After the database is backed up, it requires a log backup to truncate the log in full recovery mode. Once a full database backup has been completed, all log records that have not been backed up must remain in the log to preserve the log chain for possible media recovery until the log is backed up.

We recommend that you take log backups frequently enough to keep your work-loss exposure within the confines required by your business requirements. Log backup frequency should ideally be driven by written SLA related RPO agreements rather than technical considerations. Sometimes a transaction log backup can be larger than a database backup. For example, a database having a high transaction rate can cause the transaction log to grow quickly. In this situation, create transaction log backups more frequently

Transaction log backups generally use fewer resources than database backups. As a result, you can create them more frequently than database backups. Frequent backups decrease your risk of losing data.

Configure a job to perform transaction log backups in regular intervals to meet your Recovery Point Objective, or set the recovery model to Simple if there is no need for transaction log backups.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Introduction to Backup and Restore Strategies in SQL Server (http://msdn.microsoft.com/en-us/library/ms191239.aspx)

Backup Under the Full Recovery Model (http://msdn.microsoft.com/en-us/library/ms191164.aspx)

The transaction log may grow without a log backup for a database using the FULL recovery model (http://support.microsoft.com/kb/2523744)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Some disk partitions are not using a minimum recommended alignment offset of 64KB.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Disk performance may be slower than expected when you use disks in Windows Server 2003 and in Windows Server 2000. This issue may occur if the starting location of a partition is not aligned with the Stripe Unit size and Allocation Unit size (or Cluster size) in the disk partition that is created on RAID. 
A 64-KB offset (128 sectors) is a common value that works on many storage arrays and has been used with success in the SQL Server test labs (at the time of writing this paper). 
Partitions may contain hidden sectors and the total length of these may not coincide with a full track. For example, there are 63 hidden sectors in the MBR (Master Boot Record) so the first block of user data is written across the first and second tracks starting in the 64th sector, misaligning the entire partition. Subsequently, instead of one IOP (I/O operation) to read or write data, two IOPs are required because if the disk is not aligned, every Nth read or write crosses a boundary, and the physical disk must perform two operations. Hence, the 64-KB offset alignment value. Windows Vista and Windows Server 2008 remove the need to manually establish correct alignment and use 1024-KB (2048 sectors) as a starting offset, which will probably work well for almost any array, although this must be subject to the storage vendor approval. Note that Windows Server 2008 and Windows Server 2008 R2 the offset still defaults to 64-KB if the disk is smaller than 4 GB]]>
        </Impact>
        <Recommendation>
            <![CDATA[Align LUN partition with the underlying disk clusters. A brand new Windows Server 2008 install is not affected, but if the OS is upgraded in-place from Windows Server 2003 to Windows Server 2008, you will still experience this problem, because reconfiguring the hard disk offset setting requires the partitions to be re-created and reformatted. Therefore, this should be completed before any data is loaded.Work with your hardware vendor to optimally configure the hard drives that support RAID devices.

Alignment cannot be controlled using Windows Disk Manager. Diskpar.exe could be used to create aligned partitions and was included in Windows 2000™ Resource Kit. 
From Windows Server 2003 SP1 onward, Diskpart.exe is used instead of Diskpar.exe, and contains an ALIGN option:

Diskpart
list disk
select disk <DiskNumber>
create partition primary align=<Offset_in_KB>
assign letter=<DriveLetter>
format fs=ntfs unit=64K label="<label>" nowait

]]></Recommendation>
        <Reading>
            <![CDATA[Disk Partition Alignment Best Practices for SQL Server (http://msdn.microsoft.com/en-us/library/dd758814.aspx) 

Predeployment I/O Best Practices (http://technet.microsoft.com/en-us/library/cc966412.aspx)

A Description of the Diskpart Command-Line Utility (http://support.microsoft.com/kb/300415)

How to Use Diskpar.exe (http://technet.microsoft.com/en-us/library/bb643096(EXCHG.80).aspx)

Recommendations and Guidelines on configuring disk partitions for SQL Server (http://support.microsoft.com/kb/2023571)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>DATA or LOG partitions are formatted using the default NTFS cluster size.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[SQL Server data pages are 8192 bytes, so the default NTFS block size (4096 bytes) reads only half a page and effectively doubles the number of I/O operations. Also, consider formatting the DATA drives in 64-KB blocks, because SQL Server commonly does an eight page read-ahead to improve performance during range and table scans. If using the FILESTREAM feature, also format the volume that supports it with a 64-KB block size.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Format the DATA and LOG partitions with NTFS 64-KB block size. 
Engage your hardware vendor to optimally configure these values as they could diverge on a specific SAN or Vendor.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Disk Partition Alignment Best Practices for SQL Server (http://msdn.microsoft.com/en-us/library/dd758814.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Instant File Initialization is not used.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Using the Instant File Initialization feature will allow a boost in performance, because when extending or creating a data file (does not apply to log files), it will not be zeroed right away, saving time and I/O cycles.
This permission does come with a small security risk, because by not zeroing out the existing space, when deleting data for example, there is a possibility that data could still be read, even though it has been deleted, until some other data writes on that specific area of the data file. 
However, the performance benefits outweighs the security risk and hence the reason for this recommendation.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

This can be accomplished by granting the "Perform volume maintenance tasks" permission to the SQL Server account, which is set in the security policy console (secpol.msc).

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How and Why to Enable Instant File Initialization (http://blogs.msdn.com/b/sql_pfe_blog/archive/2009/12/23/how-and-why-to-enable-instant-file-initialization.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Lock Pages in Memory right not granted.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The user account the SQL Server service is running on was not added to the Lock Page in Memory policy (User Rights Assignments) in the Local Security Policy of the computer. Enabling this user right ensures that memory in use by SQL Server is not paged out when other applications or the OS demands more memory.
This does not affect SQL Servers dynamic memory management, allowing it to expand or shrink at the request of other memory clerks. When using the Lock Pages in Memory user right it is recommended to set an upper limit for max server memory.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Set the Lock Pages in Memory permission to the SQL Server account in Windows Server 2003 running on an x64 system. On SQL Server 2005 and above over Windows Server 2008 this assumes a much lower importance due to optimized memory management, and thus should only be used when necessary, namely if there are signs that sqlservr process is being paged out.

Consider granting the SQL Server service account the Lock Pages in Memory user right and setting an upper limit for max server memory.
Note that for 64-bit editions of SQL Server only Enterprise Edition can use the Lock pages in memory user right. This is applicable for SQL Server 2005 (RTM, SP1, SP2, SP3) and for SQL Server 2008 (RTM and SP1). 
SQL Server 2008 SP1 CU2 and SQL Server 2005 SP3 CU4 introduce support for SQL Server Standard editions to use the Lock pages in memory user right, that also add the trace flag 845, that must be set up as a start-up parameter to support page locking.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to reduce paging of buffer pool memory in the 64-bit version of SQL Server (http://support.microsoft.com/kb/918483)

Enable the Lock Pages in Memory Option (Windows) (http://msdn.microsoft.com/en-us/library/ms190730.aspx)

Support for Locked Pages on SQL Server 2008 R2 Standard Edition x64, on SQL Server 2005 Standard Edition 64-bit systems, and on SQL Server 2008 Standard Edition 64-bit systems (http://support.microsoft.com/kb/970070)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Trace Flag 2335 is not set on a SQL Server with a Max Server Memory setting of 100GB or more.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Trace flag 2335 can alleviate a known issue with using large amounts of memory, and a poor plan being chosen.]]>
        </Impact>
        <Recommendation>
            <![CDATA[When you configure max server memory sp_configure option in Microsoft SQL Server to a large value, you may notice that a particular query may run slowly. But if you lower the value for this option, the same query may run much faster. In this case, enable trace flag 2335 as a startup parameter.

This trace flag will cause SQL Server to generate a plan that is more conservative in terms of memory consumption when executing the query. It does not limit how much memory SQL Server can use.
Use SQL Server Configuration Manager to edit the startup parameters of SQL Server, adding the correct trace flag number with the T option, ensuring the trace flag is active after a server restart. Meanwhile, to enable the trace flag using the DBCC TRACEON command, with the -1 flag to enable it globally:
DBCC TRACEON (2335, -1)
Please ensure that you thoroughly test this option, before rolling it into a production environment.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Using large amounts of memory can result in an inefficient plan in SQL Server (http://support.microsoft.com/kb/2413549/en-us) 

Database Engine Service Startup Options (http://msdn.microsoft.com/en-us/library/ms190737.aspx)

DBCC TRACEON (http://msdn.microsoft.com/en-us/library/ms187329.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Paging file usage is high.</Title>
        <Category>Windows Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[The paging file (Pagefile.sys) is a hidden file on your computer's hard disk that Windows uses as if it were random access memory (RAM). The paging file and physical memory make up virtual memory. By default, Windows stores the paging file on the boot partition (the partition that contains the operating system and its support files).]]>
        </Impact>
        <Recommendation>
            <![CDATA[If the page file usage is above 70% of the total page file size or the page file usage is more than 4 gigabytes (GB), engage in root cause analysis to identify the processes that are causing paging activity. Sometimes, Windows pages out the SQL Server working set because of external memory pressure, or because a rogue process is causing memory leaks. On occasion, it could also be genuine memory pressure.

When the SQL Server working set pages out, you may see the warning messages in SQL Server error log. When this happens follow some of these steps: 
Investigate the Windows and SQL Server latest hotfixes for any known issues that cause the high degree of paging activity and apply those fixes.  
There could be several other reasons for excessive paging in SQL Server environment. Please follow the following KB link to reduce paging in 64-bit environments: How to reduce paging of buffer pool memory in the 64-bit version of SQL Server (http://support.microsoft.com/kb/918483)
As more physical RAM is added to a computer, the need for a page file decreases. Please see the following article to determine the appropriate page file sizes in your environment:
How to determine the appropriate page file size for 64-bit versions of Windows Server (http://support.microsoft.com/kb/889654)

]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Trace Flag 4199 is not set on SQL Server to control multiple query optimizer changes.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Trace flag 4199 enables all the fixes that were previously made for the query processor under many trace flags.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Because of the policy change to put query processor fixes under a trace flag, even if you are running with the latest hotfix or cumulative update installed, you are not necessarily running SQL Server with all the latest query processor fixes enabled. The fixes are only enabled by using a trace flag. To enable these fixes, trace flag 4199 has to be enabled.
Trace flag 4199 was implemented in the following versions of SQL Server:
SQL Server 2008 R2 and above
SQL Server 2008 Service Pack 2 (SP2)
SQL Server 2008 Service Pack 1 (SP1) Cumulative Update 7
SQL Server 2008 Cumulative Update 7
SQL Server 2005 Service Pack 3 (SP3) Cumulative Update 6

Use SQL Server Configuration Manager to edit the startup parameters of SQL Server, adding the correct trace flag number with the T option, ensuring the trace flag is active after a server restart. Meanwhile, to enable the trace flag using the DBCC TRACEON command, with the -1 flag to enable it globally:
DBCC TRACEON (4199, -1).
Alternatively, the query-level option "QUERYTRACEON" is available. This option lets you to enable a plan-affecting trace flag only during single-query compilation, such as TF 4199.
Please ensure that you thoroughly test this option in a test environment before rolling it into a production environment.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Trace flag 4199 is added to control multiple query optimizer changes previously made under multiple trace flags (http://support.microsoft.com/kb/974006)

Enable plan-affecting SQL Server query optimizer behavior that can be controlled by different trace flags on a specific-query level (http://support.microsoft.com/kb/2801413)

DBCC TRACEON (http://msdn.microsoft.com/en-us/library/ms187329.aspx)

Database Engine Service Startup Options  (http://msdn.microsoft.com/en-us/library/ms190737.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Trace Flag 8048 is not set on a SQL Server with 8 processors or more per NUMA node.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Newer hardware with multi-core CPUs can present more than 8 CPUs within a single NUMA node. Microsoft has observed that when you approach and exceed 8 CPUs per node the NODE based partitioning may not scale as well for specific query patterns.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The issue is commonly identified by looking as the DMVs sys.dm_os_wait_stats and sys.dm_os_spin_stats for types (CMEMTHREAD and SOS_SUSPEND_QUEUE). We usually see the spins jump into the trillions and the waits become a hot spot.

Using trace flag 8048, all NODE based partitioning is upgraded to CPU based partitioning. Remember this requires more memory overhead but can provide performance increases on these systems.
Use SQL Server Configuration Manager to edit the startup parameters of SQL Server, adding the correct trace flag number with the T option, ensuring the trace flag is active after a server restart. Please ensure that you thoroughly test this option, before rolling it into a production environment.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server 2008/2008 R2 on Newer Machines with More Than 8 CPUs Presented per NUMA Node May Need Trace Flag 8048 (http://blogs.msdn.com/b/psssql/archive/2011/09/01/sql-server-2008-2008-r2-on-newer-machines-with-more-than-8-cpus-presented-per-numa-node-may-need-trace-flag-8048.aspx)

Database Engine Service Startup Options (http://msdn.microsoft.com/en-us/library/ms190737.aspx)

DBCC TRACEON (http://msdn.microsoft.com/en-us/library/ms187329.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Trace Flag 2371 is not set on SQL Server to fine tune AUTOSTATS threshold.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The auto update statistics feature of SQL Server relies on number of rows changed or updated to determine if statistics update is needed. The statistics of a table will only be automatically updated if the number of rows changed exceed a threshold, and when AUTO_UPDATE_STATISTICS is ON. These thresholds are:
If the table cardinality was 500 or less at the time statistics were created, update every 500 modifications.
If the table cardinality was above 500 at the time statistics were created, update every 500 + 20% of modifications.
It is essential to update the statistics on a regular basis because out-of-date statistics can severely influence query performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[When a table becomes very large, the default threshold (a fixed rate  20% of rows changed) may be too high and the Autostat process may not be triggered frequently enough. This could lead to potential performance problems. SQL Server 2008 R2 Service Pack 1 and later versions introduce trace flag 2371 that you can enable to change this default behavior. The higher the number of rows in a table, the lower the threshold will become to trigger an update of the statistics.

Using trace flag 2371, the threshold to trigger update statistics will be calculated based on the number of rows in the table. However, the table still needs to have a minimum of 500 rows as the minimum row threshold did not change.
Use SQL Server Configuration Manager to edit the startup parameters of SQL Server, adding the correct trace flag number with the T option, ensuring the trace flag is active after a server restart.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Controlling Autostat (AUTO_UPDATE_STATISTICS) behavior in SQL Server (http://support.microsoft.com/kb/2754171)

Database Engine Service Startup Options (http://msdn.microsoft.com/en-us/library/ms190737.aspx)

DBCC TRACEON (http://msdn.microsoft.com/en-us/library/ms187329.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>I/O taking longer than 15 seconds.</Title>
        <Category>SQL Errors</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This message indicates that SQL Server has issued a read or write request from disk, and that the request has taken longer than 15 seconds to return. This error is reported by SQL Server and indicates a problem with the IO subsystem. This error can occur in SQL Server 2000 or SQL Server 2005.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Troubleshoot this error by examining the system event log for hardware-related error messages. Also, examine hardware-specific logs if they are available. 
Use Performance Monitor to examine the following counters:  
Average Disk Sec/Transfer
Average Disk Queue Length
Current Disk Queue Length
For example, the Average Disk Sec/Transfer time on a computer that is running SQL Server is typically less than 15 milliseconds. If the Average Disk Sec/Transfer value increases, this indicates that the I/O subsystem is not optimally keeping up with the I/O demand. 
Note: Disk access can be slowed by an antivirus program. To increase access speed, exclude the SQL Server data files that are specified in the error message from active virus scans.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Diagnostics in SQL Server 2000 SP4 and in later versions help detect stalled and stuck I/O operations
(http://support.microsoft.com/kb/897284/en-us)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>A significant part of SQL server process memory has been paged out.</Title>
        <Category>SQL Errors</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This issue can be caused by multiple factors, and there are several ways trimming can occur: 
 
Signalled Trim - The low physical memory indicator event is set by the operating system when physical memory becomes low. The SQL Server Resource Monitor, Lazy Writer, and other processes decommit portions of the BPool memory. This returns physical memory to the system and, in turn, lowers the working set of the SQL Server. When the event is signalled, the sys.dm_os_ring_buffer and RING_BUFFER_RESOURCE_MONITOR records are logged. This is the normal working behavior of the SQL Server as it tries to remain RAM resident. This activity does not trigger the error log message. 

Self-Trim - The operating system is handling a page fault for a newly allocated page and determines physical memory is low. Thus, the current working set should be trimmed. This is done by exchanging a new page in RAM and moving an older page to the page file. For example, if SQL Server is allocating a page for a stack, linked server, or other allocation and physical memory is in a low state, the operating system may directly trim the working set of the SQL Server. On current operating system builds, such as Windows 2003 SP2 and Windows XP SP2, the amount to trim can be ~1/4 the current working set size. On a 64-gigabyte (GB) system, the SQL Server working set could page out 16 GB in less than a second. A large trim as this does not occur on Windows Vista and we are looking at fixes for Windows 2003 and XP to avoid such dramatic trimming behavior. 

Hard Trim - When the system gets into very low memory conditions, a hard trim can occur. This activity can page out the entire working set for SQL Server and other processes on the box. 
 
The signalled trim allows SQL Server to properly remove the oldest references from the BPool (LRU) and maintain optimal performance for the overall server load. The Self Trim and Hard Trim activities can trigger the SQL Server error log message, thus indicating that a significant portion of the SQL Server memory has been placed in the page file. This generally leads to performance problems.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Set the Lock Pages in Memory permission to the SQL Server account in Windows Server 2003 running on an x64 system. On SQL Server 2005 and above over Windows Server 2008 this assumes a much lower importance due to optimized memory management, and thus should only be used when necessary, namely if there are signs that sqlservr process is being paged out.

To resolve this issue, follow these steps: 
 
Make sure that no other processes are quickly consuming the memory. A key to this is watching the Memory: Total Working Set Performance Counter. If you start an application that quickly uses memory, all working sets can be trimmed quickly. However, the overall _Total remains steady. If the _Total drops significantly, it is a good indication that it is not a single application that is quickly consuming memory, but instead something like MiEmptyWorkingSet or MmAllocateContigiousMemory has taken place and the operating system has decided to engage in a hard trim. The Microsoft Platform Support team can assist you in determining why the hard trim is being triggered. 
Make sure the SQL Server process ID remains constant across the problem time. We have determined that if you stop the SQL Server service and restart it, some of the SQL Server counters remain constant. This could be misleading. 
Make sure your system has the latest drivers. We have specifically encountered several drivers on X64 that trigger hard trim behavior. 
Make sure the OS Terminal Service bug is not causing the problem. For more information, see http://support.microsoft.com/default.aspx?scid=kb;EN-US;905865. When an administrator accesses the server console remotely (such as through a Remote Desktop Connection) and then later logs off, the winlogon process might initiate trimming the processes running in Session 0. The post Windows 2003 Service Pack 1 hotfix resolves this issue. On the server, you can match this to a winstations, SECURITY, or event log entry for the System - Event ID 682. Following is an example of one: 

Session reconnected to winstation: 
 
User Name:USERNAME 
Domain:MYDOMAIN 
Logon ID:(0x0,0x12A8229) 
Session Name:RDP-Tcp#3 
Client Name:MYCLIENT 
Client Address:615.513.165.351 
 
Capture a full set of performance counters as well as the sys.dm_os_ring_buffers from SQL Server. For example, when an application on the computer starts using memory quickly, the ring buffer entries clearly show the low physical memory indicator. When a self or hard trim occurs, the low physical memory indicator is not set, or is only set for a short period. In addition, when compared to the performance counters, the behavior pattern is clarified. The self and hard trim can require Microsoft Platforms support to help determine root cause. 
Apply the operating system fix described in the article, http://support.microsoft.com/default.aspx?scid=kb;EN-US;920739. The dirty page threshold is set at half the amount of physical memory. This has caused severe problems on systems with large amounts of memory when applications use buffered I/O. In addition, it leads to trimming and to large amount of data being flushed to disk when the threshold value is hit. The registry key must be set as recommended in the article, along with the application of the provided hotfix. 
Apply the operating system hotfix described in the article, http://support.microsoft.com/default.aspx?scid=kb;EN-US;931308. You may experience increased paging to the hard disk when you run a program on a Windows Server 2003-based computer. 
Apply the latest SQL Server 2005 Service Pack. Beginning with SP2, there are a few changes that allow SQL Server to react to low physical memory faster. SP2 also includes additional sys.dm_os_ring_buffer information.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to reduce paging of buffer pool memory in the 64-bit version of SQL Server 
(http://support.microsoft.com/kb/918483) 

Enable the Lock Pages in Memory Option (Windows) (http://msdn.microsoft.com/en-us/library/ms190730.aspx)

How to enable the "locked pages" feature in SQL Server 2012
(http://support.microsoft.com/kb/2659143)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Network Interface library could not register the Service Principal Name (SPN) for the SQL Server service.</Title>
        <Category>SQL Errors</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[When you register an SPN for a SQL Server service, you essentially create a mapping between an SPN and the Windows account that started the server instance service. 
You must register the SPN because the client must use a registered SPN to connect to the server instance. The SPN is composed by using the servers computer name and the TCP/IP port. If you do not register the SPN, the SSPI cannot determine the account that is associated with the SPN. Therefore, Kerberos authentication will not be used.  
 
When SQL Server is running under the local system account or under a domain administrator account, the instance will automatically register the SPN in the following format when the instance starts: 
 
MSSQLSvc/FQDN:tcpport 
  
Note: FQDN is the fully qualified domain name of the server, while tcpport is the TCP/IP port number.  
 
Because the TCP port number is included in the SPN, SQL Server must enable the TCP/IP protocol for a user to connect by using Kerberos authentication. The same rules apply for clustered configurations. Additionally, if the instance automatically registered an SPN when the instance started, the SPN will be unregistered automatically when the instance stops. 
 
Only a domain administrator account or the local system account has the required permissions to register an SPN. Therefore, if the SQL Server service is started under a non-administrator account, SQL Server cannot register the SPN for the instance. This behavior will not prevent the instance from starting. However, the following message will be logged in the Application log of the Windows Event log: 
 
Event Type: Information  
Event Source: MSSQL$InstanceName 
Event Category: (2)  
Event ID: 26037  
Date: Date 
Time: Time 
User: N/A  
Computer: ComputerName 
Description: The SQL Network Interface library could not register the Service Principal Name (SPN) for the SQL Server service. Error: 0x54b. Failure to register an SPN may cause integrated authentication to fall back to NTLM instead of Kerberos. This is an informational message. Further action is only required if Kerberos authentication is required by authentication policies. 
 
For more information, see Help and Support Center at http://support.microsoft.com. 
 
If this message is logged, you must manually register the SPN for the instance under a domain administrator account to use Kerberos authentication. To register the SPN, you can use the SetSPN.exe tool that is included with the Microsoft Windows 2000 Server Resource Kit. This tool is also included with the Microsoft Windows Server 2003 Support Tools. The Windows Server 2003 Support Tools are included in Microsoft Windows Server 2003 Service Pack 1 (SP1).  
 
For more information about how to obtain the Windows Server 2003 Service Pack 1 Support Tools, refer to the following article: 
 
Windows Server 2003 Service Pack 1 Support Tools 
(http://support.microsoft.com/kb/892777)
 
You can use a command that is similar to the following to register an SPN for an instance: 
SetSPN -A MSSQLSvc <ComputerName><DomainName>:1433 <AccountName>
  
Note: If an SPN already exists, you must delete the SPN before you can reregister it. You may have to do this if the account mapping has changed. To delete an existing SPN, you can use the SetSPN.exe tool together with the -D switch. 

How to Ensure You Are Using Kerberos Authentication   
After you connect to an instance of SQL Server 2005, run the following Transact-SQL statement in SQL Server Management Studio: 
select auth_scheme from sys.dm_exec_connections where session_id=@@spid 
 
If SQL Server is using Kerberos authentication, a character string that is listed as KERBEROS appears in the auth_scheme column in the Result window.]]></Impact>
        <Recommendation>
            <![CDATA[

To use Kerberos authentication, you must make sure that all the following conditions are true:  
 
Both the server and the client computers must be members of the same Windows domain or members of trusted domains.  
The server's SPN must be registered in the Active Directory directory service.
The instance of SQL Server 2005 must enable the TCP/IP protocol.  
 
The client must connect to the instance of SQL Server 2005 by using the TCP/IP protocol. For example, you can put the TCP/IP protocol at the top of the client's protocol order. Alternatively, you can add the prefix "tcp:" in the connection string to specify that the connection will use the TCP/IP protocol.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to make sure that you are using Kerberos authentication when you create a remote connection to an instance of SQL Server 2005 (http://support.microsoft.com/kb/909801)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The recommended service pack or hotfix for SQL Server is not installed.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Service packs are the main delivery vehicle for fixes, security patches, and general improvements to the SQL Server system. These updates can protect you from as well as provide you with solutions to known issues. Therefore, applying service packs and hotfixes as soon as possible after thorough testing can greatly reduce your vulnerability to a wide range of issues. Additionally, because service packs often include improvements to performance supportability, and diagnostics, having the latest service packs installed can improve response in general and can reduce the amount of time necessary to diagnose and troubleshoot an issue.
Finally, support for older Microsoft SQL Server versions and service packs could be discontinued, thus leaving your system in an unsupported configuration. Thus, it is essential to apply all service packs as soon as possible.
Over time, a Cumulative Update (CU) package is created by the development team to address specific product issues affecting certain customers. These CU builds contain all fixes since the product or service pack is released as stated in the KB article associated with a specific CU package release. Sometimes the issue has a broad customer impact, security implications, or both. Thus, a General Distribution Release (GDR) is issued so that all customers can receive the updates.]]>
        </Impact>
        <Recommendation>
            <![CDATA[After the availability of a service pack is announced, it should be applied to a test region of the environment. It should also be subjected to a comprehensive testing program that involves all the applications in the environment that use SQL Server.
In addition, the tests should be conducted in such a way that a load similar to, if not equal to, the loads experienced in production are run against the test system.
Hotfixes should only be installed when specifically instructed by Microsoft Support.
Cumulative Updates (CU) are only recommended for installation when specific symptoms are experienced on your SQL Server, if you are running QFE branch, review the issues fixed in the KB article associated with the release of CU and apply if your sever is experiencing any of those symptoms

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Where to find information about the latest SQL Server builds (http://support2.microsoft.com/kb/957826)

An Incremental Servicing Model is available from the SQL Server team to deliver hotfixes for reported problems
(http://support.microsoft.com/kb/935897)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>There are fewer tempdb files than the number of processors, the files are not equally sized or number of files is not a multiple of 4.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Having a poorly designed tempdb data file configuration can lead to several performance problems.]]>
        </Impact>
        <Recommendation>
            <![CDATA[When the tempdb files are set to grow with a percentage value, this could lead to cumulative performance impact on the instance if the calculated growth is a large amount. Or for ramp-up times when the calculated growth is initially too small, causing a large number of auto growths to take place.
Another common pitfall is when there are not enough tempdb data files. Having multiple tempdb data files can reduce contention and improve performance on active systems. This is because there will be one or more SGAM pages for each file, the main point of contention for mixed allocations. A common action is to check how many online schedulers there are for the instance, and compare that number to the amount of tempdb data files. To gain the maximum benefit from using multiple tempdb data files, Microsoft recommends that you create 1 file per each 2 processors, up to 8 files (mileage may vary). However, all the files must be the same size. If there is a need to increase above the initial 8 files, do so by multiples of 4 files.

To gain the maximum benefit from using multiple tempdb data files, Microsoft recommends that you create one file per each 2 processors up to 8 files (mileage may vary). For example, on very large systems (16 or 32 processors), 8 tempdb files may be sufficient, but reconsider this based on the workload. However, all the files must be the same size. Therefore ensure a suitable and fixed growth rate for tempdb database files.
Placing each file on its own disk or spindle will further increase performance by reducing disk I/O contention, although it is not necessary for the reduction of contention on the GAM and SGAM pages.
Having too many tempdb files can also be a drawback, because every object in tempdb will have multiple IAM pages. If allocation contention is occurring, consider adding more data files, but in multiples of 4.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[FIX: Concurrency enhancements for the tempdb database (http://support.microsoft.com/default.aspx?scid=kb;EN-US;328551)

Recommendations to reduce allocation contention in SQL Server tempdb database
(http://support.microsoft.com/kb/2154845/en-us)

Storage Top 10 Best Practices
(http://technet.microsoft.com/en-us/library/cc966534.aspx)

SQL Server Books Online: Optimizing TempDB Performance
(http://msdn.microsoft.com/en-us/library/ms175527.aspx)

Capacity Planning for tempdb
(http://msdn.microsoft.com/en-us/library/ms345368.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Data and log files are not on separate drives or tempdb files and user database files are not on separate drives.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Data and log files should be placed on separate drives. Placing both data and log files on the same device can cause contention for that device and result in poor performance. Placing the files on separate drives allows the I/O activity to occur at the same time for both the data and log files.
Placing the tempdb files on the same drives or disks as of the user databases may result in poor performance in the environments where tempdb is used heavily.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Access to data files is random while access to log files is sequential. Having data and log files on the same physical drive decreases log write performance. This is because the drive heads have to shift between doing a sequential write to a random read or write, and then back to a sequential write again.
When you create a new database, specify separate drives for the data and log files. To move files after the database is created, the database must be taken offline. Put the tempdb database on a fast I/O subsystem. Use disk striping if there are many directly attached disks. Put the tempdb database on disks that differ from those that are used by user databases.

Monitor the I/O activity of your drives during peak load to determine if there is disk contention. Typically, Microsoft recommends separating log files, database files, backup files and tempdb files onto separate drives. Strive to allocate as many disk spindles as possible to each.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SDB405 - SQL Server 2000 - I/O and Disk Management Internals (http://support.microsoft.com/kb/283037/)

DAT320 - SQL Server 2000: Implementing large databases on SAN technology (http://sqldev.net/download/conf/ITForum-2004-11-DK/DAT320.zip)

SQL Server Books Online: Moving Database Files (http://msdn2.microsoft.com/en-gb/library/ms189133.aspx)

SQL Server 2000 Operations Guide: Monitoring and Control: Moving User Databases (http://www.microsoft.com/technet/prodtechnol/sql/2000/maintain/sqlops5.mspx)

SQL Server Books Online: Detaching and Attaching Databases (http://msdn2.microsoft.com/en-us/library/ms190794.aspx)

Microsoft SQL Server I/O subsystem requirements for the tempdb database (http://support.microsoft.com/kb/917047)

Optimizing tempdb Performance (http://msdn.microsoft.com/en-us/library/ms175527.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>User database files and Backup files are not on separate drives.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Data/Log Files and Backup files should be placed on separate drives. Placing database backups on volumes that contain database files can slow down overall performance while backups are running. Placing database and backup files on the same volume also creates a single point of failure. If the volume becomes unavailable, both the database and its backups will be lost.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Place database and backup files on separate drives in order to avoid having a single point of failure and also to avoid contention when backups are running.

Microsoft recommends separating log files, database files, backup files and tempdb files onto separate drives. Strive to allocate as many disk spindles as possible to each.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Best Practice recommendations for SQL Server Database Backups (http://support2.microsoft.com/kb/2027537)

Optimizing Backup and Restore Performance in SQL Server (http://msdn.microsoft.com/en-us/library/ms190954.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Pagefile location or size does not allow for Kernel Memory Dump.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[A kernel memory dump records only the kernel memory. This speeds up the process of recording information in a log when your computer unexpectedly stops. A kernel dump contains only the kernel-mode read / write pages present in physical memory at the time of the crash. Since this is a kernel-mode only dump, there are no pages belonging to user-mode processes.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The size of a kernel memory dump will vary based on the amount of kernel-mode memory allocated by the Operating System and the drivers that are present on the system. Depending on the RAM in your computer, you must have up to 2 GB of paging file available on the boot volume in the case of 32-bit Windows Operating system environments. 
 On a 64 bit machine the Virtual address space is quite large and the kernel memory on those machines could potentially go to the size of the physical memory. But from the past Microsoft support experience with kernel memory dumps on 64-bit environments we provide the below guidelines: 
A kernel only dump on midrange machines would grow up 8 GB and recommend paging file size of at least 8GB. 
A kernel only dump on a high end machine would grow up to 16 GB and paging file of the size around 16 GB. 
With the new behavior in Windows 2008, the paging file does not have to be on the same partition as the partition on which the operating system is installed.

Determine the minimum and maximum page file size, using the following rule of thumb:
Minimum page file = sum of peak private bytes that are used by each process on the system. Then, subtract the amount of memory on the system. 
Maximum page file = sum of peak private bytes that are used by each process on the system. Then, add a margin of additional space. Do not subtract the amount of memory on the system. The size of the additional margin can be adjusted based on your confidence in the snapshot data that is used to estimate page file requirements.
The size of a kernel memory dump will vary based on the amount of kernel mode memory allocated by the operating system and the drivers that are present on the system. 
Depending on the RAM in your machine, you must have up to 2 GB of paging file available on the boot volume in the case of x86 Windows Server 2003. 
If your SQL Server is hosted on x64 Windows Server 2003, you may want to configure the page file size to at least 8 GB to get the kernel memory dump.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, optimize for ad-hoc workloads, has NOT been changed from the default value of 0.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[While the Optimize for ad-hoc workloads server option effectively adds a small delay in a second execution, the benefits outweigh the cost in an OLTP environment: if a query is executed countless times, the query plan only gets stored in cache on the second execution, which is probably not a major drawback. Conversely, if a query plan is cached but used only once (single-use), memory is saved by not caching query plans that will not get re-used.]]>
        </Impact>
        <Recommendation>
            <![CDATA[With this option enabled, when a query executes the first time, only the query hash in stored in cache. If the same plan is reused, then it is deemed fit for storing the entire plan in cache.

Find the number of single-use plans that are cached in SQL Server using the following script:

-- number of cached plans with usecounts = 1.
SELECT objtype, cacheobjtype, AVG(usecounts) AS Avg_UseCount, SUM(refcounts) AS AllRefObjects, SUM(CAST(size_in_bytes AS bigint))/1024/1024 AS Size_MB
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype LIKE '%Plan%' AND usecounts = 1
GROUP BY objtype, cacheobjtype
GO

-- number of cached plans with usecounts > 1.
SELECT objtype, cacheobjtype, AVG(usecounts) AS Avg_UseCount_perPlan, SUM(refcounts) AS AllRefObjects, SUM(CAST(size_in_bytes AS bigint))/1024/1024 AS Size_MB
FROM sys.dm_exec_cached_plans
WHERE cacheobjtype LIKE '%Plan%' AND usecounts > 1
GROUP BY objtype, cacheobjtype
GO

If the number of single-use plans take a significant portion of SQL Servermemory in an OLTP server, and these plans are Ad-hoc plans, use the optimize for ad-hoc workloads server option, available since SQL Server 2008, to decrease memory usage with these objects.
If the number of single-use plans take a significant portion of SQL Servermemory, and these plans are Prepared plans, although parameterization is taking place, widely varying parameters do not allow plan re-use. In this case, consider using stored procedures. Additionally, you might also consider clearing the SQL Plan cache (where ad-hoc, auto parameterized and prepared plans are cached) on a regular basis using DBCC FREESYSTEMCACHE('SQL Plans').

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: optimize for ad hoc workloads Option
(http://msdn.microsoft.com/en-us/library/cc645587.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, remote admin connections, has NOT been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[This configuration enables remote connections to the dedicated administrator connection (DAC) on a Microsoft SQL Server instance. By default, this option is disabled and allows only a local client connection to the DAC.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If you do not install client utilities on the server or you are running SQL Server on a cluster, you should change the default to a value of 1. Otherwise, if you have SQL Server on a stand-alone computer, you might still want to enable the DAC connection for remote access, because if SQL Server is unresponsive and the DAC listener is not enabled, you might have to restart SQL Server to connect with the DAC.

Leave this option disabled if you want to restrict DAC access to only the local client on the stand-alone server. If your SQL Server instance is on a cluster or if you want to use the DAC connection remotely, you should enable this option.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188787.aspx)
SQL Server Books Online: remote admin connections Option
(http://msdn2.microsoft.com/en-us/library/ms190468.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, max server memory, has not been set.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This setting configures the maximum amount of buffer pool memory for the Microsoft SQL Server instance. The default setting is 2,147,483,647. If you set this value too high, a single instance of SQL Server might have to compete for memory with other SQL instances hosted on the node or computer. However, setting this value too low could cause significant memory pressure and performance problems.
In the case of NUMA configurations, if SQL Server is configured to run on a subset of the available NUMA nodes, use the max server memory setting to preferably limit the buffer pool to the memory available in the assigned NUMA nodes.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Consider capping this configuration for each running SQL instance. Monitor overall consumption of the SQL Server process in order to determine memory requirements. 
To be more accurate with these calculations, we need to take some memory to the OS itself as above, then take the equivalent of potential SQL memory allocations outside the Max Server Memory setting, which are comprised of stack size * calculated max worker threads + -g startup parameter (or 256MB by default). What remains should be the Max Server Memory setting for a single instance setup.
Additionally, SQL Server should exist on a dedicated cluster or computer and should not be collocated with other memory-intensive applications.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188787.aspx)

SQL Server Books Online: Server Memory Options
(http://msdn2.microsoft.com/en-us/library/ms178067.aspx)

Effects of min and max server memory 
(http://msdn.microsoft.com/en-us/library/ms180797.aspx)

Growing and Shrinking the Buffer Pool Under NUMA (http://msdn.microsoft.com/en-us/library/ms345403.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, min server memory, has not been set on a clustered instance or VM.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This setting configures the minimum amount of buffer pool memory for the Microsoft SQL Server instance. The default setting is 0.]]>
        </Impact>
        <Recommendation>
            <![CDATA[In a failover cluster configuration, when several instances can exist concurrently in the same node, set the min_server_memory parameter instead of max_server_memory for the purpose of reserving memory for an instance.
Setting a min_server_memory value is essential in a VM environment to ensure memory pressure from the Host does not attempt to deallocate memory from the buffer pool on a guest SQL Server beyond what is needed for acceptable performance.

In a test environment where a workload like production can be simulated, set Max server memory to the same in production. Continue reducing Max Server Memory in controlled increments while simulating a production workload until there are notable performance degradation issues in query durations. Also monitor SQL Server Memory counters for internal memory pressure:
Page Life Expectancy begins to drop. 
Lazy Writer Thread is active
Increase in Checkpoint Pages/sec

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL)
(http://msdn2.microsoft.com/en-us/library/ms188787.aspx)

SQL Server Books Online: Server Memory Options
(http://msdn2.microsoft.com/en-us/library/ms178067.aspx)

Effects of min and max server memory (http://msdn.microsoft.com/en-us/library/ms180797.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, min memory per query, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[This option determines the minimum amount of memory in kilobytes (KB) that will be allocated for query execution. The default of 1024 KB is usually sufficient as the minimum amount.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The default value is usually sufficient. However, if you encounter significant query volumes, increase the value of min memory per query value. This can help with the performance of memory-bound queries.
However, setting the value too high can cause significant performance issues. This is because queries will be required to wait for the minimum memory to become available.

In most cases, this configuration should be left at the default value. If you have to configure this value, configure it first in a test environment in order to measure its effect. In addition, when selecting a minimum value, be careful not to set it too high

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Additional references:
SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 

SQL Server Books Online: min memory per query Option (http://msdn2.microsoft.com/en-us/library/ms181047.aspx) 

SQL Server Books Online: Optimizing Server Performance Using Memory Configuration Options (http://msdn2.microsoft.com/en-us/library/ms177455.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, priority boost, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Enabling this option sets the Microsoft SQL Server instance at a higher scheduling priority on the operating system. This option rarely results in better performance on a SQL Server instance and can reduce the overall stability of the server.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not enable this option on a stand-alone computer unless directed to do so by a Microsoft support professional. In addition, never enable this on a cluster.

If this option is enabled, you should disable it. This option only helps performance in rare and very specific situations.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx)

SQL Server 2008 Books Online: priority boost Option (http://msdn2.microsoft.com/en-us/library/ms180943.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, remote query timeout, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This option defines the timeout duration in seconds for remote queries originating from the source Microsoft SQL Server instance. The default is 600 seconds (10 minutes).]]>
        </Impact>
        <Recommendation>
            <![CDATA[Usually, the default value for this setting is a sufficient amount of time. Setting this option too low could result in premature timeouts. However, setting this option too high could result in excessive query wait times.

In most cases, this configuration should be left at the default value. If a non-default value is required, be careful about making the value too high or low.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 

SQL Server Books Online: remote query timeout Option (http://msdn2.microsoft.com/en-us/library/ms177457.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, cost threshold for parallelism, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This configuration indicates the threshold at which Microsoft SQL Server will execute a parallel plan for a query. The default value for this configuration is 5 and is appropriate for most SQL Server instances. Thus, it rarely needs to be configured.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Leave this configuration at the default value, unless a performance benefit was confirmed in a test environment with similar hardware and SQL instance configurations.

Do not change the default value without first testing the effect across your application queries in a test environment.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 

SQL Server Books Online: cost threshold for parallelism Option (http://msdn2.microsoft.com/en-us/library/ms188603.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, cross db ownership chaining, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[When this option is enabled, cross-database ownership chaining is enabled across all databases on the Microsoft SQL Server instance. Enabling this option also leads to security threats because high security data could be intentionally or inadvertently exposed. For example, database owners can create objects that can target other database objects. If these objects have the same owner across databases, SQL Server will not check permissions on the target object.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not enable this option if cross-database security is not required. If it is enabled, ensure that you can trust all highly privileged database users with data in each database.

Leave this option disabled in order to ensure maximum data security. If you have to enable it, ensure that you can trust database owners and members of the db_owner and db_ddladmin database roles.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: cross db ownership chaining Option (http://msdn2.microsoft.com/en-us/library/ms188694.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High disk fragmentation was found.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Because physical data access is the most expensive part of an I/O request, defragmentation can provide performance gains for SQL Server and other applications. Positioning related data close to each other reduces I/O operation requirements.
Also, physical fragmentation can be avoided avoid by not growing the files in very small increments, but just the appropriate amount that is not over 1GB increments. For this, it is recommended to change the default model database file grow settings, as well as any existing user database in the SQL Server instance
Furthermore, avoiding recurrent file shrink/grow operations will also avoid incurring in physical file fragmentation.]]>
        </Impact>
        <Recommendation>
            <![CDATA[When you evaluate a defragmentation utility for use with SQL Server, the utility should provide transactional data capabilities. Use defragmentation utilities that provide the following transactional data capabilities.
The original sector should not be considered moved until the new sector has been successfully established and the data successfully copied. 
The utility should protect against a system failure, such as a power outage, in a safe way that enables the files to remain logically and physically intact. To guarantee data integrity, a pullthe-plug test is highly recommended when a defragmentation utility is running on a SQL Server-based file.
The Write-Ahead Logging (WAL) protocol requires the prevention of sector re-writes to avoid data loss. The utility must maintain the physical integrity of the file as long as it does any data movement. In fact, it should work on sector boundaries in a transactional way to keep the SQL Server files correctly intact. 
The utility should provide appropriate locking mechanisms to guarantee that the file retains a consistent image for any modifications. For example, the original sector cannot be modified when it is copied to a new location. Therefore, a defragmentation utility could lose the write if modifications are allowed.
Make sure that you understand any write-caching strategies that the utility uses. Caching by such a utility might involve a non-battery-backed cache and could violate WAL protocol requirements.

We always recommend performing a full backup before you defragment those locations that contain SQL Server database and backup files.

We do not recommend Open-file defragmenting. Open-file defragmenting affects performance and defragmentation utilities may lock sections of the file, preventing SQL Server from completing a read or write operation. This can affect the concurrency of the server that is running SQL Server.
Because open-file defragmenting can affect write caching and ordering these components may change the ordering or intended nature of the write operation. If the write-through or WAL protocol tenants are broken, database damage is likely to occur.

As such, we recommend stopping SQL Server for any disk defragmentation activity. Alternatively, copying out all the files to a different disk, deleting from the original location, and copying back the same files will also achieve the same purpose of defragmenting files.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server I/O Basics (http://technet.microsoft.com/en-us/library/cc917726.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, default trace enabled, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This configuration is on by default and ensures that a default trace log is running default trace log files. In most cases, this option should be enabled.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The default trace is an excellent troubleshooting data source in the event of unexpected outages, performance issues, and application events. In most cases, this option should be enabled.

Leave this option at the default setting. However, you should temporarily disable this option if you are encountering space issues on \MSSQL\LOG or are encountering significant performance issues.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: default trace enabled Option:   (http://msdn.microsoft.com/en-us/library/ms175513(SQL.100).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, recovery interval (min), has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This configuration sets the recovery time interval in minutes, per database, that Microsoft SQL Server requires to roll transactions forward. The default value of 0 indicates that SQL Server will automatically manage the value. This default value translates to a recovery time of less than one minute.]]>
        </Impact>
        <Recommendation>
            <![CDATA[SQL Server uses checkpoints to flush log information and changed pages from memory to disk. After the data is flushed to disk, these modifications no longer need to be rolled forward. This then reduces the overall database recovery time. In general, you should leave this configuration at the default value, unless you are experiencing performance issues as a result of excessive checkpoint activity on a busy database.

Leave this configuration at the default value. You should modify it only if frequent checkpoints are causing performance issues for a database. If you have to configure this value, do it slowly and incrementally. However, setting the value too high could result in a prolonged database recovery time.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: recovery interval Option (http://msdn.microsoft.com/en-us/library/ms191154(SQL.100).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, network packet size (B), has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This configuration sets the packet size bytes used in network communication. This configuration should be changed only under well-tested scenarios. The default packet size is 4 kilobytes (KB). 
For most applications, the default packet size of 4,096 bytes is the best, but if an application does bulk copy operations, or sends or receives large amounts of text or image data, a larger packet size can improve efficiency, because it results in less network I/O. Also, the .NET Framework Data Provider for SQL Server sets a default of 8,000 bytes for network packets.
Also, the networking layer (typically TCP/IP) will break up packets into MTU (Max Transmission Unit) size, which is typically 1437 bytes or smaller, so there is no gain in increasing beyond 8KB, because an 8KB TDS (Tabular Data Stream) packet is broken into a series of smaller TCP/IP packets. Also keep in mind that SQL Server cannot handle network packets larger than 16,388 bytes (16K) in environments where encryption is enabled anyway.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Leave this configuration at the default value. You should modify it only after thoroughly testing throughput with the applications that are using the Microsoft SQL Server instance. You may see throughput improvements for some types of activity, but the effects may not be significant overall.

Leave this configuration at the default value. Most non-default values will yield little performance improvement. Changing network packet size does not require restart. After this setting is changed, all new connections receive the new value.
In addition, on systems using differing network protocols, the network packet size should be set to the size of the most common protocol used.
If you choose to change it, do so after testing, and leave the packet size 8,060 bytes or smaller.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 

SQL Server Books Online: network packet size Option (http://msdn2.microsoft.com/en-us/library/ms187866.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, index create memory (KB), has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This configuration sets the maximum amount of memory initially allocated for creating new indexes. The default value of 0 indicates that Microsoft SQL Server controls the configuration of this value automatically.]]>
        </Impact>
        <Recommendation>
            <![CDATA[In most cases, this option should be left at the default value. It should only be modified if you require significant amounts of memory for index creation operations because of parallelism or non-aligned partitioned indexes.

Leave this option at the default, self-configuring value. If you do encounter issues that are correlated with index memory pressure, adjust the value of this option in a controlled test environment first in order to find the ideal configuration value.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: index create memory Option (http://msdn2.microsoft.com/en-us/library/ms175123.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, locks, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This configuration option determines the maximum number of available locks and thus, limits the amount of memory that the database engine uses for locks. The default setting of 0 allows the Database Engine to allocate and deallocate lock structures dynamically, based on changing system requirements.
If Locks is nonzero, batch jobs will stop. In addition, an "out of locks" error message will be generated, if the value specified is exceeded.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Do not configure this setting. For earlier versions of SQL Server, change this setting to the default value and let SQL Server manage this configuration automatically.

Leave this configuration at the default setting of 0. SQL Server will manage locks dynamically.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: locks Option 
(http://msdn2.microsoft.com/en-us/library/ms175978.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, query wait (s), has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This configuration sets the number of seconds that a query will wait for system resources before a timeout occurs. The default value of -1 indicates a timeout of 25 times the estimated cost of the query. Setting this value too low could result in a premature cancellation of the query. However, setting this value too high could result in deadlocks or prolonged blocking because the query will hold locks while waiting for the resources.]]>
        </Impact>
        <Recommendation>
            <![CDATA[In most cases, leave this configuration at the default value. If you are encountering an issue, try to first determine the specific root cause of the issue. For example, try adding more memory, tuning the query, and ensuring that the referenced tables are correctly indexed.

Leave this configuration at the default value and modify it only in rare situations. The default value is very flexible because it self-adjusts, based on each query's estimated cost. However, setting this option to a fixed value can result in excessive waits or premature query cancellations.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: query wait Option 
(http://msdn2.microsoft.com/en-us/library/ms189539.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, Ad Hoc Distributed Queries, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The Ad Hoc Distributed Queries setting is disabled by default. When enabled, ad hoc access through OPENROWSET and OPENDATASOURCE is allowed.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If remote data must be accessed frequently, disable this option and create linked servers instead.

Enable this option only if infrequent ad hoc distributed queries are required for the Microsoft SQL Server instance. Otherwise, disable the option and use linked servers instead.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: Ad Hoc Distributed Queries Option (http://msdn2.microsoft.com/en-us/library/ms187569.aspx)
 
SQL Server Books Online: Linking Servers (http://msdn2.microsoft.com/en-us/library/ms188279.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, blocked process threshold (s), has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[The blocked process threshold (s) specifies the threshold, in seconds, at which blocked process reports are generated.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Use the blocked process threshold option to specify the threshold, in seconds, at which blocked process reports are generated. The threshold can be set from 0 to 86,400. By default, no blocked process reports are produced. This event is not generated for system tasks or for tasks that are waiting on resources that do not generate detectable deadlocks.  
The blocked process threshold, set with sp_configure blocked process threshold, is the mechanism for reporting any blocks that exceed this configured number of seconds. Make sure that the threshold is not set too low as it can capture false positives. After setting the blocked process threshold using sp_configure, Profiler is then used to capture the blocker and blocked statements using the Errors and Warnings object, Blocked Process Report event.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: blocked process threshold Option (http://msdn.microsoft.com/en-us/library/ms181150.aspx)

Troubleshooting Performance Problems in SQL Server 2008 (http://msdn.microsoft.com/en-us/library/dd672789.aspx)

Understanding and resolving SQL Server blocking problems (http://support.microsoft.com/kb/224453)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, Ole Automation Procedures, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This configuration determines whether OLE Automation Procedures are supported on the Microsoft SQL Server instance (sp_OACreate, sp_OADestroy, sp_OAGetErrorInfo, sp_OAGetProperty, sp_OAMethod, sp_OASetProperty, and sp_OAStop). By default, the OLE Automation Procedures setting is disabled, which is a value of 0.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Avoid calling OLE Automation Procedures and extended stored procedures. Instead, use CLR objects. OLE Automation Procedures can cause SQL Server instability, such as memory leaks, access violations, and performance issues. Additionally, they do not have the safety mechanisms available natively to CLR objects.

Do not enable this option in SQL Server 2005 or SQL Server 2008. Use CLR objects instead.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: OLE Automation Procedures Option (http://msdn2.microsoft.com/en-us/library/ms191188.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, lightweight pooling, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[This configuration is disabled by default. When enabled, Microsoft SQL Server runs under fiber mode. This means that several execution contexts can share a single thread. However, not all components are able to run with lightweight pooling enabled, because performance and scalability of the SQL Server instance could suffer.]]>
        </Impact>
        <Recommendation>
            <![CDATA[This option should be used only in very rare situations. In general, this configuration should not be enabled.

Do not change the default option for this configuration. In addition, change it only if all performance and capacity alternatives have been exhausted or you have been advised to change it by a Microsoft support professional.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: lightweight pooling Option (http://msdn2.microsoft.com/en-us/library/ms178074.aspx)
 
How to determine proper SQL Server configuration settings (http://support.microsoft.com/kb/319942/en-us)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>NTFS Compression is enabled for SQL Server folders.</Title>
        <Category>Windows Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Microsoft SQL Server databases are not supported on NTFS or FAT compressed volumes. A compressed volume does not guarantee sector-aligned writes which is needed to guarantee transactional recovery in some circumstances.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Microsoft SQL Server databases are not supported on NTFS or FAT compressed volumes. A compressed volume does not guarantee sector-aligned writes which is needed to guarantee transactional recovery in some circumstances. SQL Server backups to compressed volumes can save disk space. However, they may increase CPU usage during the backup operation. We always recommend that you use the BACKUP checksum facilities to help guarantee data integrity. In certain scenarios, a SQL Server backup is not successful to a compressed volume or compressed folder.

To work around this problem, remove the Archive attribute from the installation folder. To do this, follow these steps:
Open the folder that contains the folder that has the Compressed attribute.
Right-click the folder that you want to remove the Compressed attribute from, and then click Properties.
On the General tab, click Advanced.
On the Advanced Attributes tab, click to clear the Compress Drive to save disk space check box.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server 2005 and 2008 Setup Failures (Compressed DATA Directory) (http://blogs.msdn.com/b/psssql/archive/2009/02/24/sql-server-2005-and-2008-setup-failures-compressed-data-directory.aspx)
SQL Server databases are not supported on compressed volumes (http://support.microsoft.com/kb/231347/en-us)
SQL Server Books Online - Read-Only Filegroups and Compression (http://msdn.microsoft.com/en-us/library/ms190257.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, max worker threads, has been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[This configuration indicates the number of worker threads available to Microsoft SQL Server processes. This value is automatically calculated according to the number of available CPUs and architecture of the machine where SQL Server is running. Thus, it rarely needs to be configured.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Leave this configuration at the default value, unless a performance benefit was confirmed in a test environment with similar hardware and SQL instance configurations. If changing from the default, Microsoft recommends 2048 as maximum for max_worker_threads on x64, 1024 as the maximum for x86 SQL Server.
One goal for fewer number of threads on x86 is because each worker thread takes some amount of virtual memory for thread stack space. Because virtual memory is extremely limited in x86, more workers will reduce the amount of virtual memory available to the server.
This is also not something to be tweaked just to be tweaking. If you suspect that there is a performance problem, it is probably not the availability of worker threads. The cause is more likely something like I/O that is causing the worker threads to wait. It is best to find the root cause of a performance issue before you change the max_worker_thread setting.

Do not change the default value without first testing the effect across your application queries in a test environment.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 
 
SQL Server Books Online: max worker threads Option (http://msdn.microsoft.com/en-us/library/ms187024.aspx)
 
SQL Worker Thread Stack Sizes (http://blogs.msdn.com/b/arvindsh/archive/2008/08/24/sql-worker-thread-stack-sizes.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, backup compression, has NOT been changed from the default value.</Title>
        <Category>SQL Configuration</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Backup compression is not only a space saving feature, but because a compressed backup is smaller than an uncompressed backup of the same data, compressing a backup typically requires less device I/O and therefore usually increases backup speed significantly. Hence, restore operations will also share from this reduced I/O.
Note that compression significantly increases CPU usage, and the additional CPU consumed by the compression process might adversely impact concurrent operations on servers with already high CPU usage.
It is possible to issue backup commands with compression to databases that will not benefit from it, but it may be detrimental in such cases. Therefore, examples such as TDE-enabled databases, can be backed up using the WITH NO_COMPRESSION notation, while omitting any compression related notation on a server that has backup compression enabled at the server option will compress backups.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

If the server is not CPU bound, and if most of the databases in your server benefit from this feature, making backup compression a default for all backup operations will generate smaller backups, therefore less backup related I/O.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: sp_configure (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188787.aspx) 

Configure Backup Compression 
(http://msdn.microsoft.com/en-us/library/hh231019.aspx) 

Backup Compression 
(http://msdn.microsoft.com/en-us/library/bb964719.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases have been identified with next autogrow increment greater than 1GB.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[If you run a transaction that requires more log space than is available, and you have turned on the autogrow option for the transaction log of that database, then the time it takes the transaction to complete will include the time it takes the transaction log to grow by the configured amount. If the growth increment is large or there is some other factor that causes it to take a long time, the query in which you open the transaction might fail because of a timeout error. The same sort of issue can result from an autogrow of the data portion of your database, namely if Instant File Initialization is disabled.]]>
        </Impact>
        <Recommendation>
            <![CDATA[For a managed production system, you must consider autogrow to be merely a contingency for unexpected growth, using a value no larger than 1024MB. Do not manage your data and log growth on a day-to-day basis with autogrow.
You can use alerts or monitoring programs to monitor file sizes and grow files proactively. This helps you avoid fragmentation and permits you to shift these maintenance activities to non-peak hours.

Use ALTER DATABASE command to manage the auto growth settings.
If Auto growth were to enable for an instance of SQL Server 2005, enable Instant File Initialization by granting the SQL Server (MSSQLSERVER) service account the SE_MANAGE_VOLUME_NAME privilege.
If this is not possible, or for an instance of SQL Server 2000, set the FILEGROWTH (autogrow) value to a fixed size to avoid escalating performance problems. 
Monitor the database growth patterns, anticipate the future growths, and expand the files during the non-peak times.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Considerations for the "autogrow" and "autoshrink" settings in SQL Server. (http://support.microsoft.com/kb/315512) 

The SQL Swiss Army Knife #5 - Checking Autogrow times (http://blogs.msdn.com/b/blogdoezequiel/archive/2010/11/16/the-sql-swiss-army-knife-5-checking-autogrow-times.aspx)

SQL Server Books Online: ALTER DATABASE (T-SQL) (http://msdn.microsoft.com/en-us/library/ms174269.aspx) 

SQL Server Books Online: Database File Initialization: (http://technet.microsoft.com/en-us/library/ms175935.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases were identified with autogrow option set to percentage growth.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[When setting autogrow for Data and Log files, keep in mind that it might be preferred to set it in Megabytes instead of Percentage, to allow better control on the growth ratio, as percentage is an ever-growing amount. This is even more critical when Instant File Initialization is not in use, as long I/O might become a bottleneck. Keep in mind that transaction logs cannot leverage Instant File Initialization, so extended log growth times are especially critical. As a rule, do not set any AUTOGROW value above 1024MB.
Also, if you grow your database by small increments (or if you grow it and then shrink it) you can end up with disk fragmentation. Disk fragmentation can cause performance issues in some circumstances]]>
        </Impact>
        <Recommendation>
            <![CDATA[For a managed production system, you must consider autogrow to be merely a contingency for unexpected growth. Do not manage your data and log growth on a day-to-day basis with autogrow.

Use ALTER DATABASE command to manage the auto growth settings. Set the FILEGROWTH (autogrow) value to a fixed size to avoid escalating performance problems.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[Considerations for the "autogrow" and "autoshrink" settings in SQL Server.
(http://support.microsoft.com/kb/315512)

SQL Server Books Online: ALTER DATABASE (T-SQL) (http://msdn.microsoft.com/en-us/library/ms174269.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with Page Verify option not set to CHECKSUM on a SQL 2005 or above instance.</Title>
        <Category>Database Settings</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[The page verify checksum option discovers damaged database pages that are caused by disk I/O path errors. These errors can cause database corruption problems and are generally caused by power failures or disk hardware failures that occur at the time the page is being written to disk. Hardware induced corruption may not be detected until the next time DBCC CHECKDB is run. Data could also be lost if you have to restore to the last, clean backup.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Set the PAGE_VERIFY Database Option to CHECKSUM (http://msdn.microsoft.com/en-us/library/bb402873.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with AUTO_CLOSE option enabled.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[When the AUTO_CLOSE option is turned ON, a database will be shut down after all resources that reference this database are freed. This might have extra recovery time and delayed DDL. When the last user connection disconnects, SQL Server releases all memory resources back to the server. But albeit being a last connection, this does not mean that someone might not connect one microsecond later, and starting recovery again.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Ensure that AUTO_CLOSE is turned off by default.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: ALTER DATABASE (T-SQL) (http://msdn.microsoft.com/en-us/library/ms174269.aspx) 

Recommendations and guidelines for setting the AUTO_CLOSE database option in SQL Server
(http://support.microsoft.com/kb/2160685)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with AUTO_SHRINK option enabled.</Title>
        <Category>Database Settings</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[When enabled, the AUTO_SHRINK option causes data and log files to be automatically shrunk if more than 25 percent of a file has unused space. This option can cause performance and concurrency issues during the shrink process. It can also cause index fragmentation after a shrink process has been performed.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Ensure that AUTO_SHRINK is turned off by default. If free space must be generated, do that by using the DBCC SHRINKFILE or DBCC SHRINKDATABASE commands. In addition, always rebuild or reorganize the indexes after a shrink operation in order to fix any fragmentation that might have occurred in the data files.

Do not use the AUTO_SHRINK database option. Instead, use the DBCC commands in a controlled manner. In addition, have the AUTO_SHRINK database option disabled by default and perform shrink operations only when absolutely necessary.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: ALTER DATABASE (T-SQL) (http://msdn.microsoft.com/en-us/library/ms174269.aspx) 

SQL Server Books Online: DBCC SHRINKFILE (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms189493.aspx)

SQL Server Books Online: DBCC SHRINKDATABASE (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms190488.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with AUTO_CREATE_STATISTICS option disabled.</Title>
        <Category>Database Settings</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Statistics contain information regarding the distribution of values in the indexed or non-indexed column(s). This statistical information can be used by the query processor to determine the optimal strategy for evaluating a query. When the AUTO_CREATE_STATISTICS database option is set to ON (default), if the query optimizer does not find any statistics on columns used in a predicate, it will create the missing single-column statistics. These single-column statistics are created only on columns that are not already the first column of an existing statistics object. These events can be observed in a SQL Profiler trace as a Missing Column Statistics warning. Along with index statistics, column statistics help the SQL Server optimizer create efficient query plans.]]>
        </Impact>
        <Recommendation>
            <![CDATA[The performance of incoming queries may suffer if AUTO_CREATE_STATISTICS is disabled. Thus, you should always keep this option enabled for each user database. Additionally, statistics are essential to the query optimization process. If new statistics are needed, but have not been created, query performance will be affected.

Enable the AUTO_CREATE_STATISTICS database setting so that statistics will be created on the unindexed column(s) based on incoming query requests.

MSIT installs SQL server with the model database configured for AUTO_CREATE_STATISTICS and ensures that databases are running with this setting using SCOM 2007 and the SQL Server management pack.]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: ALTER DATABASE (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms174269.aspx) 

SQL Server Books Online: Database Properties (Options Page)  (http://msdn.microsoft.com/en-us/library/ms188124.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases identified with one or more tables, with clustered indexes that may use a GUID as key.</Title>
        <Category>Database Design</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Using uniqueidentifier (GUID) columns as keys in clustered indexes can lead to performance and maintainability issues.]]>
        </Impact>
        <Recommendation>
            <![CDATA[A GUID key causes fragmentation because it is random. The insertion point of a new record in an index is dictated by the value of the index key, so if the key value is random, so is the insertion point.
Also keep in mind that Merge replication adds a uniqueidentifier column with a unique index if one does not exist. So if you are going to use Merge replication, you should add your own GUID column with a default of NEWSEQUENTIALID.

If for business reasons, the unique identification of a record is required to be a GUID, keep in mind that it is not a good clustering key  not monotonically increasing and not candidate for range queries. In these cases, it is best to create the Primary Key constraint as non-clustered, and choose a more suitable column as the clustering key, such as a monotonically increasing, rarely updated value.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Clustered Index Design Guidelines 
(http://msdn.microsoft.com/en-us/library/ms190639.aspx)

SQL Server Books Online: uniqueidentifier
(http://technet.microsoft.com/en-us/library/ms187942.aspx)

GUIDs as PRIMARY KEYs and/or the clustering key
(http://www.sqlskills.com/blogs/kimberly/guids-as-primary-keys-andor-the-clustering-key/)

Clustered or nonclustered index on a random GUID? (http://www.sqlskills.com/blogs/paul/clustered-or-nonclustered-index-on-a-random-guid/)

Can GUID cluster keys cause non-clustered index fragmentation? (http://www.sqlskills.com/blogs/paul/can-guid-cluster-keys-cause-non-clustered-index-fragmentation/)

SQL Server Books Online: NEWID (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187802.aspx)

SQL Server Books Online: NEWSEQUENTIALID() (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms189786.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with AUTO_UPDATE_STATISTICS option disabled.</Title>
        <Category>Database Settings</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Statistics contain a sampling of values from the index key and the first column of the index key for composite indexes. The query optimizer determines whether an index is useful for a specific query by evaluating the stored statistics. If the statistics become out of date and significant changes have occurred against the underlying data, this can result in less than optimal query performance. If AUTO_UPDATE_STATISTICS is disabled, out-of-date statistics will not be automatically updated.
Conversely, when AUTO_UPDATE_STATISTICS is ON, the query optimizer automatically updates this statistical information periodically within certain thresholds:
If the table cardinality was 500 or less at the time statistics were created, update every 500 modifications.
If the table cardinality was above 500 at the time statistics were created, update every 500 + 20% of modifications.
It is essential to update the statistics on a regular basis because out-of-date statistics can severely influence query performance. In some circumstances, statistical sampling will not be able to accurately characterize the data in a table. In that situation, update the statistics manually where you control the amount of data that is sampled.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Ensure that AUTO_UPDATE_STATISTICS is enabled for each user database on the Microsoft SQL Server instance. For highly volatile data, consider more frequent statistics updates using the system stored procedure, sp_updatestats, or the UPDATE STATISTICS command.
Also, set AUTO_UPDATE_STATISTICS_ASYNC if an application frequently executes the same query, similar queries, or similar cached query plans, or has experienced client request time outs caused by one or more queries waiting for updated statistics.

Make AUTO_UPDATE_STATISTICS enabled by default for each user database. If there are concurrency issues with statistics updates on high volume databases, consider disabling this option in favour of periodic statistic updates using sp_updatestats or UPDATE STATISTICS.
Conside enabling Trace Flag 2371 to use finer update statistics thresholds, that are calculated based on the number of rows in the table.

MSIT installs SQL server with the model database configured for AUTO_UPDATE_STATISTICS and ensures that databases are running with this setting using SCOM 2007 and the SQL Server management pack.]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Using Statistics to Improve Query Performance (http://msdn.microsoft.com/en-us/library/ms190397.aspx) 
 
SQL Server Books Online: sp_updatestats (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173804.aspx) 
 
TempDB and Index Creation (http://msdn.microsoft.com/en-us/library/ms188281.aspx) 

Statistics Used by the Query Optimizer in Microsoft SQL Server 2005 (http://technet.microsoft.com/en-us/library/cc966419.aspx) 

Statistics Used by the Query Optimizer in Microsoft SQL Server 2008 (http://technet.microsoft.com/en-us/library/dd535534.aspx)

The auto update statistics option, the auto create statistics option, and the Parallelism setting are turned off in the SQL Server database instance that hosts the BizTalk Server BizTalkMsgBoxDB database (http://support.microsoft.com/kb/912262/en-us) 

Auto update statistics and auto create statistics - should you leave them on and/or turn them on? (http://www.sqlskills.com/BLOGS/KIMBERLY/post/Auto-update-statistics-and-auto-create-statistics-should-you-leave-them-on-andor-turn-them-on.aspx) 

Statistical maintenance functionality (autostats) in SQL Server  (http://support.microsoft.com/kb/195565)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Other SQL Server services are running under the same account as the Database engine.</Title>
        <Category>Security</Category>
        <Severity>Low</Severity>
        <Impact>
            <![CDATA[In addition to changing the account name, SQL Server Configuration Manager
performs additional configuration such as setting permissions in the Windows
Registry so that the new account can read the proper SQL Server service settings.
Other tools such as the Windows Services Control Manager can change the account name but do not change associated settings.
If the service cannot access the SQL Server portion of the registry, the service may not start properly.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Change the service accounts per service according to each service minimum set of requirements, using the SQL Server Configuration Manager.
Always use SQL Server tools such as SQL Server Configuration Manager to change the account used by the various SQL Server services, or to change the password for the account.
For Analysis Services instances that you deploy in a SharePoint farm, always use SharePoint Central Administration to change the server accounts for PowerPivot service applications and the Analysis Services service
Each service in SQL Server represents a process or a set of processes to manage authentication of SQL Server operations with Windows.
When choosing a service account, consider an account with the least amount of privileges needed to do the job and no more. This varies according to the service itself, whether it is Analysis Services, Integration Services or Reporting Services.
These privileges include NT Rights granted for SQL Server service accounts, Access Control Lists created for SQL Server service accounts or Windows Permissions for SQL Server services.


]]>
        </Recommendation>
        <Reading>
            <![CDATA[Setting Up Windows Service Accounts
(http://technet.microsoft.com/en-us/library/ms143504.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with AUTO_UPDATE_STATISTICS option disabled and AUTO_UPDATE_STATISTICS_ASYNC enabled.</Title>
        <Category>Database Settings</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Statistics contain a sampling of values from the index key and the first column of the index key for composite indexes. The query optimizer determines whether an index is useful for a specific query by evaluating the stored statistics. If the statistics become out of date and significant changes have occurred against the underlying data, this can result in less than optimal query performance. If AUTO_UPDATE_STATISTICS is disabled, out-of-date statistics will not be automatically updated.
Conversely, when AUTO_UPDATE_STATISTICS is ON, the query optimizer automatically updates this statistical information periodically within certain thresholds:
If the table cardinality was 500 or less at the time statistics were created, update every 500 modifications.
If the table cardinality was above 500 at the time statistics were created, update every 500 + 20% of modifications.
It is essential to update the statistics on a regular basis because out-of-date statistics can severely influence query performance. In some circumstances, statistical sampling will not be able to accurately characterize the data in a table. In that situation, update the statistics manually where you control the amount of data that is sampled.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Ensure that AUTO_UPDATE_STATISTICS is enabled for each user database on the Microsoft SQL Server instance. For highly volatile data, consider more frequent statistics updates using the system stored procedure, sp_updatestats, or the UPDATE STATISTICS command.
Also, set AUTO_UPDATE_STATISTICS_ASYNC if an application frequently executes the same query, similar queries, or similar cached query plans, or has experienced client request time outs caused by one or more queries waiting for updated statistics. Remember that for AUTO_UPDATE_STATISTICS_ASYNC to work, the option AUTO_UPDATE_STATISTICS must also be enabled.

Make AUTO_UPDATE_STATISTICS enabled by default for each user database. If there are concurrency issues with statistics updates on high volume databases, consider disabling this option in favour of periodic statistic updates using sp_updatestats or UPDATE STATISTICS. 
When AUTO_UPDATE_STATISTICS_ASYNC is needed, keep in mind that AUTO_UPDATE_STATISTICS must also be enabled.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Using Statistics to Improve Query Performance (http://msdn.microsoft.com/en-us/library/ms190397.aspx) 
 
SQL Server Books Online: sp_updatestats (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms173804.aspx) 
 
TempDB and Index Creation (http://msdn.microsoft.com/en-us/library/ms188281.aspx) 

Statistics Used by the Query Optimizer in Microsoft SQL Server 2005 (http://technet.microsoft.com/en-us/library/cc966419.aspx) 

Statistics Used by the Query Optimizer in Microsoft SQL Server 2008 (http://technet.microsoft.com/en-us/library/dd535534.aspx)

The auto update statistics option, the auto create statistics option, and the Parallelism setting are turned off in the SQL Server database instance that hosts the BizTalk Server BizTalkMsgBoxDB database (http://support.microsoft.com/kb/912262/en-us) 

Auto update statistics and auto create statistics - should you leave them on and/or turn them on? (http://www.sqlskills.com/BLOGS/KIMBERLY/post/Auto-update-statistics-and-auto-create-statistics-should-you-leave-them-on-andor-turn-them-on.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Database(s) with FORCED PARAMETERIZATION option enabled.</Title>
        <Category>Database Settings</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[When the PARAMETERIZATION option is set to FORCED, any literal value that appears in a SELECT, INSERT, UPDATE or DELETE statement, submitted in any form, is converted to a parameter during query compilation, with a few exceptions. Generally, the PARAMETERIZATION FORCED option should only be used by experienced database administrators after determining that doing this does not adversely affect performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Consider the following when you set the PARAMETERIZATION option to FORCED:
Forced parameterization, in effect, changes the literal constants in a query to parameters when compiling a query. Therefore, the query optimizer might choose suboptimal plans for queries. In particular, the query optimizer is less likely to match the query to an indexed view or an index on a computed column. It may also choose suboptimal plans for queries posed on partitioned tables and distributed partitioned views. Forced parameterization should not be used for environments that rely heavily on indexed views and indexes on computed columns. 
Distributed queries that reference more than one database are eligible for forced parameterization as long as the PARAMETERIZATION option is set to FORCED in the database whose context the query is running.
Setting the PARAMETERIZATION option to FORCED flushes all query plans from the plan cache of a database, except those that currently are compiling, recompiling, or running. Plans for queries that are compiling or running during the setting change are parameterized the next time the query is executed.
Setting the PARAMETERIZATION option is an online operation that it requires no database-level exclusive locks.
Forced parameterization is disabled (set to SIMPLE) when the compatibility of a SQL Server database is set to 80, or a database on an earlier instance is attached to an instance of SQL Server 2005 or later.
The current setting of the PARAMETERIZATION option is preserved when reattaching or restoring a database.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: ALTER DATABASE (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms174269.aspx)

SQL Server Books Online: Forced Parameterization (http://msdn.microsoft.com/en-us/library/ms175037.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
   
    <PTOClinicIssue enabled = "true">
        <Title>Index statistics are not regularly updated with a FULLSCAN.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[When you create an index, the query optimizer automatically stores statistical information about the indexed columns. Also, when the AUTO_CREATE_STATISTICS database option is set to ON (the default setting), the database engine automatically creates statistics for columns without indexes that are used in a predicate.
As the data in a column changes, index and column statistics can become out of date and cause the query optimizer to make less than optimal decisions about how to process a query. For example, if you create a table with an indexed column and 1,000 rows of data, all with unique values in the indexed column, the query optimizer sees the indexed column as a good way to collect the data for a query. If you update the data in the column so there are many duplicate values, the column is no longer an ideal candidate for use in a query. However, the query optimizer still considers it to be a good candidate because of the indexes outdated distribution statistics, which are based on the data before the update. Consequently, this will lead to poor database query performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

You have to set up a maintenance plan to update index statistics and specify WITH FULLSCAN. This is necessary to ensure that the query optimizer has current information about the distribution of data values in the tables. This allows the query optimizer to more accurately determine the best way to access data because it has more information about the data stored in the database.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Index Statistics
(http://technet.microsoft.com/en-us/library/ms190397.aspx)

SQL Server Books Online: CREATE STATISTICS (Transact-SQL)
(http://technet.microsoft.com/en-us/library/ms188038.aspx)

SQL Server Books Online: DROP STATISTICS (Transact-SQL)
(http://technet.microsoft.com/en-us/library/ms175075.aspx)

SQL Server Books Online: UPDATE STATISTICS (Transact-SQL)
(http://technet.microsoft.com/en-us/library/ms187348.aspx)

SQL Server Books Online: DBCC SHOW_STATISTICS (Transact-SQL)
(http://technet.microsoft.com/en-us/library/ms174384.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Fragmented indexes were found.</Title>
        <Category>Operational Excellence</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Index fragmentation occurs naturally as changes are made to data. Fragmentation exists when the logical ordering of pages in an index does not match the physical ordering of data on the disk. In addition, fragmentation can decrease performance by increasing the number of reads that are necessary to retrieve information.
In Microsoft SQL Server 2008 and SQL Server 2005, the sys.dm_db_index_physical_stats function is used to detect fragmentation at the index, table, or database level. In SQL Server 2000, DBCC SHOWCONTIG is used to detect fragmentation.]]>
        </Impact>
        <Recommendation>
            <![CDATA[

Rebuilding indexes is essential to good performance, and you should have a maintenance plan in place to regularly rebuild indexes.
An index that is more than 5 percent or less than 30 percent fragmented should be reorganized. In addition, an index that is more than 30 percent fragmented should be rebuilt.
Priority in both of these processes should be given to indexes which consume 1000 or more data pages, and those that are most used for range scans, as opposed to used mainly for singleton lookups.
Leverage AdaptiveIndexDefrag procedure for index defrag according to the objects needs, balancing index performance with minimal required time for this task, while being aware of all the common caveats linked to index defragmentation, namely ordering its work by range scan count and index size.
Download and full usage instructions are available here:
http://blogs.msdn.com/b/blogdoezequiel/archive/2011/07/03/adaptive-index-defrag.aspx 
Make index defragmentation routines a part of your regular maintenance tasks, as recommended in http://blogs.msdn.com/b/blogdoezequiel/archive/2012/09/18/about-maintenance-plans-grooming-sql-server.aspx.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Reorganizing and Rebuilding
Indexes (http://msdn2.microsoft.com/en-us/library/ms189858.aspx)

SQL Server 2000 Books Online: DBCC SHOWCONTIG
(http://msdn.microsoft.com/en-us/library/aa258803(SQL.80).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases identified with one or more tables, with indexes that may require update statistics.</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[When you create an index, the query optimizer automatically stores statistical information about the indexed columns. Also, when the AUTO_CREATE_STATISTICS database option is set to ON (the default setting), the database engine automatically creates statistics for columns without indexes that are used in a predicate.
As the data in a column changes, index and column statistics can become out of date and cause the query optimizer to make less than optimal decisions about how to process a query. For example, if you create a table with an indexed column and 1,000 rows of data, all with unique values in the indexed column, the query optimizer sees the indexed column as a good way to collect the data for a query. If you update the data in the column so there are many duplicate values, the column is no longer an ideal candidate for use in a query. However, the query optimizer still considers it to be a good candidate because of the indexes outdated distribution statistics, which are based on the data before the update. Consequently, this will lead to poor database query performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Statistics can be maintained either automatically or manually. To automatically update statistics, set AUTO_UPDATE_STATISTICS to the default setting of ON. This will periodically update the statistical information as the data in the tables change.

If AUTO_UPDATE_STATISTICS is disabled on your system (set to OFF), you will have to set up a maintenance plan to update index statistics. This is necessary to ensure that the query optimizer has current information about the distribution of data values in the tables. This allows the query optimizer to more accurately determine the best way to access data because it has more information about the data stored in the database.
Leverage AdaptiveIndexDefrag procedure for index defrag and statistics upkeep according to the objects needs, balancing index performance with minimal required time for this task, while being aware of all the common caveats linked to index defragmentation and statistics update.
Download and full usage instructions are available here: http://blogs.msdn.com/b/blogdoezequiel/archive/2011/07/03/adaptive-index-defrag.aspx 
For recommendations on implementing automated maintenance tasks, from integrity checking to index defrag, among other maintenance tasks, please refer the following article: http://blogs.msdn.com/b/blogdoezequiel/archive/2012/09/18/about-maintenance-plans-grooming-sql-server.aspx

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Index Statistics (http://technet.microsoft.com/en-us/library/ms190397.aspx) 

SQL Server Books Online: CREATE STATISTICS (Transact-SQL) (http://technet.microsoft.com/en-us/library/ms188038.aspx) 

SQL Server Books Online: UPDATE STATISTICS (Transact-SQL) (http://technet.microsoft.com/en-us/library/ms187348.aspx) 

SQL Server Books Online: DBCC SHOW_STATISTICS (Transact-SQL) (http://technet.microsoft.com/en-us/library/ms174384.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases identified with one or more tables, with Foreign Keys that are not indexed.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[A foreign key (FK) is a column or combination of columns that is used to establish and enforce a link between the data in two tables to control the data that can be stored in the foreign key table. In a foreign key reference, a link is created between two tables when the column or columns that hold the primary key value for one table are referenced by the column or columns in another table. This column becomes a foreign key in the second table.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Unlike primary key constraints, creating a foreign key constraint does not automatically create a corresponding index. However, manually creating an index on a foreign key is often useful because:
Foreign key columns are frequently used in join criteria when the data from related tables is combined in queries by matching the column or columns in the foreign key constraint of one table with the primary or unique key column or columns in the other table. An index enables the Database Engine to quickly find related data in the foreign key table.
Changes to primary key constraints are checked with foreign key constraints in related tables.

Check all the current Foreign Keys that are used frequently in joins and create a non-clustered index for each one of them.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Primary and Foreign Key Constraints
(http://technet.microsoft.com/en-us/library/ms179610.aspx)

SQL Server Books Online: Creating and Modifying FOREIGN KEY Constraints
(http://technet.microsoft.com/en-us/library/ms177463.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Databases identified with one or more tables, with non-unique clustered indexes.</Title>
        <Category>Database Design</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[Non-Unique clustered indexes can have a performance impact due to the requirement of SQL Server to add an uniquifier to the clustered index in order to ensure this nature.]]>
        </Impact>
        <Recommendation>
            <![CDATA[It is best practice to ensure that all clustered indexes are created with a unique set of key columns. This will ensure that additional I/O is not wasted, as well as data cache bloat due to the overhead of unique enforcement from within SQL Server.

Search for non-unique clustered indexes and start planning a redesign of these indexes in order to achieve uniqueness with the index key columns.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Creating Unique Indexes
(http://technet.microsoft.com/en-us/library/ms175132.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Partitioned indexes are not aligned with partitioned tables.</Title>
        <Category>Database Design</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Misaligned partitioned indexes can lead to performance and maintainability issues.]]>
        </Impact>
        <Recommendation>
            <![CDATA[For a particular table, partitioned indexes and their corresponding partitioning functions, partition schemes should be aligned to the partitioning key. 
To enable partition switching, all indexes on the table must be aligned.

Examine the partitioning function, the data in the table, and the business uses of the data. Through analysis and testing, find an optimal partitioning scheme based on the existing business requirements and technical capabilities. Then, execute the strategy.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: CREATE PARTITION FUNCTION (Transact-SQL) (http://msdn.microsoft.com/en-us/library/ms187802.aspx)

SQL Server Books Inline: CREATE PARTION SCHEME
(http://msdn.microsoft.com/en-us/library/ms179854.aspx)

SQL Server Books Online: Understanding Partitioning
(http://msdn.microsoft.com/en-us/library/ms188232.aspx)

SQL Server Books Online: Using Table and Index Partitioning (http://msdn.microsoft.com/en-us/library/ms162136.aspx)

SQL Server Books Online: Special Guidelines for Partitioned Indexes (http://msdn.microsoft.com/en-us/library/ms187526.aspx)

Partitioned Tables and Indexes in SQL Server 2005 (Kimberly Tripp MSDN article; also applies to SQL Server 2008)
(http://www.sqlskills.com/resources/Whitepapers/Partitioning%20in%20SQL%20Server%202005%20Beta%20II.htm)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Tables have been identified that have no indexes.</Title>
        <Category>Database Design</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[At a minimum, most tables should be indexed with a clustered index and, possibly, with non-clustered indexes, depending on the query activity against it. In rare situations, such as for staging tables and infrequently referenced small tables, a table without an index is acceptable.]]>
        </Impact>
        <Recommendation>
            <![CDATA[By default, every table should have a clustered index defined for it so it can participate in ongoing index rebuilds and reorganizations. If the table is frequently queried and the selectivity of the data lends itself to it, consider adding non-clustered indexes to the table as well. Indexes should be created based on query behavior and on the required query duration defined by the application owners in the service level agreements.
Therefore, do not use a heap when:
The data is frequently returned in a sorted order. A clustered index on the sorting column could avoid the sorting operation.
The data is frequently grouped together. Data must be sorted before it is grouped, and a clustered index on the sorting column could avoid the sorting operation.
Ranges of data are frequently queried from the table. A clustered index on the range column will avoid sorting the entire heap.
There are no non-clustered indexes and the table is large. In a heap, all rows of the heap must be read to find any row.

At a minimum, define a clustered index for the table. If the table is queried frequently, define non-clustered indexes for the columns referenced in the query criteria.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online : CREATE INDEX (Transact-SQL) (http://msdn2.microsoft.com/en-us/library/ms188783.aspx) 

SQL Server Books Online : General Index Design Guidelines (http://msdn2.microsoft.com/en-us/library/ms191195.aspx) 

Heaps (Tables without Clustered Indexes)(http://msdn.microsoft.com/en-us/library/hh213609.aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Bookmark lookup with at least twice the necessary I/O to retrieve the results.</Title>
        <Category>Performance</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[Queries that are frequently called, problematic queries, or queries that use lots of resources are good candidates for a covering index. A covering index is an index that includes all the columns that are referenced in the WHERE and SELECT clauses. The index "covers" the query, and can completely service the query without going to the base data. This is in effect a materialized view of the query. The covering index performs well because the data is in one place and in the required order. A covering index may improve scalability by removing contention and access from the main table.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If you see bookmark lookup, it means that your index is not covering. Try to make it covering if it makes sense]]>
        </Recommendation>
        <Reading>
            <![CDATA[Chapter 14 - Improving SQL Server Performance
https://msdn.microsoft.com/en-us/library/ff647793.aspx

SQL Server Optimization
https://technet.microsoft.com/en-us/library/aa964133(v=sql.90).aspx]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Critical Indexes are Missing</Title>
        <Category>Performance Metrics</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Indexes are missing that would be beneficial to performance.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Consider adding the indexes highlighted in the UsefulIndexes Excel tab.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Non-SARGable predicates in the WHERE or JOIN clause</Title>
        <Category>Performance</Category>
        <Severity>Critical</Severity>
        <Impact>
            <![CDATA[SARGable is an adjective in SQL that means that an item can be found using an index (assuming one exists). Understanding SARGability can really impact your ability to have well-performing queries. Incidentally  SARGable is short for Search ARGument Able.

]]>
        </Impact>
        <Recommendation>
            <![CDATA[You can re write most of the times to translate Non-SARGable expressions to equivalent expressions that will use your table indexes.]]>
        </Recommendation>
        <Reading>
            <![CDATA[Conor vs. Index SARGability with complex scalar expressions
https://blogs.msdn.microsoft.com/conor_cunningham_msft/2010/04/23/conor-vs-index-sargability-with-complex-scalar-expressions/

Query Performance
https://social.msdn.microsoft.com/Forums/sqlserver/en-US/a4c33760-bb80-452c-b000-a036a5d9c793/query-performance?forum=transactsql]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Rarely used indexes</Title>
        <Category>Database Design</Category>
        <Severity>Medium</Severity>
        <Impact>
            <![CDATA[This rule checks if there are any rarely used indexes in the tables. This issue could mean nn index is marked as unused in this period.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Perform usage check periodically to maintian indexes and to remove any unused index. Consider collecting data using the script in the troubleshooting section at intervals over a longer period of time to get a more accurate picture of index usage. Its also a good practice to disable, script out and save any indexes you plan to drop should you need to restore these indexes.]]>
        </Recommendation>
        <Reading>
            <![CDATA[sys.dm_db_index_usage_stats <https://msdn.microsoft.com/en-us/library/ms188755.aspx>
SQL Server: Databases and Indexes <https://technet.microsoft.com/en-us/magazine/jj128029.aspx>
CREAT INDEX (Transact-SQL) <https://msdn.microsoft.com/en-us/library/ms188783.aspx>
DROP INDEX (Transact-SQL) <http://msdn.microsoft.com/en-us/library/ms176118.aspx>
]]></Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>SQL Server build near its end or out of support</Title>
        <Category>Operational Excellence</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Microsoft provides an industry-leading Support Lifecycle policy in length and provision giving customers consistent, transparent, and predictable timelines for which software is supported. 
Service packs and Cumulative Updates are the main delivery vehicle for fixes, security patches, and general improvements to the SQL Server system. These updates can improve the security of SQL Server, as well as provide solutions to known issues and bugs.
Staying on a fully supported version of SQL Server ensures that it is running the latest and most secure version. 
SQL Server instances running an unsupported service pack leaves the environment at risk from both a supportability perspective and because they most likely lack the latest security patches and features.]]>
        </Impact>
        <Recommendation>
            <![CDATA[After the availability of a service pack is announced, it should be applied to a test version of the environment. It should also be subjected to a comprehensive testing program that involves all the applications in the environment that use SQL Server. In addition, the tests should be conducted in such a way that a load similar to, if not equal to, the loads experienced in production are run against the test system. Hotfixes should only be installed when specifically instructed by Microsoft Support. 
Cumulative Updates (CU) are only recommended for installation when specific symptoms are experienced on your SQL Server, if you are running QFE branch, review the issues fixed in the KB article associated with the release of CU and apply it if your sever is experiencing any of those symptoms.
The following summary lists the minimum supported builds depending on the version of SQL Server. 
For SQL Server 2005
SQL Server 2005 Extended Support period ended in April 12, 2016.
For SQL Server 2008
Minimum supported build: SP4 - 10.00.6000 (SP3 - 10.00.5500 for IA64)
Latest Service Pack: SP4 - 10.00.6000 (SP3 - 10.00.5500 for IA64)
For SQL Server 2008 R2
Minimum supported build: SP3 - 10.50.6000 (SP2 - 10.50.4000 for IA64)
Latest Service Pack: SP3 - 10.50.6000 (SP2 - 10.50.4000 for IA64)
For SQL Server 2012
Minimum supported build: SP2 - 11.0.5058
Latest Service Pack: SP3 - 11.0.6020
For SQL Server 2014
Minimum supported build: SP1 - 12.0.4100
Latest Service Pack: SP2 - 12.0.5000
For SQL Server 2016
Minimum supported build: RTM - 13.0.1601
Latest Service Pack: SP1 - 13.0.4001]]>
        </Recommendation>
        <Reading>
            <![CDATA[Microsoft Support Lifecycle Policy <https://support.microsoft.com/en-gb/lifecycle?wa=wsignin1.0>
SQL Server Support Lifecycle <https://support.microsoft.com/en-gb/lifecycle/search?sort=PN&alpha=sql%20server&Filter=FilterNO>]]></Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Usage of Table Variable objects that hurts performance</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Frequently, we see our customers using table variables in their stored procedures and batches and experience performance problems.
A table variable is not a memory-only structure. Because a table variable might hold more data than can fit in memory, it has to have a place on disk to store data. Table variables are created in the tempdb database similar to temporary tables. If memory is available, both table variables and temporary tables are created and processed while in memory (data cache). Disk operations tents to be high performance costs.
In general, these performance problems are introduced because of large number of rows being populated into the table variable.
Table variables were introduced in SQL Server 2000 with intention to reduce recompiles.  Over time, it gained popularity.  Many users use to to populate large number of rows and then join with other tables.
When the batch or stored procedure containing the table variable is compiled, the number of rows of the table variable is unknown. Therefore, optimizer  has to make some assumptions.   It estimates very low number of rows for the table variable.   This can cause inefficient plan.  Most of the time, a nested loop join is used with the table variable as outer table.  If large number of rows exist in the table variable, this results in inner table be executed many times.
So if you anticipate large number of rows to be populated to the table variable, you should not use it to begin with unless you dont intend to join with other tables or views.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[If you have large number of rows to be populated into the table variable, consider this solutions

Rewrite your code to avoid unneeded temporary or variable tables. Most of the times this code can be translated into CTEs, JOINS or nested queries.
You can add option recompile to the statement that involves the table variable joining with other tables.   By doing this, SQL Server will be able to detect number of rows at recompile because the rows have already been populated.
Additionally, you can also use temp tables which can provide better statistics.  ]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Performance Tuning : Table Variable Vs Temporary Tables
https://blogs.msdn.microsoft.com/naga/2015/05/10/sql-server-performance-tuning-table-variable-vs-temporary-tables/

]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High Trivial Plan Percentage.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There is a high percentage of trivial plans executed on the system.  Trivial plans are generally plans where there is only one method to execute the query.  These queries are often in the form of 
            
            SELECT *
            FROM TableName
            
Because there are a large number of these types of queries on the system, it may be worth examining the workload to see if there is a way to optimize these queries that are returning all columns and all records from a single table.

These tend to be very expensive queries, especially if the tables involved are very large.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[If these scans are associated with an ETL process, consider tuning the queries or the ETL process so that only necessary data is read from the tables rather than the entire table contents.
If these queries are from users executing ah-hoc queries, using Resource Governor may be an option so that large scans do not affect overall system performance.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>    


    <PTOClinicIssue enabled = "true">
        <Title>High Avg Query Final Cost.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The average overall query cost on the system is high.  This indicates there are high percentage of queries where the overall query cost is over 200 units.  

There may be several contributing factors to this high overall query cost:
- Queries involving non SARG-able predicates
- Out of date statistics resulting in inaccurate cardinality estimation
- Expensive sort operations
- User defined functions
]]>
        </Impact>
        <Recommendation>
            <![CDATA[The following query can be used to identify expensive queries:
 
SELECT TOP 100
	AverageLogicalReads = total_logical_reads/execution_count,
	AverageRunTimeSeconds = (total_elapsed_time/1000000.0)/execution_count,
	execution_count,
	last_worker_time,
	last_physical_reads,
	total_logical_writes,
	last_logical_writes,
	last_logical_reads,
	last_elapsed_time,
	query_hash,
	query_plan_hash,		
	sql_handle,
	StatementText = 
	LEFT((
	REPLACE(REPLACE((SUBSTRING(text, statement_start_offset/2+1,
	(CASE WHEN statement_end_offset=-1
	THEN LEN(CONVERT(NVARCHAR(MAX), text))*2
	ELSE statement_end_offset
	END-statement_start_offset)/2)),CHAR(10),''),CHAR(13),'')),250)
FROM
sys.dm_exec_query_stats q
CROSS APPLY sys.dm_exec_sql_text(sql_handle)
CROSS APPLY sys.dm_exec_query_plan(plan_handle)
ORDER BY
AverageLogicalReads DESC
            
            
            ]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>    
    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server Configuration setting, xp_cmdshell, is enabled.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The xp_cmdshell system stored procedure spawns a Windows command shell and passes in a string for execution. Any output is returned as rows of text. The xp_cmdshell option is a server configuration option that enables system administrators to control whether the xp_cmdshell extended stored procedure can be executed on a system.]]>
        </Impact>
        <Recommendation>
            <![CDATA[By default, the xp_cmdshell option is disabled on new installations. It can be enabled by running the sp_configure system stored procedure.

Avoid use of xp_cmdshell where possible and do not change the default value unless an exception has been explicitly made for this instance.

If the usage of xp_cmdshell is absolutely necessary, consider using a proxy account.

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: xp_cmdshell (Transact-SQL)
(http://msdn.microsoft.com/en-us/library/ms175046.aspx)

SQL Server Books Online: xp_cmdshell Option
(http://msdn.microsoft.com/en-us/library/ms190693.aspx)

xp_cmdshell Proxy Account:
https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/xp-cmdshell-transact-sql?view=sql-server-2017

]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>High number of remote queries executed.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There has been a high number of remote queries executed on the system.  While these may be required, they can often result in poor overall performance.  

If possible, consider moving the data to the local SQL machine to be queried.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If possible, consider moving the data to the local SQL machine to be queried.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Linked Servers (Transact-SQL)
(https://docs.microsoft.com/en-us/sql/relational-databases/linked-servers/linked-servers-database-engine)
]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>High amount of cursor usage.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There has been a high number of queries involving cursors on the system.  These queries may occur at the application level or via TSQL modules.
            
            Cursors, in general, should be avoided for normal query processing.  Relying on the database engine to perform query execution is ther preferred and optimal strategy as compared to a row-by-row approach.]]>
        </Impact>
        <Recommendation>
            <![CDATA[If possible, consider rewriting these queries to be JOIN statements and not iterative constructs.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Cursors (Transact-SQL)
(https://docs.microsoft.com/en-us/sql/t-sql/language-elements/declare-cursor-transact-sql)
]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>
    <PTOClinicIssue enabled = "true">
        <Title>Available memory MB set too high.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There is a large amount of memory not being used on the system.  This can be due to the fact that SQL Server was recently re-started and the buffer pool has not yet hit its target.
If that is the case, then you can safely ignore this message.  However, if the SQL instance has been running for quite some time then it may be useful to allocate more system memory to SQL Server by setting 'max server memory' via sp_cofigure.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[If possible, consider allocating more memory to SQL Server via 'max server memory' through sp_configure.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Memory Configuration (Transact-SQL)
https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/server-memory-server-configuration-options
]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>



    <PTOClinicIssue enabled = "true">
        <Title>The SQL Server configuration setting, cost threshold for parallelism, has NOT been changed from the default value of 5.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The sp_configure setting 'cost threshold for parallelism' has not been changed from the default value of 5.  This can cause less expensive queries to be executed amount parallel processors, which can cause more expensive queries to have to wait longer than for processor usage.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Change this from the default value of 5 to a higher value.  Setting this value in the range from 30-45 is a good start.]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: cost threshold for parallelism
(https://technet.microsoft.com/en-us/library/ms188603(v=sql.105).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>Tables have been identified that are missing critical indexes.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Tables have been identified that are missing critical indexes.]]>
        </Impact>
        <Recommendation>
            <![CDATA[Tables have been identified that are missing critical indexes.

You can use the following query to identify missing indexes on the system.  

SELECT
    IndexImpact = user_seeks * avg_total_user_cost * (avg_user_impact * 0.5),
    LastUserSeek = groupstats.last_user_seek,
    FullObjectName = details.[statement],
    TableName = REPLACE(REPLACE(REVERSE(LEFT(REVERSE(details.[statement]), CHARINDEX('.', REVERSE(details.[statement]))-1)),'[',''), ']',''),
    EqualityColumns = details.equality_columns,
    InequalityColumns = details.inequality_columns,
    IncludedColumns = details.included_columns,
    Compiles = groupstats.unique_compiles,
    Seeks = groupstats.user_seeks,
    UserCost = groupstats.avg_total_user_cost,
    UserImpact = groupstats.avg_user_impact, 
    DatabaseName = DB_NAME(details.database_id)
FROM
    sys.dm_db_missing_index_group_stats AS groupstats
INNER JOIN sys.dm_db_missing_index_groups AS groups
    ON groupstats.group_handle = groups.index_group_handle
INNER JOIN sys.dm_db_missing_index_details AS details
    ON groups.index_handle = details.index_handle
ORDER BY
    IndexImpact DESC 

]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: cost threshold for parallelism
(https://technet.microsoft.com/en-us/library/ms188603(v=sql.105).aspx)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>There are a high number of outstanding SQL Server IOs.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[As SQL Server requests data from disk or attempts to write to disk, a request is made to the operating system to complete the request.  Once the request is made, SQL Server waits asynchronously until the confirmation is back from the OS that the read/write has completed.
These pending requests represent outstanding work that SQL Server is waiting to be completed.  If there are a lot of outstanding requests, it could be a sign of an underlying problem.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Verify that the read/write response times for the underlying disks that suppor the SQL Server data and log drives are responding in a timely fashion.  Also ensure that queries are not requesting an unnecessary amount of data, which can tax the underlying disk subsystem.]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Pending IOs
https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-io-pending-io-requests-transact-sql ]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>Low Buffer Cache Hit Ratio.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The Buffer Cache Hit Ratio for the instance of SQL Server is less than 90.  

Indicates the percentage of pages found in the buffer cache without having to read from disk. The ratio is the total number of cache hits divided by the total number of cache lookups over the last few thousand page accesses. After a long period of time, the ratio moves very little. Because reading from the cache is much less expensive than reading from disk, you want this ratio to be high. Generally, you can increase the buffer cache hit ratio by increasing the amount of memory available to SQL Server or by using the buffer pool extension feature.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Because this value is below the normal threshold, it may be necessary to tune the queries running on the system or add additional memory so that SQL Server can service queries from cache rather than needing to pull the data pages from disk so often.]]>
        </Recommendation>
        <Reading>
            <![CDATA[SQL Server Books Online: Buffer Cache Hit Ratio
(https://docs.microsoft.com/en-us/sql/relational-databases/performance-monitor/sql-server-buffer-manager-object)]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases with files on the C drive.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There are databases with files located on the C drive.  The data for these files may be at risk if there is not redundancy built into the C drive disks.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[The C drive is typically a local disk with little redundancy built in.  Relocate the database files to drives with builtin redundancy.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases using Delayed Durability.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Databases using Delayed Durability.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Databases using Delayed Durability.]]>
        </Recommendation>
        <Reading>
            <![CDATA[How to control database durability:  https://docs.microsoft.com/en-us/sql/relational-databases/logs/control-transaction-durability ]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases with Parameterization Forced.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[
You can override the default simple parameterization behavior of SQL Server by specifying that all SELECT, INSERT, UPDATE, and DELETE statements in a database be parameterized, subject to certain limitations. Forced parameterization is enabled by setting the PARAMETERIZATION option to FORCED in the ALTER DATABASE statement. Forced parameterization may improve the performance of certain databases by reducing the frequency of query compilations and recompilations. Databases that may benefit from forced parameterization are generally those that experience high volumes of concurrent queries from sources such as point-of-sale applications.

When the PARAMETERIZATION option is set to FORCED, any literal value that appears in a SELECT, INSERT, UPDATE or DELETE statement, submitted in any form, is converted to a parameter during query compilation

]]>
        </Impact>
         <Recommendation>
            <![CDATA[
Forced parameterization, in effect, changes the literal constants in a query to parameters when compiling a query. Therefore, the query optimizer might choose suboptimal plans for queries. In particular, the query optimizer is less likely to match the query to an indexed view or an index on a computed column. It may also choose suboptimal plans for queries posed on partitioned tables and distributed partitioned views. Forced parameterization should not be used for environments that rely heavily on indexed views and indexes on computed columns. Generally, the PARAMETERIZATION FORCED option should only be used by experienced database administrators after determining that doing this does not adversely affect performance.

Distributed queries that reference more than one database are eligible for forced parameterization as long as the PARAMETERIZATION option is set to FORCED in the database whose context the query is running.

Setting the PARAMETERIZATION option to FORCED flushes all query plans from the plan cache of a database, except those that currently are compiling, recompiling, or running. Plans for queries that are compiling or running during the setting change are parameterized the next time the query is executed.

Setting the PARAMETERIZATION option is an online operation that it requires no database-level exclusive locks.


Forced parameterization is disabled (set to SIMPLE) when the compatibility of a SQL Server database is set to 80, or a database on an earlier instance is attached to an instance ofSQL Server 2005 or later. 

The current setting of the PARAMETERIZATION option is preserved when reattaching or restoring a database.
]]>
        </Recommendation>
        <Reading>
            <![CDATA[Forced Parameterization:  https://technet.microsoft.com/en-us/library/ms175037(v=sql.105).aspx  ]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases with Trustworthy setting enabled.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[The TRUSTWORTHY database property is used to indicate whether the instance of SQL Server trusts the database and the contents within it. By default, this setting is OFF, but can be set to ON by using the ALTER DATABASE statement.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[We recommend that you leave this setting set to OFF to mitigate certain threats that may be present when a database is attached to the server and the following conditions are true:

The database contains malicious assemblies that have an EXTERNAL_ACCESS or UNSAFE permission setting. 

The database contains malicious modules that are defined to execute as users that are members of a group that has administrative credentials.

If the owner of the database (dbo) is a privileged user, such as sa, then any user belonging to the db_owners database group in that database can elevate their permissions to sa.  This is a large security hole that should be addressed by the administrative group.
 ]]>
        </Recommendation>
        <Reading>
            <![CDATA[Trustworthy Database Property:  https://docs.microsoft.com/en-us/sql/relational-databases/security/trustworthy-database-property 

Guidelines for using the Trustworthy Database Property:  https://support.microsoft.com/en-us/help/2183687/guidelines-for-using-the-trustworthy-database-setting-in-sql-server 
]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases in StandyBy mode.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Databases in StandyBy mode.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Databases in StandyBy mode.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases not online.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Databases not online.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Databases not online.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Drives low on disk space.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There are drives on the server that are running low on disk space.  This can impact SQL Server if there are data or log files on these drives and there is no longer space to write data.  
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Ensure that any SQL files are not located on the drives that are running low on disk space. Add additional disk space and/or clean up the drives that are running low on space. ]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Databases with different sized data files.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Databases with different sized data files.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Databases with different sized data files.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>There are a number of query execution memory grants larger than 1GB.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Query memory grants are a part of server memory used to store temporary row data while sorting and joining rows. It is called "grant" because the server requires those queries to "reserve" before using memory. This reservation improves query reliability under server load because a query with reserved memory is less likely to hit out-of-memory while running, and the server prevents one query from dominating entire server memory. 

Excessive grants for memory, such as cases where the query requests a grant larger than 1GB in size, can be caused by inaccurate statistics, poorly written queries or by using constructs that can cause inaccurate cardinality estimations.  
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Research the queries that are executing on the system to find those queries where the memory grant request is very large.  These represent opportunities to tune these likely inefficient queries.]]>
        </Recommendation>
        <Reading>
            <![CDATA[https://blogs.msdn.microsoft.com/sqlqueryprocessing/2010/02/16/understanding-sql-server-memory-grant/]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Buffer Pool has not hit target.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Buffer Pool has not hit target.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Buffer Pool has not hit target.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>High number of join hints used.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[A high number of join hints have been used by queries on the system.  Join hints can lead to performance problems because they force a specific type of physical join as well as they enforce the join order for the tables in the order in which they're written in the query.  These factors can lead to sub-optimal execution plans.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[If this is a vendor application, then you likely will not be able to do much about this.  However, if this is a custom-built application and can be modified - consider altering the TSQL code to exclude any hard-coded join hints.]]>
        </Recommendation>
        <Reading>
            <![CDATA[Join Hints:  https://docs.microsoft.com/en-us/sql/t-sql/queries/hints-transact-sql-join ]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>High Locks Waits/sec.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[High Locks Waits/sec.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[High Locks Waits/sec.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>High Avg Lock Wait Time.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There is a high average lock wait time on this system.

            Lock waits are measured by the time spent waiting for a lock request to be granted once requested.  The higher this figure is, the longer queries are waiting to gain access to the resources they need.  This can appear to the end user as longer query response times.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Consider tuning queries to ensure that access to rows/pages happen as quickly as possible.
            
            Also consider enabling read-committed snapshot isolation.  This database level setting ensures that queries reading data do not conflict with queries modifying data.  Often enabling this setting at the database level can significantly reduce blocking on the system.]]>
        </Recommendation>
        <Reading>
            <![CDATA[Transaction Locking and Row Versioning Guide
            https://docs.microsoft.com/en-us/sql/relational-databases/sql-server-transaction-locking-and-row-versioning-guide?view=sql-server-2017]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>High Lock Requests/sec.</Title>
        <Category>SQL Configuration</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[High Lock Requests/sec.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[High Lock Requests/sec.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

    <PTOClinicIssue enabled = "true">
        <Title>Optimize for ad-hoc workloads is wasting memory.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[A high number of ad-hoc plans exist in the plan cache, which is causing memory performance issues as too many ad-hoc plans are using too much memory.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Enable the sp_configure setting 'optimize for ad-hoc workloads' to prevent single use plans from bloating the plan cache.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>Other memory consumers are competing with the Buffer Pool for Memory.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Other memory consumers are using a large portion of memory.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Queries that are generated from ad-hoc code or queries that are generating a large amount of workspace memory grants are competing with the buffer pool for memory usage.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


    <PTOClinicIssue enabled = "true">
        <Title>There are statements executed that may suffer from parameter sensitivity.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There are statements executed that may suffer from parameter sensitivity.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[There are statements executed that may suffer from parameter sensitivity.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


<PTOClinicIssue enabled = "true">
        <Title>High database log write percent.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[High database log write percent.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[High database log write percent.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

<PTOClinicIssue enabled = "true">
        <Title>Query store is not enabled.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Query store is not enabled.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Query store is not enabled.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

<PTOClinicIssue enabled = "true">
        <Title>Query store max size is small.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[Query store max size is small.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[Query store max size is small.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

<PTOClinicIssue enabled = "true">
        <Title>High CPU for the Managed Instance.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[High CPU for the Managed Instance.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[High CPU for the Managed Instance.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>

<PTOClinicIssue enabled = "true">
        <Title>There are queries that are driving high amounts of logical reads.</Title>
        <Category>Performance</Category>
        <Severity>High</Severity>
        <Impact>
            <![CDATA[There are queries that are driving high amounts of logical reads.
]]>
        </Impact>
        <Recommendation>
            <![CDATA[There are queries that are driving high amounts of logical reads.]]>
        </Recommendation>
        <Reading>
            <![CDATA[]]>
        </Reading>
        <Services>
            <![CDATA[]]>
        </Services>
    </PTOClinicIssue>


</IssuesList>